\section{Application to Model Selection for Primal SVM}

\subsection{Introduction}

In this chapter the nonconvex inexact bundle algorithm is applied to the problem of model selection for support vector machines (SVM) solving classification tasks.
It relies on a bilevel formulation proposed by Kunapuli \cite{Kunapuli2008} and Moore et al. \cite{Moore2011}.

A natural application for the inexact bundle algorithm is an optimization problem where the objective function value can only be computed iteratively. This is for example the case in bilevel optimization.
%If this objective function is iteratively optimized, the function value i never exact. But by the stopping condition it is (often???) possible to have the needed error bound on the function value (what about the gradient???). (Comment on asymptotical exactness?)

%A general bilevel problem can be formulated as 
%Bilevel optimization deals with a class of constrained optimization problems where one or more of the optimization variables are again constrained by optimization problems.
A general bilevel program can be formulated as \cite{Kunapuli2008}
\begin{equation}
	\begin{array}{cll}
	\displaystyle\min_{x \in X, y} & F(x,y) & \text{upper level} \vspace{0.5ex}\\
	\text{s.t.} & G(x,y) \leq 0 & \vspace{1em}\\
	& y \in \begin{Bmatrix} \displaystyle\argmax_{y\in Y} & f(x,y) \vspace{0.5ex}\\
	                        \text{s.t.} & g(x,y) \leq 0 
													\end{Bmatrix}. & \text{lower level}
	\end{array}
\end{equation}

It consists of an \emph{upper} or \emph{outer level} which is the overall function to be optimized. Contrary to usual constrained optimization problems which are constrained by explicitly given equalities and inequalities a  bilevel program is additionally constrained to a second optimization problem, the \emph{lower} or \emph{inner level} problem.

Solving bilevel problems can be divided roughly in two classes: implicit and explicit solution methods. \\
In the explicit methods the lower level problem is usually rewritten by its KKT conditions and the upper and lower level are solved simultaneously. For the setting of model selection for support vector machines as it is used here, this method is described in detail in \cite{Kunapuli2008}.

The second approach is the implicit one. Here the lower level problem is solved directly in every iteration of the outer optimization algorithm and the solution is plugged into the upper level objective. \\
Obviously if the inner level problem is solved numerically, the solution cannot be exact. Additionally the \emph{solution map} \( S(x) = \{y \in \R^k | y\) solves the lower level problem\(\}\) is often nondifferentiable \cite{Outrata1998} and since elements of  the solution map are plugged into the outer level objective function in the implicit approach, the outer level function becomes nonsmooth itself. \\
This is why the inexact bundle algorithm seems a natural choice to tackle these bilevel problems. \\ 
Moore et al. use the implicit approach in \cite{Moore2011} for support vector regression. However they use a gradient decent method which is not guaranteed to stop at an optimal solution. \\
In \cite{Moore2010a} he also suggests the nonconvex exact bundle algorithm of Fuduli et al. \cite{Fuduli2004a} for solving the bilevel regression problem. This allows for nonsmooth inner problems and can theoretically solve some of the issues of the gradient descent method. It ignores however, that the objective function values can only be calculated approximately. A fact which is not addressed in Fuduli's algorithm.

%\subsection{Notation and Expressions}

%training set, validation /holdout set...

\subsection{Introduction to Support Vector Machines}
%In times of big data machine learning is a very active field of research. 
Support vector machines are linear learning machines that were developed in the 90's by Vapnik and co-workers. Soon they could outperform several other programs in this area \cite{Cristianini2000} and the subsequent interest in SVMs lead to a very versatile application of these machines \cite{Kunapuli2008}.

The case that is considered here is binary support vector classification using supervised learning. \\
In classification data from a possibly high dimensional vector space \(\tilde{X} \subseteq \R^n\), the \emph{feature} or \emph{input space} is divided into two classes. These lie in the \emph{output domain} \(\tilde{Y} = \{-1,1\}\). Elements from the feature space will mostly be called \emph{data points} here. They get \emph{labels} from the feature space. Labeled data points are called \emph{examples}. \\
The functional relation between the features and the class of an example is given by the usually unknown \emph{response} or \emph{target function} \(f(x)\). \\
Supervised learning is a kind of machine learning task where the machine is given examples of input data with associated labels, the so called \emph{training data} \((X,Y)\). Mathematically this can be modeled by assuming that the examples are  drawn identically and independently distributed (iid) from the fixed joint distribution \(P(x,y)\). This usually unknown distribution states the probability that an data point \(x\) has the label \(y\) \cite{Vapnik1999}. \\
The overall goal is then to optimize the generalization ability, meaning the ability to predict the output for unseen data correctly \cite{Cristianini2000}.
%The overall goal of machine learning is to optimize the generalization ability of the algorithm --> this means: predict output for unlabeled input as well as possible.
%task is to find a mapping that predicts the output given unlabeled input as good as possible \cite{Cristianini2000}. 

\subsubsection{Risk minimization}
The concept of SVM's was originally inspired by the statistical learning theory developed by Vapnik. For a throughout analysis see \cite{Vapnik1998}. \\
%from: Vapnik - Statistical Learning Theory overview \cite{Vapnik1998}
The idea of \emph{risk minimization} is to find from a fixed set or class of functions the one that is the best approximation to the response function. This is done by minimizing a loss function that compares the given labels of the examples to the response of the learning machine.
%goal: find best available (from set of functions that was chosen before) approximation to the actual (unknown) ``response function'' --> function that ``classifies'' the data
%Minimize loss between response of ``supervisor'' and response of learning machine

As the response function is not known only the expected value of the loss can be calculated. It is given by the \emph{risk functional} 

\begin{equation}
	R(\lambda) = \int{\mathcal{L}(y,f_{\lambda}(x))\text{d}P(x,y)}
\label{risk_func}
\end{equation}

Where \(\mathcal{L}: \R^2 \to \R\) is the loss function, \(f_{\lambda}: \R^{n}\cap\mathcal{F} \to \R, ~ \lambda \in \Lambda\) the response function found by the learning machine and \(P(x,y)\) the joint distribution the training data is drawn from. The goal is now to find a function \(f_{\lambda^*}(x)\) in the chosen function space \(\mathcal{F}\) that minimizes this risk functional \cite{Vapnik1999}.

As the only given information is given by the training set inductive principles are used to work with the empirical risk, rather than with the risk functional.
The empirical risk only depends on the finite training set  and is given by 

%goal: find \(f_{\lambda_0}(x)\) that minimizes this functional
%\(P(x,y)\) is unknown
%Because only a data sample is known: Work with the empirical risk

\begin{equation}
	R_{emp}(\lambda) = \frac{1}{l} \sum_{i = 1}^{l}\mathcal{L}(y_i,f_{\lambda}(x^i)),
\label{emp_risk}
\end{equation}

where \(l\) is the number of data points.
%L: Loss function 
%Goal: approximate \(Q(z,\lambda_0)\) by \(Q(z,\lambda_l)\) that minimizes the empirical risk. --> is called ERM (empirical risk minimization induction principle)
The law of large numbers ensures that the empirical risk converges to the risk functional as the number of data points grows to infinity. This however does not guarantee that the function \(f_{\lambda,emp}\) that minimizes the empirical rist also converges towards the function \(f_{\lambda^*}\) that minimizes the risk functional.
The theory of consistency provides necessary and sufficient conditions that solve this issue \cite{Vapnik1999}.

Vapnik introduced therefore the structural risk minimization induction principle (SRM). It ensures that the used set of functions has a structure that makes it strongly consistent \cite{Vapnik1999}. Additionally it takes the complexity of the function that is used to approximate the target function into account. ``The SRM principle actually suggests a tradeoff between the quality of the approximation and the complexity of the approximating function'' \cite{Vapnik1999}.
This reduces the risk of \emph{overfitting}, meaning to overly fit the function to the training data with the result of poor generalization \cite{Cristianini2000}.

%Therefore: structural risk Minimization induction Principle (SRM) --> is strongly consistent --> idea: minimize the empirical risk by considering the complexity of the model (given by CV-dimension)

Support Vector machines fulfill all conditions of the SRM principle. Due to the kernel trick that allows for nonlinear classification tasks it is also very powerful. For more detailed information on this see \cite{Kunapuli2008, Vapnik1998} and references therein.

\subsubsection{Support Vector machines}

In the case of linear binary classification one searches for a an affine hyperplane \(\bm{w}\) shifted by \(b\) to separate the given data. The vector \(\bm{w}\) is called weight vector and \(b\) is the bias. \\
Let the data be linearly separable. The function deciding how the data is classified can then be written as

\[ f(x) = sign(\bm{w}^{\top}x-b). \]

Support vector machines aim at finding such a hyperplane that separates also unseen data optimally.

???Picture of hyperplane

One problem of this intuitive approach is that the representation of a hyperplane is not unique. If the plane described by \((\bm{w},b)\) separates the data there exist infinitely many hyperplanes \((t\bm{w},b), ~ t>0\) that separate the data in the same way. \\
To have a unique description of a separation hyperplane the \emph{canonical hyperplane for given data} \(x \in X\) is defined by 

\[ f(x) = \bm{w}^{\top} x - b  \quad \text{s.t.} \quad \min_i |\bm{w}^{\top}x^i-b| = 1\] 

This is always possible in the case where the data is  linearly separable and means that the inverse of the norm of the weight vector is equal to the distance of the closest point \(x \in X \) to  the hyperplane \cite{Kunapuli2008}.


%Kunapuli: representation of hyperplane not unique --> if \((\bm{w},b)\) separates data, infinitely many hyperplanes \((t\bm{w},tb) \forall t > 0\) also separate data --> need unique representation --> w.l.o.g. define \emph{canonical hyperplane} for given data \(x \in X\) as function 
%only valid for separable data

This gives rise to the following definition:
The \emph{margin} is the minimal Euclidean distance between a training example \(x^i\) and the separating hyperplane.
%one can show that margin invese proportional to norm of \(\bm{w}\).
A bigger margin means a lower complexity of the function \cite{Cristianini2000}. 

A \emph{maximal margin hyperplane} is the hyperplane that realizes the maximal possible margin for a given data set.

%Theorem 6.1
\begin{theorem} \cite{Cristianini2000}
Given a linearly separable training sample \(\Omega = ((x^i,y_i),...,(x^l,y_l))\) the hyperplane \((\bm{w},b)\) that solves the optimization problem

\[\|\bm{w}\|^2 \quad \text{subject to} \quad y_i(\bm{w}^{\top}x-b)\geq 1 \quad i = 1,...,l \]

realizes a maximal margin hyperplane  
\end{theorem}

Generally one cannot assume the data to be linearly separable. This is why in most applications a so called \emph{soft margin classifier} is used. It introduces the slack variables \(\xi_i\) that measure the distance of the misclassified points to the hyperplane:

%from this follows: soft margin classifyer --> not any more assumption that data linearly separable --> slack variables \(\xi_i\) measure the distance of misclassified points

%Def 2.6:
%\begin{definition} \cite{Cristianini2000}
Fix \(\gamma > 0\). A \emph{margin slack variable of the example} \((x^i,y_i)\) with respect to the hyperplane \((\bm{w},b)\) and target margin \(\gamma\) is 

\[\xi_i = \max(0, \gamma - y_i(\bm{w}^{\top}x+b))\] 

%\end{definition}  
If \(\xi_i > \gamma\) the point is misclassified. \\
One can also say that \(\|\xi\|\) measures the amount by which training set ``fails to have margin \(\gamma\)'' \cite{Cristianini2000}.

For support vector machines the target margin is set to \(\gamma = 1\).

%Book uses \(\xi^2\) -> then \(\xi>0\) not needed any more; only \(\xi\) also used -> 1-norm
This results finally in the following slightly different optimization problems for finding an optimal separating hyperplane \((\bm{w},b)\): 

\begin{align}
\begin{split}
	\min_{\bm{w},b,\xi} \quad & \frac{1}{2} \|\bm{w}\|^2_2+C\sum_{i=1}^l{\xi_i} \\
	\text{subject to} \quad & y_i(\bm{w}^{\top}x^i-b) \geq 1 - \xi_i \\
	& \xi_i \geq 0 \\
	& 	\forall i = 1, \dots , l
\end{split}
\label{SVM_1}
\end{align}

and

\begin{align}
\begin{split}
	\min_{\bm{w},b,\xi} \quad & \frac{1}{2} \|\bm{w}\|^2_2+C\sum_{i=1}^l{\xi_i^2} \vspace{0.5ex}\\
	\text{subject to} \quad & y_i(\bm{w}^{\top}x^i-b) \geq 1 - \xi_i \vspace{0.5ex}\\
	& 	\forall i = 1, \dots , l
\end{split}
\label{SVM_2}
\end{align}


The parameter \(C > 0\) gives a trade-off between the richness of the chosen set of functions \(f_alpha\) to reduce the error on the training data and the danger of overfitting to have good generalization. It has to be chosen a priori \cite{Kunapuli2008}.


\subsection{Explanation Bilevel Approach and Inexact Bundle Method}
%Check if regression case also included?????

The hyper-parameter \(C\) in the objective function of the classification problem has to be set before hand. This step is part of the model selection process.
To set this parameter optimally different methods can be used.
A very intuitive and widely used approach is doing  and cross validation (CV) with a grid search implementation.
%very costly, discrete parameter choice, not practicable in case of many parameter

To prevent overfitting and get a good parameter selection, especially in case of little data, commonly \(T\)-fold cross validation is used. \\
For this technique the training data is randomly partitioned into \(T\) subsets of equal size. One of these subsets is then left out for training and instead used afterwards to get an estimate of the generalization error.  \\
To use CV for model selection it has to be embedded into an optimization algorithm over the hyper-parameter space. 
Commonly this is done by discretizing the parameter space and for \(T\)-fold CV training \(T\) models at each grid point. The resulting models are then compared to find the best parameters in the grid.
Obviously for a growing number of hyper-parameters this is very costly. An additional drawback is that the parameters are only chosen from a finite set \cite{Kunapuli2008}.

\subsubsection{Reformulation as bilevel problem}

A more recent approach is the formulation as a bilevel problem used in \cite{Kunapuli2008, Moore2011}.
This makes it possible to optimize the hyper-parameters continuously.

%Formal description of the bilevel problem for t-fold cross validation
%Better model description --> see 2.2 (p.46) in thesis 

Let \(\Omega = {(x^1,y_1),...,(x^l,y_l)} \subseteq \R^{n+1}\) be a given data set of size \(l = |\Omega|\). The associated index set is denoted by \(\mathcal{N}\). For classification the labels \(y_i\) are \(\pm1\).
For \(T\)-fold cross validation let \(\bar{\Omega}_t\) and \(\Omega_t\) be the training set and the validation set within the \(t\)'th fold and \(\bar{\mathcal{N}}_t\) and \(\mathcal{N}_t\) the respective index sets.
Furthermore let \(f^t:\R^{n+1}\cap\mathcal{F} \to \R\) be the response function trained on the \(t\)'th fold and \(\lambda \in \Lambda\) the hyper-parameters to be optimized.
For a general machine learning problem with upper and lower loss function \(\mathcal{L}_{upp}\) and \(\mathcal{L}_{low}\) respectively the bilevel problem writes
 
%%% introduce \mathcal(F) already before as the function space

\begin{equation}
	\begin{array}{cll}
	\displaystyle\min_{\lambda, f^t} & \mathcal{L}_{upp}\left(\lambda,f^1|_{\Omega_1},...,f^T|_{\Omega_T}\right) & \text{upper level} \vspace{0.5ex}\\
	\text{s.t.} & \lambda \in \Lambda & \vspace{1em}\\
	& \text{for } t = 1,...,T: & \\
	& f^t \in \begin{Bmatrix} \displaystyle\argmin_{f\in \mathcal{F}} & \mathcal{L}_{low}(\lambda,f,(x^i,y_i)_{i = 1}^l\in \bar{\Omega}_t) \vspace{0.5ex}\\
	                        \text{s.t.} & g_{low}(\lambda,f) \leq 0 
													\end{Bmatrix}. & \text{lower level}
	\end{array}
\end{equation}

In the case of support vector classification the \(T\) inner problems are one of the classical SVM formulations (\ref{SVM_1}) or (\ref{SVM_2}) (but all \(T\) problems have the same formulation).
The problem can also be rewritten into a unconstrained form. This form will be helpful when using the inexact bundle algorithm for solving the bilevel problem.
For the \(t\)'th fold the resulting hyperplane is identified with the pair \((\bm{w}^t,b_t) \in \R^{n+1}\).
The inner level problem for the \(t\)'th fold can therefore be stated as

\begin{equation}
	(\bm{w}^t,b_t) \in \argmin_{\bm{w},b}\left\{ \frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)}\right\}
\label{lower_t_1}
\end{equation}

or

\begin{equation}
	(\bm{w}^t,b_t) \in \argmin_{\bm{w},b} \left\{\frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\left(\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)\right)^2}\right\}
\label{lower_t_2}
\end{equation}

Where the hyper-parameter \(\lambda = \frac{1}{C}\) was used  due to numerical stability \cite{Kunapuli2008}.

For the upper level objective function there are different choices possible.
Simply put the outer level objective should compare the different inner level solutions and pick the best one. An intuitive choice would therefore be to pick the misclassification loss, that count how many data points of the respective validation set \(\Omega_t\) were misclassified when taking function \(f^t\).

The misclassification loss can be written as

\begin{equation}
	\mathcal{L}_{mis} = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\left[-y_i((\bm{w}^t)^{\top}x-b_t)\right]_{\star}}
\label{misclass_loss}
\end{equation}

where the step function \(()_{\star}\) is defined componentwise for a vector as
\begin{equation}
	(r_{\star})_i = \left\{\begin{array}{c} 1, \quad \text{if } r_i > 0, \\ 0, \quad \text{if } r_i \leq 0 \end{array} \right. .
\label{step_fun}
\end{equation}

The drawback of this simple loss function is, that it is not continuous and as such not suitable for subgradient based optimization. Therefore another loss function is used for the upper level problem - the \emph{hinge loss}. It is an upper bound on the misclassification loss and reads

\begin{equation}
		\mathcal{L}_{hinge} = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}.
\label{hinge_loss}
\end{equation}

Hence the two final resulting bilevel formulations for model selection in support vector are 

\begin{align}
\begin{split}
	\min_{\bm{W},\bm{b}} \quad &  \mathcal{L}_{hinge}(\bm{W},\bm{b}) = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}\\
	\text{subject to} \quad &  \lambda > 0 \\
	& \text{for } t = 1,...,T \\
	& (\bm{w}^t,b_t) \in \argmin_{\bm{w},b}\left\{ \frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)}\right\} \\
\end{split}
\label{model_1}
\end{align}

and 

\begin{align}
\begin{split}
	\min_{\bm{W},\bm{b}} \quad &  \mathcal{L}_{hinge}(\bm{W},\bm{b}) = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}\\
	\text{subject to} \quad & \lambda > 0 \\
	& \text{for } t = 1,...,T \\
	& (\bm{w}^t,b_t) \in \argmin_{\bm{w},b}\left\{ \frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\left(\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)\right)^2}\right\}. \\
\end{split}
\label{model_2}
\end{align}


\subsubsection{The Inexact Bundle Method}

%ingredients for bundle method

To solve the given bilevel problem with the above presented nonconvex inexact bundle algorithm the algorithm jumps between the two levels. Once the inner level problems are solved for a given \(\lambda\) - this is possible with any QP-solver as the problems are convex - the bundle algorithm takes the outcoming \(w\) and \(b\) to optimizes the hyper-parameter again.

The difficulty with this approach is that the bundle algorithm needs one subgradient of the outer level objective function with respect to the parameter \(\lambda\). However to compute this subgradient also one subgradient of \(w\) and \(b\) with respect to \(\lambda\) has to be known.

-> theory partial derivatives for subgradients?????????

??? Formula \(??? \in \partial L_{upp}{\partial \lambda}\)

???one has to assume that the inner level problem is locally Lipschitz (or more general: its nonconvex subdifferential is well defined at every point). \\
Subdifferential has to have again a subdifferential!!! -> w.r.t. \(\lambda\) \\
The main idea is to replace the inner level problem by its optimality condition

\begin{equation}
	0 \in \partial(w,b)\mathcal{L}_{low}(\lambda,w,b).
\label{opt_con}
\end{equation}

\(\partial(w,b)\) means in this case that the subdifferential is taken with respect to the variables \(w\) and \(b\). \\
-> theory for subdifferentials in more than one variable!!! \\

For convex inner level problem this replacement is equivalent to the original problem.

The difference to the approach described in \cite{Kunapuli2008} is that the problem is not smoothly replaced by its KKT conditions but only by this optimality condition. The weight vector \(\bm{w}\) and bias \(b\) are treated as a function of \(\lambda\) and are optimized separately from this hyper-parameter.
The reformulated bilevel problem becomes:

\begin{align}
	\begin{split}
	\min_{\bm{W},\bm{b}} \quad &  \mathcal{L}_{hinge}(\bm{W},\bm{b}) = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}\\
	\text{subject to} \quad & \lambda > 0 \\
	& \text{for } t = 1,...,T \\
	& 0 \in \partial(w,b)\mathcal{L}_{low}(\lambda,w^t,b_t) \\
\end{split}
\label{SVM_opt_cond}
\end{align}

where \(\mathcal{L}_low\) can be the objective function of either of the two presented lower level problems.



solve the inner level problem (quadratic problem in constrained case) by some QP solver \\
put solution into upper level problem and solve it by using bundle method \\
difficulty: subgradient is needed to build model of the objective function --> need subgradient \(\frac{\partial \mathcal{L}}{\partial \lambda}\) --> for this need \(\frac{\partial (W,b)}{\partial \lambda}\) \\
but \((w,b)\) not available as functions -> only values

Moore et al. \cite{Moore2011} describe a method for getting the subgradient from the KKt-conditions of the lower level problem:

lower level problem convex -> therefore optimality conditions (some nonsmooth version -> source???) necessary and sufficient -> make ``subgradient'' of optimality conditions and then derive subgradient of w, b from this. \\
---> what are the conditions? optimality condition Lipschitz? 

Say (show) that all needed components are locally Lipschitz; state theorems about differentiability almost everywhere and convex hull of gradients gives set of subgradients\\
introduce special notation (only for this chapter) and because of readability adopt ``gradient writing''

Subgradients:
\(\mathcal{G}_{upp,\lambda}, \mathcal{G}_{upp,w},\mathcal{G}_{upp,b}\) -> subgradients of outer objective \\
\(g_w, g_b\) -> subgradient of w, b

\[ final subgradient = \left(\mathcal{G}_{upp,w}(w,b,\lambda)\right)^{\top}g_w+\left(\mathcal{G}_{upp,b}(w,b,\lambda)\right)^{\top}g_b+\mathcal{G}_{upp,\lambda}(w,b,\lambda) \]

subgradients \(\mathcal{G}_{upp,...}\) easy to find (assumption that locally Lipschitz) -> in this application differentiable

difficulty: find \(g_w, g_b\)
important: optimality condition must be a linear system in \(w,b\) -> this is the case in this application
\[ H(\lambda)\cdot (w,b)^{\top} = h(\lambda) \]

find subgradients of each element (from differentiation rules follows)

\[ \partial_{\lambda} H\cdot (w,b)^{\top} + H \cdot(\partial_{\lambda} w, \partial_{\lambda} b)^{\top} = \partial_{\lambda} h  \]

solve this for \((w,b)\):
\[ (\partial_{\lambda} w, \partial_{\lambda}b)^{\top} = H^{-1}\left(\partial_{\lambda}h-\partial_{\lambda} H \cdot(w,b)^{\top}\right) \]

matrix \(H\) has to be inverted -> in the feature space so scalable with size of data set -> still can be very costly \cite{Moore2011}

Applied to the two bilevel classification problems derived above, the subgradients have the following form:

derivative of upper level objective:
Notation: \(\delta_i := 1-y_i(w^{\top}x^i-b)\)

\begin{align}
	\partial_{w}\mathcal{L}_{upp}&= \frac{1}{T}\sum_{t=1}^T\frac{1}{\mathcal{N}_t}\sum_{i \in \mathcal{N}_t}{\left\{\begin{array}{cl} -y_ix^i & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.} \\
	\partial_{b}\mathcal{L}_{upp}&= \frac{1}{T}\sum_{t=1}^T\frac{1}{\mathcal{N}_t}\sum_{i \in \mathcal{N}_t}{\left\{\begin{array}{cl} y_i & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.}
	\end{align}

for hingequad: -> here subgradient \\
optimality condition:
\begin{align}
	0 = \partial_{\bm{w}}\mathcal{L}_{low} &= \lambda \bm{w}+2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (1-y_i(w^{\top}x^i-b))(-y_ix^i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.} \\
	0 = \partial_b\mathcal{L}_{low} &= 2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (1-y_i(w^{\top}x^i-b))(y_i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.}
	\end{align}
	
subgradient??? is this smooth? with respect to \(\lambda\)
\begin{align}
	0 &= \bm{w}+\lambda\partial_{\lambda}\bm{w}+2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (-y_i(\partial_{\lambda}w^{\top}x^i-\partial_{\lambda}b))(-y_ix^i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.} \\
	0 &= 2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (-y_i(\partial_{\lambda}w^{\top}x^i-\partial_{\lambda}b))(y_i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.}
\end{align}

From this the needed subgradients can be calculated via:

\begin{equation}
	2\cdot\begin{pmatrix} \sum_{i \in \bar{\mathcal{N}_t}}\frac{\lambda}{2}+{y_i^2x^i(x^i)^{\top}} & \sum_{i \in \bar{\mathcal{N}_t}}	{-y_i^2x^i} \\ \sum_{i \in \bar{\mathcal{N}_t}}{-y_i^2(x^i)^{\top}} & \sum_{i \in \bar{\mathcal{N}_t}}{y_i^2}\end{pmatrix}
	\cdot \begin{pmatrix} \partial_{\lambda}w \\ \partial_{\lambda}b\end{pmatrix}
	= \begin{pmatrix}-w \\ 0\end{pmatrix}
\label{subgr_wb}
\end{equation}

for hinge not quad:

not as much information in the subgradient/derivative

similar calculation leads to 

\begin{align}
	\partial_{\lambda}w &= -\frac{w}{\lambda} \\
	\bartial_{\lambda}b &= 0
\end{align}

\subsection{Numerical Experiments}

The bilevel-bundle algorithm for classification was tested for four different data sets taken from the UCI Machine Learning Repository \emph{citations as said in ``names'' data??? }.
For comparability with the already existing results presented in \cite{Kunapuli2008} the following data and specifications of it were taken:

\begin{center}
\emph{Table like in Kunapuli}
\begin{table}[H]%
	\begin{tabular}{lcccc}
		\hline
    Data set & \(l_{train}\) & \(l_{test}\) & n & T \\
		\hline
		Pima Indians Diabetes Database & 240 & 528 & 8 & 3 \\
		Wisconsin Breast Cancer Database & 240 & 443 & 9 & 3 \\
		Cleveland Heart Disease Database & 216 & 81 & 13 & 3 \\
		John Hopkins University Ionosphere Database & 240 & 111 & 33 & 3
	\end{tabular}
	\caption{}
	\label{}
\end{table}
\end{center}

As described in the Phd theseis the data was first standardized to unit mean and zero variance (\emph{not the 0,1 column in ? dataset}). The bilevel problem with cross validation was executed 20 times to get averaged results.
The results are compared by cross validation error, test error -> write which error this is and computation time.
Additionally write \(\bm{w}\), \(b\), \(\lambda\) ???
The objective function was scaled by 100. -> also test error (to get percentage) 

Table ??? shows the results 

\begin{center}
\begin{table}[H]%
	\begin{tabular}{lllll}
		\hline
		Data set & Method & CV Error & Test Error & Time (sec.) \\
		\hline
		\texttt{pima} & hingequad & \(60.72 \pm 9.56\) & \(24.11\pm 2.71\) & \(2.15 \pm 0.52\)\\
		 &              hinge loss & & & \\
		\texttt{cancer} &  hingequad & \(10.75\pm 7.52\) & \(3.41 \pm 1.16\) & \(3.43 \pm 28.84\) \\
		 &              hinge loss & & & \\
		\texttt{heart} &  hingequad & \(48.73 \pm 5.53\) & \(15.56 \pm 4.44\) & \(3.43 \pm 43.39\)\\
		 &              hinge loss & & & \\
		\texttt{ionosphere} &  hingequad & \(39.30 \pm 5.32\) & \(12.21 \pm 4.10\) & \(14.17 \pm 51.27\)\\
		 &              hinge loss & & & \\
	\end{tabular}
	\caption{}
	\label{res_table}
\end{table}
\end{center}

\textcolor{red}{interesting: 0 computing time for ionosphere???}

Extra table for \(\bm{w}\), \(b\), \(\lambda\) ?

First experiment: Classification

Write down bilevel classification problem and (if needed) which specification of the inexact bundle algorithm is used.

Write down the sets were used and how they were prepared.



