\section{Application to Model Selection for Primal SVM}

\textcolor{red}{Skalarprodukt anpassen, Vektoren nicht fett oder neue definition, notation, \(\lambda \in \Lambda\) einfugen}

\subsection{Introduction}

In this chapter the nonconvex inexact bundle algorithm is applied to the problem of model selection for \emph{support vector machines} (SVM) solving classification tasks.
It relies on a bilevel formulation proposed by Kunapuli \cite{Kunapuli2008} and Moore et al. \cite{Moore2011}.

A natural application for the inexact bundle algorithm is an optimization problem where the objective function value can only be computed iteratively. This is for example the case in bilevel optimization.
%If this objective function is iteratively optimized, the function value i never exact. But by the stopping condition it is (often???) possible to have the needed error bound on the function value (what about the gradient???). (Comment on asymptotical exactness?)

%A general bilevel problem can be formulated as 
%Bilevel optimization deals with a class of constrained optimization problems where one or more of the optimization variables are again constrained by optimization problems.
A general bilevel program can be formulated as \cite{Kunapuli2008}
\begin{equation}
	\begin{array}{cll}
	\displaystyle\min_{x \in X, y} & F(x,y) & \text{upper level} \vspace{0.5ex}\\
	\text{s.t.} & G(x,y) \leq 0 & \vspace{1em}\\
	& y \in \begin{Bmatrix} \displaystyle\argmax_{y\in Y} & f(x,y) \vspace{0.5ex}\\
	                        \text{s.t.} & g(x,y) \leq 0 
													\end{Bmatrix}. & \text{lower level}
	\end{array}
\end{equation}

It consists of an \emph{upper} or \emph{outer level} which is the overall function to be optimized. Contrary to usual constrained optimization problems which are constrained by explicitly given equalities and inequalities a  bilevel program is additionally constrained to a second optimization problem, the \emph{lower} or \emph{inner level} problem.

Solving bilevel problems can be divided roughly in two classes: implicit and explicit solution methods. \\
In the explicit methods the lower level problem is usually rewritten by its KKT conditions and the upper and lower level are solved simultaneously. For the setting of model selection for support vector machines as it is used here, this method is described in detail in \cite{Kunapuli2008}.

The second approach is the implicit one. Here the lower level problem is solved directly in every iteration of the outer optimization algorithm and the solution is plugged into the upper level objective. \\
Obviously if the inner level problem is solved numerically, the solution cannot be exact. Additionally the \emph{solution map} \( S(x) = \{y \in \R^k | y\) solves the lower level problem\(\}\) is often nondifferentiable \cite{Outrata1998} and since elements of  the solution map are plugged into the outer level objective function in the implicit approach, the outer level function becomes nonsmooth itself. \\
This is why the inexact bundle algorithm seems a natural choice to tackle these bilevel problems. \\ 
Moore et al. use the implicit approach in \cite{Moore2011} for support vector regression. However they use a gradient decent method which is not guaranteed to stop at an optimal solution. \\
In \cite{Moore2010a} he also suggests the nonconvex exact bundle algorithm of Fuduli et al. \cite{Fuduli2004a} for solving the bilevel regression problem. This allows for nonsmooth inner problems and can theoretically solve some of the issues of the gradient descent method. It ignores however, that the objective function values can only be calculated approximately. A fact which is not addressed in Fuduli's algorithm.

%\subsection{Notation and Expressions}

%training set, validation /holdout set...

\subsection{Introduction to Support Vector Machines}
%In times of big data machine learning is a very active field of research. 
Support vector machines are linear learning machines that were developed in the 90's by Vapnik and co-workers. Soon they could outperform several other programs in this area \cite{Cristianini2000} and the subsequent interest in SVMs lead to a very versatile application of these machines \cite{Kunapuli2008}.

The case that is considered here is binary support vector classification using supervised learning. \\
In classification data from a possibly high dimensional vector space \(\tilde{X} \subseteq \R^n\), the \emph{feature} or \emph{input space} is divided into two classes. These lie in the \emph{output domain} \(\tilde{Y} = \{-1,1\}\). Elements from the feature space will mostly be called \emph{data points} here. They get \emph{labels} from the feature space. Labeled data points are called \emph{examples}. \\
The functional relation between the features and the class of an example is given by the usually unknown \emph{response} or \emph{target function} \(f(x)\). \\
Supervised learning is a kind of machine learning task where the machine is given examples of input data with associated labels, the so called \emph{training data} \((X,Y)\). Mathematically this can be modeled by assuming that the examples are  drawn identically and independently distributed (iid) from the fixed joint distribution \(P(x,y)\). This usually unknown distribution states the probability that an data point \(x\) has the label \(y\) \cite{Vapnik1999}. \\
The overall goal is then to optimize the generalization ability, meaning the ability to predict the output for unseen data correctly \cite{Cristianini2000}.
%The overall goal of machine learning is to optimize the generalization ability of the algorithm --> this means: predict output for unlabeled input as well as possible.
%task is to find a mapping that predicts the output given unlabeled input as good as possible \cite{Cristianini2000}. 

\subsubsection{Risk minimization}
The concept of SVM's was originally inspired by the statistical learning theory developed by Vapnik. For a throughout analysis see \cite{Vapnik1998}. \\
%from: Vapnik - Statistical Learning Theory overview \cite{Vapnik1998}
The idea of \emph{risk minimization} is to find from a fixed set or class of functions the one that is the best approximation to the response function. This is done by minimizing a loss function that compares the given labels of the examples to the response of the learning machine.
%goal: find best available (from set of functions that was chosen before) approximation to the actual (unknown) ``response function'' --> function that ``classifies'' the data
%Minimize loss between response of ``supervisor'' and response of learning machine

As the response function is not known only the expected value of the loss can be calculated. It is given by the \emph{risk functional} 

\begin{equation}
	R(\lambda) = \int{\mathcal{L}(y,f_{\lambda}(x))\text{d}P(x,y)}
\label{risk_func}
\end{equation}

Where \(\mathcal{L}: \R^2 \to \R\) is the loss function, \(f_{\lambda}: \R^{n}\cap\mathcal{F} \to \R, ~ \lambda \in \Lambda\) the response function found by the learning machine and \(P(x,y)\) the joint distribution the training data is drawn from. The goal is now to find a function \(f_{\bar{\lambda}}(x)\) in the chosen function space \(\mathcal{F}\) that minimizes this risk functional \cite{Vapnik1999}.

As the only given information is given by the training set inductive principles are used to work with the empirical risk, rather than with the risk functional.
The empirical risk only depends on the finite training set  and is given by 

%goal: find \(f_{\lambda_0}(x)\) that minimizes this functional
%\(P(x,y)\) is unknown
%Because only a data sample is known: Work with the empirical risk

\begin{equation}
	R_{emp}(\lambda) = \frac{1}{l} \sum_{i = 1}^{l}\mathcal{L}(y_i,f_{\lambda}(x^i)),
\label{emp_risk}
\end{equation}

where \(l\) is the number of data points.
%L: Loss function 
%Goal: approximate \(Q(z,\lambda_0)\) by \(Q(z,\lambda_l)\) that minimizes the empirical risk. --> is called ERM (empirical risk minimization induction principle)
The law of large numbers ensures that the empirical risk converges to the risk functional as the number of data points grows to infinity. This however does not guarantee that the function \(f_{\lambda,emp}\) that minimizes the empirical rist also converges towards the function \(f_{\bar{\lambda}}\) that minimizes the risk functional.
The theory of consistency provides necessary and sufficient conditions that solve this issue \cite{Vapnik1999}.

Vapnik introduced therefore the structural risk minimization induction principle (SRM). It ensures that the used set of functions has a structure that makes it strongly consistent \cite{Vapnik1999}. Additionally it takes the complexity of the function that is used to approximate the target function into account. ``The SRM principle actually suggests a tradeoff between the quality of the approximation and the complexity of the approximating function'' \cite[p. 994]{Vapnik1999}.
This reduces the risk of \emph{overfitting}, meaning to overly fit the function to the training data with the result of poor generalization \cite{Cristianini2000}.

%Therefore: structural risk Minimization induction Principle (SRM) --> is strongly consistent --> idea: minimize the empirical risk by considering the complexity of the model (given by CV-dimension)

Support vector machines fulfill all conditions of the SRM principle. Due to the kernel trick that allows for nonlinear classification tasks it is also very powerful. For more detailed information on this see \cite{Kunapuli2008, Vapnik1998} and references therein.

\subsubsection{Support Vector machines}

In the case of linear binary classification one searches for a an affine hyperplane \(\bm{w}\) shifted by \(b\) to separate the given data. The vector \(\bm{w}\) is called weight vector and \(b\) is the bias. \\
Let the data be linearly separable. The function deciding how the data is classified can then be written as

\[ f(x) = sign(\bm{w}^{\top}x-b). \]

Support vector machines aim at finding such a hyperplane that separates also unseen data optimally.

\textcolor{red}{???Picture of hyperplane}

One problem of this intuitive approach is that the representation of a hyperplane is not unique. If the plane described by \((\bm{w},b)\) separates the data there exist infinitely many hyperplanes \((t\bm{w},b), ~ t>0\) that separate the data in the same way. \\
To have a unique description of a separation hyperplane the \emph{canonical hyperplane for given data} \(x \in X\) is defined by 

\[ f(x) = \bm{w}^{\top} x - b  \quad \text{s.t.} \quad \min_i |\bm{w}^{\top}x^i-b| = 1\] 

This is always possible in the case where the data is  linearly separable and means that the inverse of the norm of the weight vector is equal to the distance of the closest point \(x \in X \) to  the hyperplane \cite{Kunapuli2008}.


%Kunapuli: representation of hyperplane not unique --> if \((\bm{w},b)\) separates data, infinitely many hyperplanes \((t\bm{w},tb) \forall t > 0\) also separate data --> need unique representation --> w.l.o.g. define \emph{canonical hyperplane} for given data \(x \in X\) as function 
%only valid for separable data

This gives rise to the following definition:
The \emph{margin} is the minimal Euclidean distance between a training example \(x^i\) and the separating hyperplane.
%one can show that margin invese proportional to norm of \(\bm{w}\).
A bigger margin means a lower complexity of the function \cite{Cristianini2000}. 

A \emph{maximal margin hyperplane} is the hyperplane that realizes the maximal possible margin for a given data set.

%Theorem 6.1
\begin{theorem}[{\cite[Theorem 6.1]{Cristianini2000}}]
Given a linearly separable training sample \(\Omega = ((x^i,y_i),...,(x^l,y_l))\) the hyperplane \((\bm{w},b)\) that solves the optimization problem

\[\|\bm{w}\|^2 \quad \text{subject to} \quad y_i(\bm{w}^{\top}x-b)\geq 1 \quad i = 1,...,l \]

realizes a maximal margin hyperplane  
\end{theorem}

Generally one cannot assume the data to be linearly separable. This is why in most applications a so called \emph{soft margin classifier} is used. It introduces the slack variables \(\xi_i\) that measure the distance of the misclassified points to the hyperplane:

%from this follows: soft margin classifyer --> not any more assumption that data linearly separable --> slack variables \(\xi_i\) measure the distance of misclassified points

%Def 2.6:
%\begin{definition} \cite{Cristianini2000}
Fix \(\gamma > 0\). A \emph{margin slack variable of the example} \((x^i,y_i)\) with respect to the hyperplane \((\bm{w},b)\) and target margin \(\gamma\) is 

\[\xi_i = \max(0, \gamma - y_i(\bm{w}^{\top}x+b))\] 

%\end{definition}  
If \(\xi_i > \gamma\) the point is misclassified. \\
One can also say that \(\|\xi\|\) measures the amount by which training set ``fails to have margin \(\gamma\)'' \cite{Cristianini2000}.

For support vector machines the target margin is set to \(\gamma = 1\).

%Book uses \(\xi^2\) -> then \(\xi>0\) not needed any more; only \(\xi\) also used -> 1-norm
This results finally in the following slightly different
optimization problems for finding an optimal separating hyperplane \((\bm{w},b)\): 

\begin{align}
\begin{split}
	\min_{\bm{w},b,\xi} \quad & \frac{1}{2} \|\bm{w}\|^2_2+C\sum_{i=1}^l{\xi_i} \\
	\text{subject to} \quad & y_i(\bm{w}^{\top}x^i-b) \geq 1 - \xi_i \\
	& \xi_i \geq 0 \\
	& 	\forall i = 1, \dots , l
\end{split}
\label{SVM_1}
\end{align}

and

\begin{align}
\begin{split}
	\min_{\bm{w},b,\xi} \quad & \frac{1}{2} \|\bm{w}\|^2_2+C\sum_{i=1}^l{\xi_i^2} \vspace{0.5ex}\\
	\text{subject to} \quad & y_i(\bm{w}^{\top}x^i-b) \geq 1 - \xi_i \vspace{0.5ex}\\
	& 	\forall i = 1, \dots , l
\end{split}
\label{SVM_2}
\end{align}

The first part of the respective objective functions are the regularizations, the second part are the actual loss functions. 
The parameter \(C > 0\) gives a trade-off between the richness of the chosen set of functions \(f_{\alpha}\) to reduce the error on the training data and the danger of overfitting to have good generalization. It has to be chosen a priori \cite{Kunapuli2008}.
The two optimization problems only differ in the norm chosen for the loss function. In (\ref{SVM_1}) the one-norm is chosen, in (\ref{SVM_2}) the squared two-norm is used.
Problem (\ref{SVM_2}) is the one that will finally be used in the bilevel approach where smoothness of the objective function of the inner level problem is needed to calculate all needed subgradients.

\subsection{Bilevel Approach and Inexact Bundle Method}
%Check if regression case also included?????

The hyper-parameter \(C\) in the objective function of the classification problem has to be set beforehand. This step is part of the model selection process.
To set this parameter optimally different methods can be used.
A very intuitive and widely used approach is doing  and \emph{cross validation} (CV) with a grid search implementation.
%very costly, discrete parameter choice, not practicable in case of many parameter

To prevent overfitting and get a good parameter selection, especially in case of little data, commonly \(T\)-fold cross validation is used. \\
For this technique the training data is randomly partitioned into \(T\) subsets of equal size. One of these subsets is then left out for training and instead used afterwards to get an estimate of the generalization error.  \\
To use CV for model selection it has to be embedded into an optimization algorithm over the hyper-parameter space. 
Commonly this is done by discretizing the parameter space and for \(T\)-fold CV training \(T\) models at each grid point. The resulting models are then compared to find the best parameters in the grid.
Obviously for a growing number of hyper-parameters this is very costly. An additional drawback is that the parameters are only chosen from a finite set \cite{Kunapuli2008}.

\subsubsection{Reformulation as bilevel problem}

A more recent approach is the formulation as a bilevel problem used in \cite{Kunapuli2008, Moore2011}.
This makes it possible to optimize the hyper-parameters continuously.

%Formal description of the bilevel problem for t-fold cross validation
%Better model description --> see 2.2 (p.46) in thesis 

Let \(\Omega = {(x^1,y_1),...,(x^l,y_l)} \subseteq \R^{n+1}\) be a given data set of size \(l = |\Omega|\). The associated index set is denoted by \(\mathcal{N}\). For classification the labels \(y_i\) are \(\pm1\).
For \(T\)-fold cross validation let \(\bar{\Omega}_t\) and \(\Omega_t\) be the training set and the validation set within the \(t\)'th fold and \(\bar{\mathcal{N}}_t\) and \(\mathcal{N}_t\) the respective index sets.
Furthermore let \(f^t:\R^{n+1}\cap\mathcal{F} \to \R\) be the response function trained on the \(t\)'th fold and \(\lambda \in \Lambda\) the hyper-parameters to be optimized.
For a general machine learning problem with upper and lower loss function \(\mathcal{L}_{upp}\) and \(\mathcal{L}_{low}\) respectively the bilevel problem writes
 
%%% introduce \mathcal(F) already before as the function space

\begin{equation}
	\begin{array}{cll}
	\displaystyle\min_{\lambda, f^t} & \mathcal{L}_{upp}\left(\lambda,f^1|_{\Omega_1},...,f^T|_{\Omega_T}\right) & \text{upper level} \vspace{0.5ex}\\
	\text{s.t.} & \lambda \in \Lambda & \vspace{1em}\\
	& \text{for } t = 1,...,T: & \\
	& f^t \in \begin{Bmatrix} \displaystyle\argmin_{f\in \mathcal{F}} & \mathcal{L}_{low}(\lambda,f,(x^i,y_i)_{i = 1}^l\in \bar{\Omega}_t) \vspace{0.5ex}\\
	                        \text{s.t.} & g_{low}(\lambda,f) \leq 0 
													\end{Bmatrix}. & \text{lower level}
	\end{array}
\end{equation}

In the case of support vector classification the \(T\) inner problems hve the classical SVM formulation %(\ref{SVM_1}) or 
(\ref{SVM_2}). % (but all \(T\) problems have the same formulation).
The problem can also be rewritten into a unconstrained form. This form is helpful when using the inexact bundle algorithm for solving the bilevel problem.
For the \(t\)'th fold the resulting hyperplane is identified with the pair \((\bm{w}^t,b_t) \in \R^{n+1}\).
The inner level problem for the \(t\)'th fold can therefore be stated as

%\begin{equation}
	%(\bm{w}^t,b_t) \in \argmin_{\bm{w},b}\left\{ \frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)}\right\}
%\label{lower_t_1}
%\end{equation}

%or

\begin{equation}
	(\bm{w}^t,b_t) \in \argmin_{\bm{w},b} \left\{\frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\left(\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)\right)^2}\right\}
\label{lower_t_2}
\end{equation}

Where the hyper-parameter \(\lambda = \frac{1}{C}\) was used  due to numerical stability \cite{Kunapuli2008}.

For the upper level objective function there are different choices possible.
Simply put the outer level objective should compare the different inner level solutions and pick the best one. An intuitive choice would therefore be to pick the misclassification loss, that count how many data points of the respective validation set \(\Omega_t\) were misclassified when taking function \(f^t\).

The misclassification loss can be written as

\begin{equation}
	\mathcal{L}_{mis} = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\left[-y_i((\bm{w}^t)^{\top}x-b_t)\right]_{\star}}
\label{misclass_loss}
\end{equation}

where the step function \(()_{\star}\) is defined componentwise for a vector as
\begin{equation}
	(r_{\star})_i = \left\{\begin{array}{c} 1, \quad \text{if } r_i > 0, \\ 0, \quad \text{if } r_i \leq 0 \end{array} \right. .
\label{step_fun}
\end{equation}

The drawback of this simple loss function is, that it is not continuous and as such not suitable for subgradient based optimization. Therefore another loss function is used for the upper level problem - the \emph{hinge loss}. It is an upper bound on the misclassification loss and reads

\begin{equation}
		\mathcal{L}_{hinge} = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}.
\label{hinge_loss}
\end{equation}

It is also possible to square the max term. This results in the loss function

\begin{equation}
		\mathcal{L}_{hinge} = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)^2}.
\label{hingequad_loss}
\end{equation}

In figure (\ref{??}) it can be seen that its minimum and overall progress is more similar to the misclassification loss than the one of the hinge loss. For this reason we progress taking the squared form of the hinge loss, abbreviating with \emph{hingequad loss} for convenience.

%\textcolor{red}{also possible to square the max term\\
%formula \\
%from plots: seems to match the misclassification loss a bit better -> used it instead/also}

Hence the final resulting bilevel formulation for model selection in support vector classification is 

%\begin{align}
%\begin{split}
	%\min_{\bm{W},\bm{b}} \quad &  \mathcal{L}_{hinge}(\bm{W},\bm{b}) = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}\\
	%\text{subject to} \quad &  \lambda > 0 \\
	%& \text{for } t = 1,...,T \\
	%& (\bm{w}^t,b_t) \in \argmin_{\bm{w},b}\left\{ \frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)}\right\} \\
%\end{split}
%\label{model_1}
%\end{align}
%
%and 

\begin{align}
\begin{split}
	\min_{\bm{W},\bm{b}} \quad &  \mathcal{L}_{hinge}(\bm{W},\bm{b}) = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)^2}\\
	\text{subject to} \quad & \lambda > 0 \\
	& \text{for } t = 1,...,T \\
	& (\bm{w}^t,b_t) \in \argmin_{\bm{w},b}\left\{ \frac{\lambda}{2} \|\bm{w}\|^2_2+\sum_{i\in \bar{\mathcal{N}}_t}{\left(\max\left(1-y_i(\bm{w}^{\top}x^i-b),0\right)\right)^2}\right\}. \\
\end{split}
\label{model_2}
\end{align}


\subsubsection{Solution of the Bilevel Program}

\textcolor{red}{ab hier: Theorie fehlt\\
!!! notation - oder in prelininaries einfugen}
%ingredients for bundle method

To solve the given bilevel problem with the above presented nonconvex inexact bundle algorithm the algorithm jumps between the two levels. Once the inner level problems are solved for a given \(\lambda\) - this is possible with any QP-solver as the problems are convex - the bundle algorithm takes the outcome \(w\) and \(b\) and optimizes the hyper-parameter again.

The difficulty with this approach is that the bundle algorithm needs one subgradient of the outer level objective function with respect to the parameter \(\lambda\). However to compute this subgradient also one subgradient of \(w\) and \(b\) with respect to \(\lambda\) has to be known.

\textbf{The Differentiable Case}
\textcolor{red}{example in differentiable case}\\
Let us first assume that the outer and inner objective functions and \(w(\lambda)= \argmin \mathcal{L}_{low}(w,\lambda)\) are sufficiently often continuously differentiable to demonstrate the procedure of calculating the needed {(sub-)gradients}.

Let \(\mathcal{L}_{upp}(w,\lambda)\) be the objective function of the outer level problem, where the variable \(b\) was left out for the sake of simplicity.
To find an optimal hyper parameter \(\lambda\) given the input \(w\) the gradient \(g^{upp}_{\lambda}\) of \(\mathcal{L}_{upp}\) with respect to \(\lambda\) is needed in every iteration of the solving algorithm.
In order to calculate this gradient the chain rule is used yielding

\begin{equation*}
	g^{upp}_{\lambda} = \left(\frac{\partial}{\partial w}\mathcal{L}_{upp}(w,\lambda) \right)^{\top}\frac{\partial w(\lambda)}{\partial\lambda}+\frac{\partial}{\partial \lambda}\mathcal{L}_{upp}(w,\lambda).
\end{equation*}

The challenge is here to find the term \(\frac{\partial w(\lambda)}{\partial\lambda}\) because 

\[ \frac{\partial w}{\partial \lambda} \in \frac{\partial }{\partial \lambda} \argmin_{w} \mathcal{L}_{low}(w,\lambda). \]

Assuming \(\mathcal{L}_{low}\) is twice continuously differentiable at the  optimal solution \(w^*\) of the lower level problem the optimality condition for any parameter \(\lambda_0 > 0\)

\begin{equation}
	0 = \frac{\partial}{\partial w}\mathcal{L}_{low}(w^*,\lambda_0)
\label{opt_con}
\end{equation}


can be used to calculate the needed gradient in an indirect manner.

In the differentiable case the theoretical framework for the following calculations is given by the implicit function theorem.

\begin{theorem}[{c.f. \cite[chapter 3.4]{Koenigsberger2002}}]
	Let \(F: U \times V \to Z \), \(U\in\R^m, V,Z\in \R^n \), be a \(\mathcal{C}^1\) mapping, \((x_0,y_0) \in U\times V\)  and \(F(x_0,y_0) = 0\). If the matrix \(\frac{\partial}{\partial y}F(x_0,y_0)\) is invertible, there exist neighborhoods \(U_0\subset U\) of \(x_0\) and \(V_0\subset V\) of \(y_0\) and a continuously differentiable mapping \(f:U_0 \to V_0\) with 
	\begin{equation*}
	F(x,y) = 0, (x,y)\in U_0\times V_0  \quad \Leftrightarrow \quad y = f(x), x\in U_0.
	%\label{imp_fun_theo}
	\end{equation*}
\end{theorem}

Identifying \(x \estimates \lambda\), \(y \estimates w\) and  \(F(x_0,y_0) \estimates \frac{\partial}{\partial w} \mathcal{L}_{low}(w^*,\lambda_0)\) and assuming \(\frac{\partial^2 }{\partial  w^2}\mathcal{L}_{low}(w^*,\lambda_0)\) is invertible this theorem provides the existence of the continuously differentiable function \(w(\lambda)\) whose gradient is needed. \\
\textcolor{red}{what about the neighborhoods?}

If the inner level loss function yields a linear optimality condition in \(w\) it is possible to calculate the gradient explicitly. This is for example the case for SVM loss functions with a squared one- or two-norm as given in problem (\ref{SVM_2}).
The optimality condition can then be written as the linear system

\begin{equation*}
	H(\lambda)w = h(\lambda).
\end{equation*}

By taking the partial derivative with respect to \(\lambda\) on both sides of the system one gets

\begin{equation*}
	\frac{\partial H(\lambda)}{\partial \lambda}w+H(\lambda)\frac{\partial w}{\partial \lambda} = \frac{\partial h(\lambda)}{\partial  \lambda}.
\end{equation*}

If \(H(\lambda)\) is invertible for all \(\lambda \in \Lambda\) then the needed gradient is given by 

\begin{equation*}
	\frac{\partial w}{\partial \lambda} = H^{-1}(\lambda)\left(\frac{\partial h(\lambda)}{\partial \lambda}-\frac{\partial H(\lambda)}{\partial \lambda}w\right).
\end{equation*}

%\textcolor{red}{why H invertible??? - because we assume existence of \(w\)???}\\

\textbf{The Nondifferentiable Case}
\textcolor{red}{now for subgradients}

In practice we cannot expect \(\mathcal{L}_{low}\) to satisfy such strong differentiability properties.
It is therefore only assumed that \(\mathcal{L}_{low}\) is once continuously differentiable in \(w\). This assures that the optimality condition of the lower level problem is an equality like in (\ref{opt_con}).
%If \(\mathcal{L}_{low}\) was only locally Lipschitz the optimality condition states that zero is part of the subgradient set  \(\partial \mathcal{L}_{low}(w^*,\lambda_0)\).  
Contrary to the exemplary calculations from above in practice the second derivative \(\frac{\partial^2}{\partial w \partial \lambda} \mathcal{L}_{low}(w(\lambda),\lambda)\) however is not existent in this form, but a set of subgradients.



\textcolor{red}{\textbf{Notation}}\\
First the theoretical framework given to derive the results from above in the nondifferentiable case



\textcolor{red}{An important result about Lipschitz functions is Rademacher's theorem which states that these functions are differentiable almost everywhere but on a set of Lebesgue measure zero\cite[Theorem 3.1]{Heinonen2004}. 
Clarke deduces from this that the subdifferential at each of the nondifferentiable points is the convex hull of the limits of the sequence gradients a these points \cite[see Theorem 2.5.1]{Clarke1990}.}

This motivates the multidimensional definition of Clake's generalized gradient 

\textcolor{red}{
\begin{definition}[{\cite[Definition 2.6.1]{Clarke1990}}]
	\emph{generalized Jacobian}: \(F:\R^n \to \R^m\), with locally Lipschitz component functions \(F(x) = (f_1(x),...,f_m(x))\).\\
	Denote generalized Jacobian by \(\partial F(x) = conv\left(\lim JF(x_i)|x_i\to x, x_i \notin \Omega_F \right) \) where \(\Omega_F\) is the set of nondifferentiable points of \(F\)
\end{definition}
(after that comes proposition with properties of \(\partial F\))}


\textcolor{red}{To facilitate readability we use the following notation for the derivation of the nondifferentiable results.}

The \emph{'partial' subdifferential} of a function \(f(a^*,b_0,c_0,...)\) at the point \(a^*\) with respect to one variable \(a\) when all other variables are fixed is denoted by 
\begin{equation*}
	\partial^{a} f(a^*,b_0,c_0,...).
\end{equation*}

A subgradient of this subdifferential is written \(g^{a} \in \partial^{a} f(a^*,b_0,c_0,...)\).

Next step: show that chain rule is still valid in the nonsmooth case
Chain rules for subdifferential \\

\begin{theorem}[{\cite[Theorem 2.6.6]{Clarke1990}}]
	Let \(f(x)=\phi(F(x))\), with the locally Lipschitz functions \(F: \R^n \to \R^m\) and \(\phi:\R^m \to \R\). Then \(f\) is locally Lipschitz and it holds
	\begin{equation*}
		\partial f \subset \text{conv}\left\{\partial \phi(F(x))\partial F(x)\right\}.
	\end{equation*}
	If in addition \(\phi\) is strictly differentiable at \(F(x)\), then equality holds.
\end{theorem}

\textcolor{red}{strictly differentiable: c.f. \cite[Theorem 9.17 and 9.18]{Rockafellar2009} locally Lipschitz continuous and at most one subgradient at the point in question (see also comment to Definition 91 in \cite{Rockafellar2009})}

\begin{theorem}[c.f. {\cite[Theorem 7.1]{Rockafellar1985}}]
	Let \(p(x) = f(F(x))\), where \(F:\R^n \to \R^d\) is locally Lipschitz and \(f:\R^d \to \R\) is lower semicontinuous. 
	Assume \[\nexists y \in \partial^{\infty}f(F(\bar{x})), y \neq 0 \quad \text{with} \quad 0 \in y \partial F(\bar{x}). \]
	Then for the sets 
	\[ M(\bar{x}):= \partial f(F(\bar{x}))\partial F(\bar{x}), \quad M^{\infty}(\bar{x}):= \partial^{\infty}f(F(\bar{x}))\partial F(\bar{x}), \]
	one has \(\hat{\partial}p(\bar{x}) \subset M(\bar{x})\) and \(\hat{\partial}^{\infty} p(\bar{x}) \subset M^{\infty}(\bar{x})\).
\end{theorem}

\textcolor{red}{show that functions fulfill assumptions}\\

Implicit function theorem>\\
\begin{theorem}[{\cite[Theorem 5.2]{Outrata1998}}]
	(a) Assume there exists a single valued  Lipschitz function \(F\)
\end{theorem}
in original theorem around 0< can always be done by 

\begin{itemize}
	\item notation - check
	\item definition of subgradient-''matrix'' - check
	\item chain rule - check
	\item optimality condition /check
\end{itemize}

\begin{itemize}
	\item welche art von inexaktheit -> Funktionswerte \(w,b\) inexakt\\
	-> gradient im Endeffekt exakt, da von exakter optimalit'tsbedingung ausgegangen wird
\end{itemize}


\textcolor{red}{In practice this (Rademacher) means that it is possible to choose a subgradient by using the (one sided) gradients at nondifferentiable points.
We keep this in mind when analyzing the procedure of finding a subgradient \( g^{\lambda} \in \partial^{\lambda} w(\lambda)\) in the nondifferentiable case.}

\textcolor{red}{what else???}

\textcolor{red}{explanation}








For me: \(f\) locally Lipschitz??? then partial derivatives are the same! Else: check definition of derivatives!

\textcolor{red}{-> theory partial derivatives for subgradients????????? \\
??? Formula \(??? \in \partial L_{upp}{\partial \lambda}\) \\
???one has to assume that the inner level problem is locally Lipschitz (or more general: its nonconvex subdifferential is well defined at every point). \\
Subdifferential has to have again a subdifferential!!! -> w.r.t. \(\lambda\)}


The main idea is to replace the inner level problem by its optimality condition


\(\partial(w,b)\) means in this case that the subdifferential is taken with respect to the variables \(w\) and \(b\). \\
-> theory for subdifferentials in more than one variable!!! \\

For convex inner level problem this replacement is equivalent to the original problem.

The difference to the approach described in \cite{Kunapuli2008} is that the problem is not smoothly replaced by its KKT conditions but only by this optimality condition. The weight vector \(\bm{w}\) and bias \(b\) are treated as a function of \(\lambda\) and are optimized separately from this hyper-parameter.
The reformulated bilevel problem becomes:

\begin{align}
	\begin{split}
	\min_{\bm{W},\bm{b}} \quad &  \mathcal{L}_{hinge}(\bm{W},\bm{b}) = \frac{1}{T}\sum_{t=1}^T\frac{1}{|\mathcal{N}_t|}\sum_{i \in \mathcal{N}_t}{\max\left(1-y_i((\bm{w}^t)^{\top}x-b_t),0\right)}\\
	\text{subject to} \quad & \lambda > 0 \\
	& \text{for } t = 1,...,T \\
	& 0 \in \partial(w,b)\mathcal{L}_{low}(\lambda,w^t,b_t) \\
\end{split}
\label{SVM_opt_cond}
\end{align}

where \(\mathcal{L}_{low}\) can be the objective function of either of the two presented lower level problems.



solve the inner level problem (quadratic problem in constrained case) by some QP solver \\
put solution into upper level problem and solve it by using bundle method \\
difficulty: subgradient is needed to build model of the objective function --> need subgradient \(\frac{\partial \mathcal{L}}{\partial \lambda}\) --> for this need \(\frac{\partial (W,b)}{\partial \lambda}\) \\
but \((w,b)\) not available as functions -> only values

Moore et al. \cite{Moore2011} describe a method for getting the subgradient from the KKt-conditions of the lower level problem:

lower level problem convex -> therefore optimality conditions (some nonsmooth version -> source???) necessary and sufficient -> make ``subgradient'' of optimality conditions and then derive subgradient of w, b from this. \\
---> what are the conditions? optimality condition Lipschitz? 

Say (show) that all needed components are locally Lipschitz; state theorems about differentiability almost everywhere and convex hull of gradients gives set of subgradients\\
introduce special notation (only for this chapter) and because of readability adopt ``gradient writing''

Subgradients:
\(\mathcal{G}_{upp,\lambda}, \mathcal{G}_{upp,w},\mathcal{G}_{upp,b}\) -> subgradients of outer objective \\
\(g_w, g_b\) -> subgradient of w, b

\[ final subgradient = \left(\mathcal{G}_{upp,w}(w,b,\lambda)\right)^{\top}g_w+\left(\mathcal{G}_{upp,b}(w,b,\lambda)\right)^{\top}g_b+\mathcal{G}_{upp,\lambda}(w,b,\lambda) 
\label{subgr_upp}\]

subgradients \(\mathcal{G}_{upp,...}\) easy to find (assumption that locally Lipschitz) -> in this application differentiable

difficulty: find \(g_w, g_b\)
important: optimality condition must be a linear system in \(w,b\) -> this is the case in this application
\[ H(\lambda)\cdot (w,b)^{\top} = h(\lambda) \]

find subgradients of each element (from differentiation rules follows)

\[ \partial_{\lambda} H\cdot (w,b)^{\top} + H \cdot(\partial_{\lambda} w, \partial_{\lambda} b)^{\top} = \partial_{\lambda} h  \]

solve this for \((w,b)\):
\[ (\partial_{\lambda} w, \partial_{\lambda}b)^{\top} = H^{-1}\left(\partial_{\lambda}h-\partial_{\lambda} H \cdot(w,b)^{\top}\right) \]

matrix \(H\) has to be inverted -> in the feature space so scalable with size of data set -> still can be very costly \cite{Moore2011}

Applied to the two bilevel classification problems derived above, the subgradients have the following form:

derivative of upper level objective:
Notation: \(\delta_i := 1-y_i(w^{\top}x^i-b)\)

\begin{align}
	\partial_{w}\mathcal{L}_{upp}&= \frac{1}{T}\sum_{t=1}^T\frac{1}{\mathcal{N}_t}\sum_{i \in \mathcal{N}_t}{\left\{\begin{array}{cl} -y_ix^i & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.} \\
	\partial_{b}\mathcal{L}_{upp}&= \frac{1}{T}\sum_{t=1}^T\frac{1}{\mathcal{N}_t}\sum_{i \in \mathcal{N}_t}{\left\{\begin{array}{cl} y_i & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.}
	\end{align}
	
	here at the kink subgradient 0 is taken

for hingequad: -> here subgradient \\
optimality condition:
\begin{align}
	0 = \partial_{\bm{w}}\mathcal{L}_{low} &= \lambda \bm{w}+2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (1-y_i(w^{\top}x^i-b))(-y_ix^i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.} \\
	0 = \partial_b\mathcal{L}_{low} &= 2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (1-y_i(w^{\top}x^i-b))(y_i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.}
	\end{align}
	
subgradient??? is this smooth? with respect to \(\lambda\)
\begin{align}
	0 &= \bm{w}+\lambda\partial_{\lambda}\bm{w}+2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (-y_i(\partial_{\lambda}w^{\top}x^i-\partial_{\lambda}b))(-y_ix^i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.} \\
	0 &= 2\sum_{i \in \bar{\mathcal{N}_t}}{\left\{\begin{array}{cl} (-y_i(\partial_{\lambda}w^{\top}x^i-\partial_{\lambda}b))(y_i) & \text{if } \delta_i >0 \\ 0 & \text{if } \delta_i \leq 0 \end{array} \right.}
\end{align}

From this the needed subgradients can be calculated via:

\begin{equation}
	2\cdot\begin{pmatrix} \sum_{i \in \bar{\mathcal{N}_t}}\frac{\lambda}{2}+{y_i^2x^i(x^i)^{\top}} & \sum_{i \in \bar{\mathcal{N}_t}}	{-y_i^2x^i} \\ \sum_{i \in \bar{\mathcal{N}_t}}{-y_i^2(x^i)^{\top}} & \sum_{i \in \bar{\mathcal{N}_t}}{y_i^2}\end{pmatrix}
	\cdot \begin{pmatrix} \partial_{\lambda}w \\ \partial_{\lambda}b\end{pmatrix}
	= \begin{pmatrix}-w \\ 0\end{pmatrix}
\label{subgr_wb}
\end{equation}

for hinge not quad:

not as much information in the subgradient/derivative

similar calculation leads to 

\begin{align}
	\partial_{\lambda}w &= -\frac{w}{\lambda} \\
	\partial_{\lambda}b &= 0
\end{align}

\subsubsection{The Algorithm???}

The inexact bundle algorithm for the support vector classification task in bilevel formulation 
\vspace{1.5em}
\hrule  \vspace{0.4ex} \hrule
\vspace{1ex}
\textbf{Bilevel Bundle Method}
\vspace{1ex}
\hrule
\vspace{1ex}
Initiate all parameters, select a starting hyper-parameter \(\lambda_1\) and solve the lower level problem for \(\bm{w}^1\) and \(b_1\). \\
Calculate arbitrary subgradients of \(\bm{w}^1\) and \(b_1\) with respect to \(\lambda\) via \ref{subgr_wb} and a subgradient of the upper level problem by \ref{subgr_upp}.  
For \(k = 1,2,3,  \dotsc \)   

\begin{enumerate}
	\item Calculate the step \(d^k\) by minimizing the model of the convexfied objective
	\item Compute the aggregate subgradient and error and the stopping tolerance \(\delta\).
		If \(\delta_k \leq \mathtt{tol} \rightarrow \) STOP.
	\item Set \( \lambda^{k+1} = \hat{\lambda}^k + d^k \).
	\item solve again the inner level problem and calculate all subgradients needed to compute a subgradient of the outer level objective \\
	Calculate function value and a subgradient for the outer level objective function and test if a serious step was done 
	If yes, set \(\hat{\lambda}^{k+1} = \lambda^{k+10}\) and select \(t_{k+1} > 0\). \\
	Otherwise \(\rightarrow\) nullstep \\
	Set \(\hat{\lambda}^{k+1} = \hat{\lambda}^k\) and choose \(0 < t_{k+1} \leq t_k\). 	
	\item Select new bundle index set \(J_{k+1}\). Calculate convexification parameter \(\eta_k\)
	and update the model \(M^k\)
\end{enumerate}
\vspace{1ex}
\hrule
\vspace{1.5em}

Names for algorithms: BBMH -> hinge as inner level, BBMH2 -> hingequad as inner level 

\subsection{Numerical Experiments}

The bilevel-bundle algorithm for classification was tested for four different data sets taken from the UCI Machine Learning Repository \emph{citations as said in ``names'' data??? }.
For comparability with the already existing results presented in \cite{Kunapuli2008} the following data and specifications of it were taken:

\begin{center}
\emph{Table like in Kunapuli}
\begin{table}[H]%
	\begin{tabular}{lcccc}
		\hline
    Data set & \(l_{train}\) & \(l_{test}\) & n & T \\
		\hline
		Pima Indians Diabetes Database & 240 & 528 & 8 & 3 \\
		Wisconsin Breast Cancer Database & 240 & 443 & 9 & 3 \\
		Cleveland Heart Disease Database & 216 & 81 & 13 & 3 \\
		John Hopkins University Ionosphere Database & 240 & 111 & 33 & 3
	\end{tabular}
	\caption{}
	\label{}
\end{table}
\end{center}

As described in the PhD thesis the data was first standardized to unit mean and zero variance (\emph{not the 0,1 column in ? dataset}). The bilevel problem with cross validation was executed 20 times to get averaged results.
The results are compared by cross validation error, test error -> write which error this is and computation time.
Additionally write \(\bm{w}\), \(b\), \(\lambda\) ???
The objective function and test error were scaled by 100. -> also test error (to get percentage) 

After every run the calculated \(\lambda\) was taken and the algorithm was trained with \(\frac{T}{T-1} \lambda\) on the whole training set.
Then the percentage of misclassifications on the test set was calculated via

\begin{equation}
	E_{test} = \frac{1}{l_{test}}\sum_{i=1}^{l_{test}}{\frac{1}{2}|sign\left(\bm{w}^{\top}x^i-b \right)-y_i|}
\label{test_err}
\end{equation}

Table ??? shows the results 

\begin{center}
\begin{table}[H]%
	\begin{tabular}{llclll}
		\hline
		Data set & Method & \(T/(T-1)\lambda\) & CV Error & Test Error & Time (sec.) \\
		\hline
		\texttt{pima} & hingequad & \(10^{-15}\) & \(60.72 \pm 9.56\) & \(24.11\pm 2.71\) & \(2.15 \pm 0.52\)\\
		 &              hinge loss & & & \\
		\texttt{cancer} &  hingequad & \(0.6<\lambda<10.3\) & \(10.75\pm 7.52\) & \(3.41 \pm 1.16\) & \(3.43 \pm 28.84\) \\
		 &              hinge loss & & & \\
		\texttt{heart} &  hingequad & \(10^{-16}\) & \(48.73 \pm 5.53\) & \(15.56 \pm 4.44\) & \(3.43 \pm 43.39\)\\
		 &              hinge loss & & & \\
		\texttt{ionosphere} &  hingequad & \(3<\lambda<7.5\) & \(39.30 \pm 5.32\) & \(12.21 \pm 4.10\) & \(14.17 \pm 51.27\)\\
		 &              hinge loss & & & \\
	\end{tabular}
	\caption{}
	\label{res_table}
\end{table}
\end{center}

\textcolor{red}{\(\lambda\) values in table not right, don`t know with which algorithms they were reached}

23.06.2017\\
found out: for real results, have to do it with the functions I take in the algorithm -> this is hingequad for inner level and either hingeloss or hingequad for outer level\\
from plots it seems that hingequad is closer to the misclassification loss\\
generally: from plots it looks as if all \(\lambda\)s are best =0\\
with bundle bilevel and hingeloss (outer): \(\lambda\) very much depending on starting value -> why??? graphs seem to be monotonously decreasing into 0  \\


\textcolor{red}{plots such as objective function (upper hingeloss and lower hingequad) in bilevel bundle algorithm: No minimum visible (also for ionosphere and cancer???...)\\
\textbf{analysis of every plot:}\\
pima: looks the same as ``old'' plot, minimum is 0\\
wine quality red and red 56: minimum is 0 \\
covtype: same\\
cancer: doesn't really look similar to ``old'' plot, slope different, minimum different; minimum at 0 and not at about 10\\
ionosphere: slope, ect. look similar to ``old'' plot; but minimum at 0 \\
heart: like ``old'' plot \\
maybe cancer and ionosphere plots just ``incidents'' -> because of special choice of validation set????\\
-> no, can also be that I averaged over 20 times...}

\textcolor{red}{\textbf{misclassification loss as upper level objective}\\
pima: average seems to fo to 0 as min \\
generally: misclassification loss seems as if no optimization of \(\lambda\) possible because choice of validation set seems to have much more influence}

\textcolor{red}{Results for\(\lambda\) only if it stayed there after second run with second starting value\\
change in \(\lambda\) has very little effect; only after comma for ``percent-writing''\\
errors like in table for all but ionosphere -> has only 5\% error?; ???-> error the smaller, the smaller \(\lambda\)???\\
pima simply not depending on \(\lambda\)\\
cancer not really depending in \(\lambda\), only if it gets really big \(>1000\) (for \(>10\) minor change)\\
heart: changes a lot for the different \(\lambda\), but best: \(\lambda=0\)\\
seems that results come because of scaling of objective -> consistent with the plots I made\\
also consider: loss function of optimizer is not the one that calculates the test error } 

%\textcolor{red}{interesting: 0 computing time for ionosphere??? - solved now -> vertippt}

Extra table for \(\bm{w}\), \(b\), \(\lambda\) ?

First experiment: Classification

Write down bilevel classification problem and (if needed) which specification of the inexact bundle algorithm is used.

%Write down the sets were used and how they were prepared.











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\textcolor{red}{\textbf{Covtype tests}\\
Datensatz zerteilt: 1000, 5000, 10000, 50000 Datens\"atze\\
Ergebnisse mit Matlab App (linear SVM, 3 folds, parallel used):\\
\begin{tabular}{c|c|c}
Datensatz & Zeit & Fehler \\
\hline
1,000 & 16.22 sek & 34.2\% \\
5,000 & 10.524 sek & 18.3\% \\
10,000 & 14.689 sek & 16.5\% \\
50,000 &  643.57 sek & 16.9\% \\
50,000 (Rechnerhalle, 4 parallel) & 326.83 & 16.9\% \\
100,000 --''-- & 2492 sek = 41.5333 min & 21.3\%
\end{tabular}}

\textcolor{red}{scheint bei 50000 tats\"achlich so lange zu dauern, da ziemlich genau doppelt so schnell bei parallel-Rechnung mit 4 anstatt 2 ``Rechnern'' --> kein Arbeitsspeicher Problem}

\textcolor{red}{Test mit bundle bilevel-Algorithmus:\\
for covtype, ``Hare''-stopping condition\\
\begin{tabular}{c|c|c|c|c|c|c}
trainigs set & starting value & lambda & time & k & inull & Fehler\\
\hline
1,000 & 1 &  & 62.8280 sek & 29 & 0 & \\
1,000 & 86 &  & 130.1105 sek & 61 & 8 & 19\% \\
1,000 (quadprog) & 86 & & 6.5572 sek & 61 & 8 & \\
5,000 qp & 1 &  & 16.2338 sek & 27 & 0 & \\
5,000 qp & 47 &  & 34.9573 sek & 60 & 0 & 14.74\% \\
10,000 qp & 1 &  & 59.7462 sek & 49 & 0 & \\
10,000 qp & 50 &  & 85.0816 sek & 69 & 0 & 15.32\% \\
50,000 qp &  & 588.2828 sek & 45 & 0 & \\
50,000 qp & 60 &  & 897.8123 sek & 69 & 0 & 14.86\% \\
100,000 qp & 1 &  & 1358.7 sek = 22.6455 min & 37 & 0 & 39.41
\end{tabular}
\\
!!!!!!!!!!!did not take \(\lambda\) but the error value!!!!!!!!!!!!!!!\\
Fehler berechnet f\"ur Daten 1001 bis 2000 von komplettem Datensatz\\
Versuch mit Daten 1001-5000: scheint Problem bei matrizenaufbau zu haben \\
Analyse Timer: \(100\%\) der Zeit in postpro-wb-class-hinge-qpas dabei \(100\%\) der Zeit für qpas\\
Ergebnis viel!!! schneller, wenn quadprog und sparse-matrizen, gibt EXAKT! gleiches Ergebnis für fehler\\
geht dann auch mit mehr datens\"atzen (4000) -> Fehler nur 7.75\%??? \\
f\"ur Datensatz \( 5000 \to\) Testdaten 5001 bis 10000 \\
exemplarisch getestet: wie gross ist der Unterschied bei ergebnisse bei verschiedenen Starwerten? - bei 10000: \(1e-15\) , bei 50000: exact\\
f\"ur Datensatz 10000: Testdaten 10001 bis 15000 \\
50000: deutlicher Anstieg der Rechenzeit merkbar irgendwo zwischen 10000 und 50000 muss eine Schwelle liegen - Speicher? - ab dieser Matrix Gr\"osse gibt matlab fehler wenn nicht sparse matrizen explizit erstellt werden sollen (zu viel Speicher) unwahrscheinlich - siehe erkl\"arung App \\
komish: warum dauert es bei n\"aherem Startwert so viel l\"anger??? \\
unconstrained tested for 1000: much faster; no difference in steps for \(x_0 = 1\), 70, 6 for \(x_0=86\), 7.2 sek}

\textcolor{red}{\textbf{0815-Funktion f\"ur bilevel optimierung:}\\
für covtype1000: in sehr wenig Zeit: lambda = 100 -> selbes ergebnis im Fehler: 19\%\\
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textcolor{orange}{\textbf{Infos on Data sets} \\
\begin{tabular}{c|c|c|c|c|c|c}
Data Set & instances, attributes & 1/C (SVC) & C, \(\varepsilon\) (SVR) & Test Error & Source & Comment \\
\hline
Adult & ~300 000, 10+1 & & 0.017, 0.19 & 24.99\% & \cite{Musicant2004} & think linear, 25\% occurred more often \cite{Musicant2000} \\
Boston Housing & 506, 12+1 & & 0.25, 0.015 & 19.44\% & \cite{Musicant2004} & think linear\\
Adult & Tset: 11221 & 1/0.05 & & & \cite{Platt1999} & linear \\
Adult & & \(>10^8\) & & 14.1 & \cite{Kao2004} & accuracy and C do not belong together directly
\end{tabular}\\
If more values found: take best}













