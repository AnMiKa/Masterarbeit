\section{Application to Model Selection for Primal SVM}

\subsection{Introduction}

In this chapter the nonconvex inexact bundle algorithm is applied to the problem of model selection for support vector machines (SVM) solving classification tasks.
It relies on a bilevel formulation proposed by Kunapuli and Moore et al. in \cite{Kunapuli2008} and \cite{Moore2011}.

A natural application for the inexact bundle algorithm is an optimization problem where the objective function value can only be computed iteratively. This is for example the case in bilevel optimization.
%If this objective function is iteratively optimized, the function value i never exact. But by the stopping condition it is (often???) possible to have the needed error bound on the function value (what about the gradient???). (Comment on asymptotical exactness?)

%A general bilevel problem can be formulated as 
%Bilevel optimization deals with a class of constrained optimization problems where one or more of the optimization variables are again constrained by optimization problems.
A general bilevel program can be formulated as \cite{Kunapuli2008}
\begin{equation}
	\begin{array}{cll}
	\displaystyle\max_{x \in X, y} & F(x,y) & \text{upper level} \vspace{0.5ex}\\
	\text{s.t.} & G(x,y) \leq 0 & \vspace{1em}\\
	& y \in \begin{Bmatrix} \displaystyle\argmax_{y\in Y} & f(x,y) \vspace{0.5ex}\\
	                        \text{s.t.} & g(x,y) \leq 0 
													\end{Bmatrix}. & \text{lower level}
	\end{array}
\end{equation}

It consists of an \emph{upper} or \emph{outer level} which is the overall function to be optimized. Contrary to usual constrained optimization problems which are constrained by explicitly given equalities and inequalities a  bilevel program is additionally constrained to a second optimization problem, the \emph{lower} or \emph{inner level} problem.

Solving bilevel problems can be divided roughly in two classes: implicit and explicit solution methods.
In the explicit methods the lower level problem is usually rewritten by its KKT conditions and the upper and lower level are solved simultaneously. For the setting of model selection for support vector machines as it is used here, this method is described in detail in \cite{Kunapuli2008}.

The second approach is the implicit one. Here the lower level problem is solved directly in every iteration of the outer optimization algorithm and the solution is plugged into the upper level objective.
Obviously if the inner level problem is solved numerically, the solution cannot be exact. Additionally the \emph{solution map} \(S(x) = \{y \in \R^k | y \text{ solves the lower level problem}\}\) is often nondifferentiable \cite{Outrata1998} and since elements of  the solution map are plugged into the outer level objective function in the implicit approach, the outer level function becomes nonsmooth itself. \\
This is why the inexact bundle algorithm seems a natural choice to tackle these bilevel problems. \\ 
Moore et al. use the implicit approach in \cite{Moore2011} for support vector regression. However they use a gradient decent method which is not guaranteed to stop at an optimal solution.
In \cite{Moore2010a} he also suggests the nonconvex exact bundle algorithm of Fuduli et al. \cite{Fuduli2004a} for solving the bilevel regression problem. This allows for nonsmooth inner problems and can theoretically solve some of the issues of the gradient descent method. It ignores however, that the objective function values can only be calculated approximately. A fact which is not addressed in Fuduli's algorithm.

%\subsection{Notation and Expressions}

%training set, validation /holdout set...

\subsection{Introduction to Support Vector Machines}
%In times of big data machine learning is a very active field of research. 
Support vector machines are linear learning machines that were developed in the 90's by Vapnik and co-workers. Soon they could outperform several other programs in this area \cite{Cristianini2000} and the subsequent interest in SVMs lead to a very versatile application of these machines \cite{Kunapuli2008}.

The case that is considered here is support vector classification using supervised learning.
In classification data from a possibly high dimensional vector space \(\tilde{X} \subseteq \R^n\), the \emph{feature} or \emph{input space} is divided into two classes. These lie in the \emph{output domain} \(\tilde{Y} = \{-1,1\}\) \cite{Cristianini2000}. 
Supervised learning is a kind of machine learning task where the machine is given examples of input data with associated labels, the so called \emph{training data}.
The goal of such a machine learning task is to find a mapping that predicts the output given unlabeled input as good as possible\cite{Cristianini2000}. 

 


\subsection{Explanation Bilevel Approach and Inexact Bundle Method}
%Check if regression case also included?????
The parameter in the objective function of the classification problem has to be set before hand. This step is part of the model selection process (citation)
goal: set this parameter optimally
A very intuitive and widely used approach: grid search (description) --> very costly, discrete parameter choice, not practicable in case of many parameter
A more recent approach is the formulation as a bilevel problem used in \cite{Kunapuli2008, Moore2011}.




\subsection{Numerical Experiments}

The bilevel-bundle algorithm for classification was tested for four different data sets taken from the UCI Machine Learning Repository \emph{citations as said in ``names'' data??? }.
For comparability with the already existing results presented in \cite{Kunapuli2008} the following data and specifications of it were taken:

\emph{Table like in Kunapuli}
\begin{table}%
	\begin{tabular}{lcccc}
		\hline
    Data set & \(l_{train}\) & \(l_{test}\) & n & T \\
		\hline
		Pima Indians Diabetes Database & 240 & 528 & 8 & 3 \\
		Wisconsin Breast Cancer Database & 240 & 443 & 9 & 3 \\
		Cleveland Heart Disease Database & 216 & 81 & 13 & 3 \\
		John Hopkins University Ionosphere Database & 240 & 111 & 33 & 3

	\end{tabular}
	\caption{}
	\label{}
\end{table}

As described in the Phd theseis the data was first standardized to unit mean and zero variance (\emph{not the 0,1 column in ? dataset}). The bilevel problem with cross validation was executed 20 times to get averaged results.
The results are compared by cross validation error, test error -> write which error this is and computation time.
Additionally write w, b, \(\lambda\) ???
The objective function was scaled by 100. -> also test error (to get percentage) 

Table ??? shows the results 

\begin{table}%
	\begin{tabular}{lllll}
		\hline
		Data set & Method & CV Error & Test Error & Time (sec.) \\
		\hline
		\texttt{pima} & hingequad & \(60.72 \pm 9.56\) & \(24.11\pm 2.71\) & \(2.15 \pm 0.52\)\\
		 &              hinge loss & & & \\
		\texttt{cancer} &  hingequad & \(10.75\pm 7.52\) & \(3.41 \pm 1.16\) & \(3.43 \pm 28.84\) \\
		 &              hinge loss & & & \\
		\texttt{heart} &  hingequad & \(48.73 \pm 5.53\) & \(15.56 \pm 4.44\) & \(3.43 \pm 43.39\)\\
		 &              hinge loss & & & \\
		\texttt{ionosphere} &  hingequad & \(39.30 \pm 5.32\) & \(12.21 \pm 4.10\) & \(14.17 \pm 51.27\)\\
		 &              hinge loss & & & \\
	\end{tabular}
	\caption{}
	\label{res_table}
\end{table}

\textcolor{red}{interesting: 0 computing time for ionosphere???}

Extra table for w, b, \(\lambda\) ?

First experiment: Classification

Write down bilevel classification problem and (if needed) which specification of the inexact bundle algorithm is used.

Write down the sets were used and how they were prepared.



