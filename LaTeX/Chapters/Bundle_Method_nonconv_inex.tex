\section{Proximal Bundle Method for Nonconvex Functions with Inexact Information}
\label{sec_nonconv_inex}

%\textcolor{blue}{introduction}

This chapter focuses on the proximal bundle method presented by Hare et al. in \cite{Hare2016}.
The idea is to extend the basic bundle algorithm for nonconvex functions with both inexact function and subgradient information.
The key idea of the algorithm is the one already developed by Hare and Sagastizábal in \cite{Hare2010}: When dealing with nonconvex functions a very critical difference to the convex case is that the linearization errors are not necessarily nonnegative any more. To tackle this problem the errors are manipulated to enforce nonnegativity. In this case this is done my modeling not the objective function directly but a convexified version of it.

\subsection{Derivation of the Method}
%\textcolor{blue}{``assumptions and notations''}

%\textcolor{red}{introduce exact optimization problem that is used in this section and its properties if not already introduce in ``Preliminaries''.}

Throughout this chapter we consider the optimization problem

\begin{equation}
	\min_{x} f(x) \quad \text{s.t.} \quad x \in X.
\label{opt_prob_nonconv_inex}
\end{equation}

The objective function \(f:\R^n \to \R\) is locally Lipschitz  and (subdifferentially) regular. \(X \subset \R^n\) is assumed to be a convex compact set. 

%\begin{definition} \cite{Rockafellar2009}
	%A function \(f:\R^n \to \bar{\R} = [-\infty, + \infty]\) is called \emph{proper} if \(f(x) < \infty\) for at least one \(x\in \R^n\) and \(f(x) > \infty ~ \forall x \in \R^n\).
%\end{definition}

\begin{definition} \cite[Theorem 7.25, p. 260]{Rockafellar2009}
	\(f: \R^n \to {\R}\) is called \emph{subdifferentially regular} at \(\bar{x} \in \R^n\) if %\(f(\bar{x})\) is finite and 
	the epigraph
	\[epi(f) := \left\{(x, \alpha) \in \R^n\times \R \mid \alpha \geq f(x) \right\}\]
	is Clarke regular at \(\bar{x},f(\bar{x})\).
\end{definition}

Clarke regularity basicly means, that epigraph of a function \(f\) does not have any 'inward' corners. An exact definition is given in \cite[Definition 6.4, p. 199]{Rockafellar2009}.

\subsubsection{Inexactness}

It is assumed that both the function value as well as one element of the subdifferential can be provided in an inexact form.
For the function value inexactness is defined straight forwardly: If 
\begin{equation*}
	\|f_x - f(x)\| \leq \sigma_x,
\label{fun_inex}
\end{equation*}

then \(f_x\) approximates the value \(f(x)\) within \(\sigma_x\). This is slightly different from the definition in (\ref{conv_inexactness_1}). In the convex case it follows from (\ref{sig_p_thet_subgr}) that \( \bar{\sigma} \geq \sigma_x \geq -\theta_x \geq - \bar{\theta}\) and therefore \(f_x \in [f(x)-\bar{\theta},f(x)+\bar{\sigma}]\) for the overall error bounds \(\bar{\sigma}\) and \(\bar{\theta}\).

As the 'normal' \(\varepsilon\)-subdifferential is not defined for nonconvex functions we adopt the notation used in \cite{Noll2013} and interpret inexactness in the following way: \(g \in \R^n\) approximates a subgradient  of \( \partial f(x)\) within \(\theta \geq 0\) if
\begin{equation*}
	g_x \in \partial f(x) + B_{\theta}(0) =: \partial_{[\theta]}f(x)
\label{subgr_inex}
\end{equation*}

where \(\partial f(x)\) is the Clarke subdifferential %\textcolor{red}{defined in Preliminaries section}
of \(f\).

The given definition of the inexactness can be motivated by the relation 
	\[ g_x \in \partial_{[\theta]}f(x)\Leftrightarrow g_x \in \partial(f+\theta\|\cdot-x\|)(x) \]
noticed in \cite{Treiman1986}. It means that the approximation of the subgradient of \(f(x)\) is an exact subgradient of a small perturbation of \(f\) at \(x\).
The set \(\partial_{[\varepsilon]}f(x)\) is also known as the Fréchet \(\varepsilon\)-subdifferential of \(f(x)\).

%\begin{definition}\cite{Jofre1998}
	%Let \(f:\R^n \to \R\), \(\varepsilon\) > 0. The \emph{Fréchet \(\varepsilon\)-subdifferential} of \(f\) at \(x\) is defined by 
	%\[ \partial_{[\varepsilon]}f(x) := \left\{g \in \R^n | \liminf_{\|h\| \to 0} \frac{f(x+h)-f(x)-\langle g, h\rangle}{\|h\|} \geq -\varepsilon \right\}. \]
%\end{definition}


\begin{remark}
For convex objective functions this approximate subdifferential does \emph{not} equal the usual convex \(\varepsilon\)-subdifferential. The two can however be related via

\[ \partial_{\theta}f(x) \subset \partial_{[\theta']}f(x) \]
 
for a suitable \(\theta'\). Generally an explicit relation between \(\theta\) and \(\theta'\) is hard to find \cite[p. 558]{Noll2013}.
\end{remark}

Like in the paper it is assumed that the errors are bounded although the bound does not have to be known:

\begin{equation}
	|\sigma_j| \leq \bar{\sigma}, \bar{\sigma} > 0 \quad \text{and} \quad 0 \leq \theta_j \leq \bar{\theta} \quad \forall j \in J^k \text{ and } \forall k.
\label{err_bound}
\end{equation}

%In the context of inexact information it is important to make a distinction between the (unknown) exact function value and its approximation. Throughout this section we therefore write \(f(x^j)\) for the exact function value  whereas the approximation will be written as \(f_j\)  or \(\hat{f}_k\) for the approximation at the current stability center.
For ease of notation we write from now on \(f_j\) and \(g_j\) instead of \(f_{x^j}\) and \(g_{x^j}\) for the approximation of the function value and subgradient at the \(j\)'th iterate in the bundle \(J^k\). The approximation at the \(k\)'th stability center reads \(\hat{f}_k\).

\begin{remark}
	Before the exact subgradient was denoted with \(g_j\). Here the same notation is used for the approximate subgradient.
	We leave this double notation for the sake of readability and remark that for the remainder of this thesis \(g_j\) denotes the approximate subgradient. In the few situations, where the exact gradient is needed, it is marked clearly.
\end{remark}


%\textcolor{red}{remark to subgradient notation\\
%sensible in view of optimality condition \(\to\)
%\[  \] 
%this means that perturbation of \(f\) is critical at \(x\) \\
%for convex functions the two subdifferentials can be related
%\[ \partial_{\theta}f(x) \subset \partial_{[\theta']}f(x) \]
%for convex \(f\) set  \(\partial_{[\theta]} f(x)\) coincides with Fréchet \(\varepsilon\)-subdifferential \\
%subdifferential in Hare-Paper:
%\[ \partial f(\bar{x}) = \left\{g \in \R^n | \lim_{x \to \bar{x}} \inf_{x \neq \bar{x}} \frac{f(x)-f(\bar{x})-\langle g, x - \bar{x}\rangle}{\|x - \bar{x}\|} \geq 0 \right\} \]
%Fréchet-Subdifferential in \cite{Kruger2003}
%\[ \partial f(\bar{x}) = \left\{g \in \R^n | \liminf_{x \to \bar{x}} \frac{f(x)-f(\bar{x})-\langle g, x - \bar{x}\rangle}{\|x - \bar{x}\|} \geq 0 \right\} \]
%said to be the same as \emph{regular subdifferential} in \cite{Rockafellar2009}
%\[ \hat{\partial}f(\bar{x}) = \left\{ v \in \R^n | \liminf_{\substack{x \to \bar{x} \\x \neq \bar{x}}} \frac{f(x)-f(\bar{x})- \langle v, x - \bar{x}\rangle}{\|x-\bar{x}\|} \geq 0\right\} \]
%\(\Rightarrow\) all the same \\
%\cite{Jofre1998} defines Fréchet \(\varepsilon\)-subdifferential for \emph{lower semicontinuous} functions as
%\[ \partial^{F}_{\varepsilon}f(x) = \left\{g \in \R^n | \liminf_{\|h\| \to 0} \frac{f(x+h)-f(x)-\langle g,h \rangle}{\|h\|} \geq -\varepsilon \right\} \]
%also says \(f\) convex \(to\) limiting Fréchet \(\varepsilon\)-subdifferential is exactly \(\varepsilon\)-enlargement of Frenchel subdifferential.\\
%Treiman \cite{Treiman1986}: \(g \in \partial^F_{\varepsilon}f(x) \Leftrightarrow g \in \partial^F(f+\varepsilon\|\cdot-x\|)(x)\)\\
%\(f\) convex \(\to\)
%\[ \partial^{F}_{\varepsilon}f(x) = \left\{g \in \R^n | \frac{f(x+h)-f(x)-\langle g,h \rangle}{\|h\|} \geq -\varepsilon, \forall h \in \R^n\right\}  \]
%``normal'' \(\varepsilon\)-subdifferential (VL Ulbrich, \cite{Hiriart-Urruty1993})
%\[ \partial_{\varepsilon}f(x) = \left\{g \in \R^n | f(y) -f(x) -\langle g, y-x\rangle \geq - \varepsilon, \forall y \in \R^n \right\} \]
%in Rockafellar: \(\bar{\partial}f(x)\) is generalization of Clarke subdifferential.\\
%in case of Lipschitz contiuity: convex hull of limiting proximal subgradients\\
%\[\bar{\partial} f(\bar{x})= \left\{v| (v,-1) \in \bar{N}_{\text{epi }f}(\bar{x},f(\bar{x}))\right\}\]
%\(f\) subdifferentially regular \(\Rightarrow\) \(\bar{\partial}f(\bar{x}) = \partial f(\bar{x})\) \\
%8.30: \(f\) subdifferentially regular: \(\Rightarrow\) \(\partial f(\bar{x}) = \left\{v|\langle v,w \rangle \leq \text{d}f(\bar{x})(w),  \forall w\right\}\)\(\to\) 8.4 same for \(\hat{\partial}f(x)\)\\
%8.9\\
%\(\text{d}f(\bar{x})(\bar{w}) = \liminf_{\substack{\tau \searrow 0 \\ w \to \bar{w}}}\frac{f(\bar{x}+\tau w)-f(\bar{x})}{\tau}\)\\
%8.11: subdifferentially regular functions \(\partial f(x) = \hat{\partial}f(x)\) \\
%\(\partial f(x) \) is a general subgradient (8.3)}

%\textcolor{red}{Closed convex sets are Clarke regular, so in particular the epigraph of \textcolor{red}{lower \(\mathcal{C}^2\)-functions?}.}

%\textcolor{red}{Definition semismooth for later:} \\
%\begin{definition} % as in \cite{Mifflin1977}
	%A function \(f: \R^n \to \R\) is called \emph{semismooth} ar \(x\in \R^n\) if \(f\) is Lipschitz near \(x\) and for each \(d \in \R^n\) and for any sequences \(\{t_k\} \subseteq \R_{+}, \{\theta^k\} \subseteq \R^n\) and \(\{g^k\} \subseteq \R^n\) such that 
	%\[ \{t_k\} \downarrow 0, \quad \{\theta^k/t_k\} \to 0 \in \R^n \quad \text{and} \quad g^k \in \partial f (x+t_kd+\theta^k), \]
	%the sequence \(\{\langle g^k, d \rangle\}\) has exactly one accumulation point.
%\end{definition}

%\begin{definition} % as in \cite{Haarala2004a}
%A point \(x \in \R^n\) that satisfies \(0 \in \partial f(x)\) is called a \emph{stationary point} of \(f\).
%\end{definition}

%\textcolor{blue}{explanation}

\subsubsection{Nonconvexity}

A main issue both nonconvexity and inexactness entail is that the linearization errors \(e_j^k\) are not necessarily nonnegative any more.
So based on the results in \cite{Hare2009} not the objective function but a convexified version of it is modeled as the objective function of the subproblem.
%\textcolor{red}{explain locally convexified more precisely? Is it because no global convexification? different than in [18]??}\\

%When looking at the subproblem formulated as in (\ref{sub_prob_long}) one can see that the new iterate \(x^{k+1}\) is in fact a \emph{proximal point} of the subproblem.\\
%The \emph{proximal point mapping} or \emph{prox-operator} is defined as
%\begin{equation}
	%prox_{t,f}(x) = \argmin_{y}\left\{\check{f}(y) + \frac{1}{2t}\|x-y\|^2\right\}, \quad t > 0
%%\label{prox_op}
%\end{equation}

%For \(\check{f}(x) := m_(x)+\mathbb{I}_X(x)\) and \(\mu:=\frac{1}{t_k}\) this is just subproblem (\ref{sub_prob_long}) with the constraint \(x\in X\) incorporated in the objective function. Because of this special form of the subproblems primal bundle methods are also called proximal bundle methods.\\

As already pointed out in section \ref{subsec_prox_op} the bundle subproblem can be formulated by means of the prox-operator (\ref{prox_op}).

%\textcolor{red}{explain in much more detail when read about calculation of proximal points for nonconvex functions. At the moment just main ideas.}

The idea is to use the relation
\begin{equation*}
	prox_{T = \frac{1}{\eta}+t, f}(x) = prox_{t,f+\eta/2\|\cdot - x\|^2}(x).
\label{prox_relation}
\end{equation*}

This means, that the proximal point of the function \(f\) for the parameter \(T=\frac{1}{\eta}+t\), \(\eta, t > 0\), is the same as the one of the convexified function 

\begin{equation}
	\tilde{f}(y) = f(y) + \frac{\eta}{2}\|y-x\|^2
\label{conv_obj}
\end{equation}

with respect to the parameter \(t\) \cite{Hare2010}. The parameter \(\eta\) is therefore also called the \emph{convexification parameter} and \(t\) the \emph{prox-parameter}.

%\textcolor{red}{say why/how... this is related to current stability center}
%\textcolor{red}{Because new function to be approximated, subgradients ``new'':}
The main difference of the method in \cite{Hare2016} to the basic bundle algorithm \ref{sec_basic_bundle}.1 is that the function that is modeled by the cutting plane model is no longer the original objective function \(f\) but the convexified version \(\tilde{f}\). This results in the following changes:

In addition to downshifting the linear functions forming the model  they have a tilted slope.
This is because instead of subgradients of the original objective \(f\) subgradients of the function \(\tilde{f}\) are taken. We call them \emph{augmented subgradients}.
At the iterate \(x^j\) such a subgradient is given by

\begin{equation}
	s^k_j := g^j + \eta_k \left(x^j-\hat{x}^k\right).
	\label{aug_subgr}
\end{equation}

%Additionally they are shifted downwards to keep the linearization error nonnegative
Downshifting is done in a way that keeps the linearization error nonnegative.
The \emph{augmented linearization error} is therefore defined as 

\begin{equation}
	0 \leq c^k_j := e^k_j + b_j^k, \quad \text{with} \quad \left\{ \begin{array}{l} e_j^k := \hat{f}_k - f_j - \Langle g^j, \hat{x}^k-x^j\Rangle	 \vspace{1ex} \\
	b_j^k := \frac{\eta_k}{2}\|x^j-\hat{x}^k\|^2 \end{array}\right.
\label{aug_lin_err}
\end{equation}

and 

\begin{equation*}
	\eta_k \geq \max\left\{\max_{j \in J_k, x^j \neq \hat{x}^k}{ \frac{-2e_j^k}{\|x^j-\hat{x}^k\|^2}}, 0 \right\} + \gamma. 
\label{eta}
\end{equation*}

The parameter \(\gamma \geq 0\) is a safeguarding parameter to keep the calculations numerically stable.


The new model function can therefore be written as
\begin{equation}
  M_k(\hat{x}^k +d) := \hat{f}_k + \max_{j \in J_k} \left\{\Langle s^k_j,d\Rangle -c^k_j \right\}.
\label{aug_model}
\end{equation}

At the proximal center \(\hat{x}^k\) holds \(	M_k(\hat{x}^k) = \hat{f}_k\) for all \( k\) by the fact that then \(d = 0\) and \(c^k_j = 0\). 

\subsubsection{Aggregate Objects}

The definitions of the \emph{augmented aggregate subgradient} \(S^k\),  \emph{error} \(C_k\) and \emph{linearization} \(A_k\) follow straightforwardly from the KKT-conditions. Again \(\alpha_j^k, j \in J^k\) denote the Lagrangian multipliers of the subproblem.

\begin{align}
	&S^k := \sum_{j \in J_k} \alpha_j^k s_j^k \label{aug_agg_subgr}, \\
	&C_k := \sum_{j \in J_k}{\alpha_j^k c_j^k} \text{ and } \\%\label{aug_agg_err} 
	& A_k(\hat{x}^k+d) := M_k(x^{k+1})+\left\langle S^k, d-d^k \right\rangle \label{aug_agg_lin}.
\end{align}

The model decrease is
\begin{equation}
	\delta^k := C_k + t_k \|S^k + \nu^k\|^2 = C_k+\frac{1}{t_k}\|
	d^k\|^2,
\label{mod_dec2}
\end{equation}

which contains the normal vector 
\begin{equation}
	\nu^k \in \partial\mathtt{i}_X(x^{k+1})
	\label{nu}
\end{equation}
of the constraint set \(X\).

The second formulation in (\ref{mod_dec2}) follows from the relation \(d^k = -t_k(S^k+\nu^k)\) which itself comes from the KKT-conditions.

By the same argumentation as for (\ref{agg_err}) the KKT conditions also reveal another useful characterization of the augmented aggregate linearization error:

\begin{equation}
	C_k = \hat{f}_k - M_k(x^{k+1})+\Langle S^k, d^k \Rangle.
\label{aug_agg_err2}
\end{equation}

As the model function \(M_k\) is convex even for nonconvex objective functions it is still minorized by the aggregate linearization. It holds 

\begin{equation}
	A_k(\hat{x}^k+d) \leq M_k(\hat{x}^k+d) \quad \forall d \in \R^n.
\label{A_leq_M}
\end{equation}

The update of \(t_k\) can be done in the same way as described in (\ref{t_plus}) and (\ref{t_minus}) for the basic bundle method. Similarly  the methods to update the bundle index set \(J^k\) stay valid.
The update conditions (\ref{model_update_1}) and (\ref{model_update_2}) for the model are now written with respect to the augmented aggregate linearization and the approximate function value \(\hat{f}_{k+1}\).

\begin{align}
	&M_{k+1}(\hat{x}^k+d) \geq \hat{f}_{k+1}-c^{k+1}_{k+1}+\left\langle s^{k+1},d \right\rangle, \quad \forall d \in \R^n \text{ and} \label{Model_update_1}\\
	&M_{k+1}(\hat{x}^k+d) \geq A_k(\hat{x}^k+d), \quad \forall d \in \R^n.
	\label{Model_update_2}
\end{align}

A bundle algorithm that deals with nonconvexity and inexact function and subgradient information can now be stated.

%\textcolor{blue}{algorithm}


\begin{minipage}{\linewidth}

\vspace{1em}

\hrule  \vspace{0.4ex} \hrule
\vspace{1ex}
\textbf{Algorithm \ref{sec_nonconv_inex}.1: Nonconvex Proximal Bundle Method with Inexact Information}
\vspace{1ex}
\hrule
\vspace{1ex}
Select parameters \( m \in (0,1), \gamma > 0 \) and a stopping tolerance \( \mathtt{tol} \geq 0\). Choose a starting point \(x^1 \in \R^n\) and compute \(f_1\) and \(g^1\). Set the initial index set \(J_1:=\{1\}\) and the initial prox-center to \(\hat{x}^1 := x^1\). Set \(\hat{f}_1 = f_1\), \(s^1_1 = g^1\) and select \(t_1 > 0\). Initialize \(c^1_1 = 0\) and \(M_1(\hat{x}^1+d)=\hat{f}_1+\Langle s^1_1,d\Rangle\).
\end{minipage}

For \(k = 1,2,3,  \dotsc \)   

\begin{enumerate}
	\item Calculate 
	\begin{equation}
		d^k = \argmin_{d \in \R^n} \left\{ M_k(\hat{x}^k+d)+\mathtt{i}_X(\hat{x}^k+d)+\frac{1}{2t_k}\|d\|^2\right\}.
		\label{bundle_subprob_noncnov_inex}
	\end{equation}
	
	\item Set
		\begin{align*} 
		  G^k &= \sum_{j \in J_k}{\alpha_j^k s_j^k} \\%, \quad \nu^k \in \partial \mathtt{i}_{X}(x^{k+1}) \\
			C_k &= \sum_{j \in J_k}{\alpha_j^k c_j^k} \text{ and} \\
	    \delta_k &=  C_k + \frac{1}{t_k}\|d^k\|^2.
		\end{align*}
		If \(\delta_k \leq \mathtt{tol} \rightarrow \) STOP.
	\item Set \( x^{k+1} = \hat{x}^k + d^k \).
	\item Compute \(f^{k+1}, g^{k+1}\). \\
	If \(f^{k+1} \leq \hat{f}^k - m\delta_k \quad \rightarrow\) serious step: \\
	\noindent\hspace*{2em}%
	Set \(\hat{x}^{k+1} = x^{k+1}, \hat{f}^{k+1} = f^{k+1}\) and select \(t_{k+1} > 0\). \\
	Otherwise \(\quad \rightarrow\) nullstep: \\
	\noindent\hspace*{2em}%
	Set \(\hat{x}^{k+1} = \hat{x}^k, \hat{f}^{k+1}=\hat{f}^{k}\) and choose \(0 < t_{k+1} \leq t_k\). 	
	\item Select new bundle index set \(J_{k+1}\), calculate 
	\[ \eta_{k+1} = \max{\left\{\max_{j \in J_{k+1}, x^j \neq \hat{x}^{k+1}}{\frac{-2e_j^{k+1}}{|x^j - \hat{x}^{k+1}|^2}, 0}\right\}}+\gamma  \]
	and \(c^{k+1}_j\) by (\ref{aug_lin_err}) for all \(j \in J^{k+1}\).
	Update the model \(M_{k+1}\) such that conditions (\ref{Model_update_1}) and (\ref{Model_update_2}) are fulfilled.
\end{enumerate}
\vspace{1ex}
\hrule

\vspace{1.5em}

\subsection{On Different Convergence Results} \label{On_diff_conv_res}

In terms of usability of the described algorithm it is interesting to see if stronger convergence results are possible if additional assumptions are put on the objective function. This is investigated in the following section.

\subsubsection{The Constraint Set}

%The method above can handle nonconvex objective functions. It is not explicitly assumed, that the function taken has a finite minimum. Therefore
The constraint set \(X\) ensures the boundedness of the sequence \(\{\hat{x}^k\}\).
This is not necessary if the objective function is assumed to have bounded level sets \(\{x \in \R^n \mid f(x) \leq f(\hat{x}^1)\}\), an assumption commonly  used when optimizing nonconvex functions.
As the objective function is assumed to be continuous bounded level sets are compact. Additionally the descent test ensures that \(f(\hat{x}^{k+1}) \leq f(\hat{x}^k)\) for all \(k\). The proof holds therefore in the same way as with the set \(X\).

%Another possibility is to bound the step sizes \(t_k\) also from above. Then the sequence \(\{\hat{x}^k\}\) stays bounded and the proof still holds.
In \cite{Oliveira2014} another stopping criterion is proposed that ensures convergence even for unbounded sequences \(\{\hat{x}^k\}\).
%\textcolor{red}{Is this also possible in my case or only for convex???}

\subsubsection{Exact Information and Vanishing Errors}

As the presented algorithm was originally designed for nonconvex objective functions where function values as well as subgradients are available in an exact manner, all convergence results stay the same with the error bounds \(\bar{\sigma} = \bar{\theta} = 0\).
As already indicated previously this is the case because inexactness can be seen as a kind of nonconvexity and no additional concepts had to be added to the method when generalizing it to the inexact setting.

If we additionally require the objective function to be lower-\(\mathcal{C}^2\) it can be proven that the sequence \(\{\eta_k\}\) is bounded \cite[Lemma 3, p. 2454]{Hare2010}.
This is not possible in the case of inexact information even for convex objective functions (see example in \cite[p. 22]{Hare2016}).

For asymptotically vanishing errors, meaning \(\lim_{k \to \infty} \sigma_k = 0 \) and \(\lim_{k \to \infty} \theta_k = 0 \) the convergence theory holds equally well with error bounds \(\bar{\sigma} = \bar{\theta} = 0\) in \cite[Lemma 5, p. 11]{Hare2016}. Still it is difficult if not impossible to show that the sequence \(\{\eta_k\}\) is bounded without further assumptions.
Under the assumption that \(f\) is lower-\(\mathcal{C}^2\) and some continuity bounds on the errors
\[ \frac{|\sigma_j-\hat{\sigma}_k|}{\|x^j-\hat{x}^k\|^2} \leq L_{\sigma}, \qquad \frac{\theta_j}{\|x^j-\hat{x}^k\|} \leq L_{\theta} \quad \forall k \text{ and } \forall j \in J_k\]
boundedness of the sequence \(\{\eta_k\}\) can be shown. The question remains however if those assumptions are possible to be assured in practice.

%\textcolor{red}{remark on \(\eta_k\)? how does it behave in my applications???}


%It can be shown however that under the condition that the error on the function value at the centers is Lipschitz continuous and the objective function is lower-\(\mathcal{C}^2\) the sequence \(\{\eta_k\}\) is  bounded.
%
%Let
%\begin{equation}
	%\frac{|\sigma_j-\hat{\sigma}_k|}{\|x^j-\hat{x}^k\|} \leq L_{\sigma}.
	%\label{Lip_error}
%\end{equation}
 %Then from (\ref{eta}) follows that 
%\begin{align}
	%\eta_k &= \max \left\{ \max_{\substack{j \in J^k \\ x^j \neq \hat{x}^k}} \frac{-2e^k_j}{\|x^j-\hat{x}^k\|^2},0\right\} +\gamma \\
	%&\leq \max \left\{ \max_{\substack{j \in J^k \\ x^j \neq \hat{x}^k}} \frac{-2\left(f(\hat{x}^k)+\hat{\sigma}_k-f(x^j)-\sigma_j-\langle \tilde{g}^j,\hat{x}^k-x^j\rangle-\langle p^j,\hat{x}^k-x^j \rangle \right)}{\|x^j-\hat{x}^k\|^2},0\right\} +\gamma \\
	%&\leq 2 (\frac{L_{\sigma} + 2L_{f}+\bar{\theta}}{\|x^{j^*}-\hat{x}^k\|})+\gamma
%\end{align}
%
%where \(j^* \in J^k\) yields the maximum of the expression for all \(j \in J^k\).
%Due to the local Lipschitz property of \(f\) and the fact that all iterates \(x^j \in X\). From \cite[Proposition 2.1.2]{Clarke1990} and the decomposition \(g^j = \tilde{g}^j+p^j\), \(\tilde{g}^j \in \partial_f(x^j)\) and \(p \in \mathbb{B}_{\theta_j}(0)\) follows \(\|g^j\| \leq L{f}+\theta_j \leq L_{f}+\bar{\theta}\).
%It is possible to satisfy assumption (\ref{Lip_error}) if it the accuracy with which each \(f\) is calculated is known. For example if \(f\) is the result of an optimization process where the exactness of the output can be controlled by the stopping tolerance.
%
%The above calculations for the boundedness of \(\{\eta_k\}\) still hold in case of exact function values but inexact subgradients by just setting \(\sigma_j=\bar{\sigma}=0\) for all \(j \in J\).

%\textcolor{red}{Problem: \(\{\eta_k\}\) seems to stay bounded even if objective is not lower-\(\mathcal{C}^2\) and therefore convexified function never gets convex \(\to\) no! have \(\|...\|^2\) then not bounded.\\
%for lower-\(\mathcal{C}^2\): \(\exists t_k\) such that \(f(x)+\frac{1}{2t_k}\|x-\hat{x}^k\|^2\) is convex; then ``exact'' part of the linearization error is always positive; \(\to\) then ``quadratic Lipschitz'' bound on \(\sigma\) needed and Lipschitz continuity on \(\theta\)\\
%makes maybe not so much sense then }

\subsubsection{Convex Objective Functions}

An obvious gain when working with convex objective functions is that the approximate stationarity condition of \cite[Lemma 5 (iii), p. 12]{Hare2016} is then an approximate optimality condition.
If one takes the error definitions (\ref{conv_inexactness_1}) and (\ref{conv_inexactness_2}) that are available in the convex case
and assumes \(X = \R^n\), statement (22) in \cite[p. 12]{Hare2016} therefore means that 

%0 is in the \(\bar{\sigma}+\bar{\theta}\)-subdifferential of \(f(\bar{x})\).

\[ 0 \in \partial_{\bar{\sigma}+\bar{\theta}}f(\bar{x}). \]

Thus \(\bar{x}\) is \((\bar{\sigma}+\bar{\theta})\)-optimal.

This follows from the definition of \(S^k\) in (\ref{aug_agg_subgr}) and local Lipschitz continuity of the \(\varepsilon\)-subdifferential \cite[Proposition 12.68, p. 573]{Rockafellar2009}.

%...as laid down previously the definition of an approximate subgradient does not coincide with the usual \(\varepsilon\)-subdifferential for convex functions.
%\textcolor{red}{both definitions used for interpretation of last statement of Lemma 5.}
%It is therefore assumed that for convex objective functions the approximate subgradient \(g^j\) is taken from the \(\theta_j\)-subdifferential of \(f_j\)with \(\theta_j < \bar{\theta}\) for all \(j \in J\) as defined in (\ref{conv_inexactness_1}) and (\ref{conv_inexactness_2}).

%If \(f\) is convex and \(D = \R^n\) the last statement of \cite[Lemma 5]{Hare2016} states that if \(\{\hat{x}^k\} \to \bar{x}\) for \(K\ni k \to \infty\), then 

%\begin{equation}
	%0 \in \partial_{2\bar{\sigma}}f(\bar{x})+B_{\bar{\theta}}(0)
%\end{equation}
%
%if we stick to the definition of the Fréchet-\(\varepsilon\)-subgradients defined for the nonconvex case.
%
%If one takes the \(\bar{\theta}\)-subgradient definition it results in 
%
%\begin{equation}
	%0 \in \partial_{2\bar{\sigma}+\bar{\theta}}.
%\end{equation}
%
%\textcolor{red}{How can that be?????? Which optimality condition is right???}

%\textcolor{red}{Better convergence results? \(\bar{\sigma}\) instead of \(2\bar{\sigma}\)}

%\textcolor{red}{beweis für \(\bar{\sigma}\)-optimalität}

%\textcolor{red}{bounded \(t_k\) instead of D? better????}
%This means we consider now inexactness of the following type
%
%\begin{align}
	%f_j &= f(x^j)-\sigma_j, \sigma_j \leq \bar{\sigma} \\
	%g^j &\in \R^n \text{ such that } f(\cdot) \geq f^j + \langle g^j, \cdot - x^j \rangle - \theta_j,  ~\forall x^j \in \R^n, \theta_j \leq \bar{\theta}
%\end{align}

%\textcolor{red}{with the following changes in the proof of Lemma 5(iv?) ... changes ... can be achieved -> for both kinds of subgradients\\
%important in convex case: when is linearization error negative? -> then for example noise attenuation.\\
%think different concept needed for better error bounds}
%
%\begin{itemize}
%	\item convex objective function \\
%	\(\to\) generally better convergence properties possible \\
%	but more or less only on error bound???
%	\(\to\) different concept of algorithm for convex inexact functions  to exploit convexity (contrary to nonconvex obj functions)
%\end{itemize}


%In the exact case boundedness of the sequence \(\{\eta_k\}\) is proven for lower-\(\mathcal{C}^2\) functions in \cite{Hare2010}. This is not possible in the inexact case, even if the objective function \(f\) is convex.

%A further simplification of the method for exact information is not necessary as the method is already almost as simple as the basic bundle method for nonconvex exact functions. Additionally no new concepts needed to be introduced when doing the step from nonconvex exact problems, for which the algorithm was originally designed, to problems with inexact information.

%\textcolor{red}{\begin{remark}
	%We want to add here, that the simplicity of the algorithm is rather special for methods suitable for nonconvex problems. Often a linesearch algorithm has to be inserted in the nonconvex case, which is not needed here. 
%\end{remark}}

%\subsection{Convergence analysis}
%
%\subsubsection{Results for objectives with exact information}
%
%The main ideas of the algorithm are basicly the ones developed in \cite{Hare2010} for the redistributed proximal bundle method for exact nonconvex problems. \\
%Setting the error bounds \(\bar{\sigma}\) and \(\bar{\theta}\) to zero results therefore in the following convergence theorem. 
%
%\begin{theorem}
	%Let the sequence \(\{\eta_k\}\) be bounded, \(\liminf_{k \to \infty }\) and the cardinality of the set \(\{j \in J_k | \alpha_j^k > 0\}\) be uniformly bounded in \(k\). \\
	%Then every accumulation point of sequence of serious iterates \(\{\hat{x}^k\}\) is a stationary point of the problem.
%\end{theorem}
%
%\textcolor{red}{think last condition only interesting in inexact case.}
%
%In the exact case boundedness of the sequence \(\{\eta_k\}\) is proven for lower-\(\mathcal{C}^2\) functions in \cite{Hare2010}. This is not possible in the inexact case, even if the objective function \(f\) is convex.

To conclude this section we can say: At the moment there exist two fundamentally different approaches to tackle inexactness in various bundle methods depending on if the method is developed for convex or nonconvex objective functions.
In the nonconvex case inexactness is only considered in the paper by Hare, Sagastiz{\`{a}}bal and Sodolov \cite{Hare2016} presented above and Noll \cite{Noll2013}. In these cases the inexactness can be seen as an additional nonconvexity. In practice this means that the algorithm can be taken from the nonconvex case with no or only minor changes. This includes that all results of the exact case remain true as soon as function and subgradient are evaluated in an exact way.
In case of convex objective functions with inexact information stronger convergence results are possible. However to be able to exploit convexity in order to achieve those results the algorithms look different from those designed for nonconvex objective functions and are generally not able to deal with such functions. 

\subsubsection{Aggregation}

In \cite{Hare2016} it is assumed, that the set \(\{j \in J^k \mid \alpha^k_j > 0\}\) is uniformly bounded for all \(k\).
In Remark 2 in \cite[p. 11]{Hare2016} it is mentioned that this is possible if the set \(X\) is polyhedral and an active-set solver is used to solve the subproblem.

Another possibility to assure boundedness of the set, is the aggregation technique mentioned in section \ref{sec_agg}.
In this context is has however to be mentioned, that it is not immediately clear if the proof of Lemma 5 in \cite[p. 11]{Hare2016} still holds in that case.
For exact function and subgradient information convergence, of a very similar method to algorithm \ref{sec_nonconv_inex}.1 is proven also using of the aggregation technique in \cite{Hare2010}.
%For algorithm \ref{sec_nonconv_inex}.1 this lies beyond the scope of this thesis.

The lemma states the following results:
\begin{lemma}[{\cite[Lemma 5, p. 11]{Hare2016}}]\label{Lemma5}
	Suppose that the cardinality of the set \(\{j \in J^k\mid \alpha_j^k > 0\}\) is uniformly bounded in \(k\).
	
	(i) If \(C^k \to 0\) as \(k \to \infty\), then 
	\[ \sum_{j \in J^k}\alpha_j^k\|x^j-\hat{x}^k\| \to 0 \text{ as } k \to \infty. \]
	
	(ii) If additionally for some subset \(\tilde{K} \subset \{1,2,\dots\}\),
	\[\hat{x}^k \to \bar{x}, S^k \to \bar{S} \text{ as } K \ni k \to \infty, \text{ with } \{\eta_k\mid k \in \tilde{K}\} \text{ bounded,} \]
	
	then we also have 
	\[\bar{S} \in \partial f(\bar{x})+B_{\bar{\theta}}(0).\]
	
	(iii) If in addition \(S^k + \nu^k \to 0\) as \(\tilde{K} \in k \to \infty\), then \(\bar{x}\) satisfies the approximate stationarity condition 
	\begin{equation}
		0 \in \left(\partial f(\bar{x}) + \partial \mathtt{i}_X(\bar{x}) \right) + B_{\bar{\theta}}(0).
	\end{equation}
	
	(iv) Finally if \(f\) is also lower-\(\mathcal{C}^1\), then for each \(\varepsilon > 0\) there exists \(\rho > 0\) such that
	\begin{equation}
		f(y) \geq f(\bar{x})-(\bar{\theta}+\varepsilon)\|y-\bar{x}\|-2\bar{\sigma}, \quad \text{ for all } y \in X\cap B_{\rho}(\bar{x}).
	\end{equation}
\end{lemma}

For algorithm \ref{sec_nonconv_inex}.1 it has to be shown that item (ii) of this lemma also holds if there are aggregate subgradients in the bundle.
Here only the missing links are sketched to extend the proof to this case. The full proof can be found in \cite[p. 12]{Hare2016}

Therefore let (as in \cite{Hare2016}) \(p^j\) denote for each \(j\)  the orthogonal projection of the approximate subgradient \(g^j\) on the closed convex set \(\partial f(x^j)\). Then it holds by assumption on the approximate subgradient that \(\Vert g^j-p^j\Vert \leq \theta_j \leq \bar{\theta}\).
Furthermore let \(S^k_n\) denote the (augmented) aggregate subgradients in the \(k\)'th bundle.
Without loss of generality we can assume that there is only one aggregate subgradient in the bundle. (If there are more, this results just in more aggregate terms of the same form and behavior in the convex sum, so it does not alter the argumentation.)
The subscript \(n\) of \(S^k_n\) signals, that already \(n\) aggregate subgradients were incorporated in the current aggregate subgradient.

The proof works with induction over the number of aggregate subgradients contained in the current subgradient.

Denote the multiplier of the \(k\)'th index set corresponding to the aggregate subgradient \(\alpha^k_n\). The remainder of the index set \(J^k\), corresponding to the approximate subgradients, is denoted by \(J^k_g\).

To begin the induction recall that at iteration \(k\) \(S^k_0 = \sum_{j \in J^k_g} \alpha^k_j(g^j+\eta_k(x^j-\hat{x}^k))\) by (\ref{aug_subgr}) and (\ref{aug_agg_subgr}) and that \(J^k_g = J^k\) in this case.

From this follows that
\begin{align}
\begin{split}
	S^k_1 &= \sum_{j \in J^k_g}\alpha_j^k\left(g^j+\eta_k(x^j-\hat{x}^k)\right) + \alpha^k_n S^k_0 \\
	&=\sum_{j \in J^k_g}\alpha_j^k\left(g^j+\eta_k(x^j-\hat{x}^k)\right) + \alpha^k_n \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j\left(g^j+\eta_{k-1}(x^j-\hat{x}^{k-1})\right) \\
	&= \sum_{j \in J^k_g}\alpha_j^k p^j + \sum_{j \in J^k_g}\alpha_j^k (g^j-p^j) + \eta_k \sum_{j \in J^k_g}\alpha_j^k (x^j-\hat{x}^k) \\
	&+ \alpha^k_n \left(\sum_{j \in J^{k-1}_g} \alpha^{k-1}_j p^j + \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j(g^j-p^j) + \eta_{k-1}\sum_{j \in J^{k-1}_g} \alpha^{k-1}_j(x^j-\hat{x}^{k-1})\right)
	\label{calc_lem5_1}
\end{split}
\end{align}

As it is explained in the paper, due to the uniform boundedness of the set \(J^k\) it is possible to consider this set as some fixed index set (for example \(\{1,...,N\}\)). Unused indices are filled with \(\alpha^k_j=0\). Let then \(J\) be the set of all \(j\in J^k\) such that \(\liminf_{k\to\infty}\alpha^k_j >0\). From item (i) of Lemma \ref{Lemma5} follows then that \(\Vert x^j-\hat{x}^k\Vert \to 0\). Hence also \(\Vert x^j-\bar{x}\Vert \leq \Vert x^j-\hat{x}^k\Vert + \Vert \hat{x}^k-\bar{x}\Vert \to 0 \).
It holds that \(p^j \in \partial f(x^j)\) and \(x^j \to \bar{x}\) for \(j \in J\) and \(\{\alpha^k_j\} \to 0 \) for \(j \notin J\). Due to the boundedness of the subdifferentials (this follows from Lipschitz continuity of the function \(f\) and is explained in more detail for example in the proof of Theorem \ref{theo_inf_ser_steps}) passing on a subsequence \(K \subset \{1,2,...\}\) yields that there exists \(\bar{p}\) with \(p^j \to \bar{p}\) for \(j \in J\) and \(k \in K\).
The second and fifth term in the third equation of equation \ref{calc_lem5_1} are in \(B_{\bar{\theta}}(0)\) and the third and sixth term go to zero.
Then 

\begin{align*}
	\lim_{K \ni k \to \infty} S^k_1 &=  \lim_{K \ni k \to \infty} \sum_{j \in J^k_g}\alpha_j^k p^j + \sum_{j \in J^k_g}\alpha_j^k (g^j-p^j) + \eta \sum_{j \in J^k_g}\alpha_j^k (x^j-\hat{x}^k) \\
	&+ \lim_{K \ni k \to \infty} \alpha^k_n \left(\sum_{j \in J^{k-1}_g} \alpha^{k-1}_j p^j + \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j(g^j-p^j) + \eta_{k-1}\sum_{j \in J^{k-1}_g} \alpha^{k-1}_j(x^j-\hat{x}^{k-1})\right) \\
	&= \lim_{K \ni k \to \infty} \sum_{j \in J^k_g}\alpha_j^k p^j + \alpha^k_n \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j p^j + \sum_{j \in J^k_g}\alpha_j^k (g^j-p^j) + \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j(g^j-p^j) \\
	&= \lim_{K \ni k \to \infty} \sum_{j \in J^k_g}\alpha_j^k p^j + \alpha^k_n \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j p^j + B_{\bar{\theta}}(0),
\end{align*}

and due to outer semicontinuity of the Clarke subdifferential \cite[Proposition 6.6 p. 202]{Rockafellar2009} and \(\sum_{j\in J^k}\alpha^k_j = 1\) it follows that 

\[ \lim_{K \ni k \to \infty} \sum_{j \in J^k_g}\alpha_j^k p^j + \alpha^k_n \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j p^j \in \partial f(\bar{x}). \]

This proves assertion (ii) for a bundle containing one aggregate subgradients that only consist of genuine approximate (augmented) subgradients.

Assume now that \(\lim_{K\ni k \to \infty} S^k_{n} \in \partial f(\bar{x})+B_{\bar{\theta}}(0)\) holds also for bundles containing aggregate subgradients of the form \(S^k_n\).
The inductive step gives then:

\begin{align}
\begin{split}
	S^k_{n+1} &= \sum_{j \in J^k_g}\alpha_j^k\left(g^j+\eta_k(x^j-\hat{x}^k)\right) + \alpha^k_{n+1} \left( \alpha^{k-1}_n S^{k-1}_n+\sum_{j \in J^{k-1}_g} \alpha^{k-1}_j\left(g^j+\eta_{k-1}(x^j-\hat{x}^{k-1})\right)\right) \\
	&= \sum_{j \in J^k_g}\alpha_j^k\left(g^j+\eta_k(x^j-\hat{x}^k)\right) + \alpha^k_{n+1} \sum_{j \in J^{k-1}_g} \alpha^{k-1}_j\left(g^j+\eta_{k-1}(x^j-\hat{x}^{k-1})\right) \\
	&+ \alpha^k_{n+1}\alpha^{k-1}_n S^{k-1}_n
	\label{calc_lem5_2}
\end{split}
\end{align}

With the same argumentation as above it follows that in the limit the first tow terms of the second equation in (\ref{calc_lem5_2}) are in \(\partial f(\bar{x})+B_{\bar{\theta}}(0)\). 
From the induction assumption follows that also the last term is in \(\partial f(\bar{x})+B_{\bar{\theta}}(0)\).
This proves item (ii) also for the use of aggregation technique.

%first show that also for aggregate gradients \(G^j\) one can find vectors \(P^j\) such that \(\Vert G^j-P^j\Vert \leq \bar{\theta}\).
%
%By induction:
%
%\(k=2\): only one aggregate subgradient \(G^1 = \sum_{j \in J^1}\alpha^1_j g^j\) possible, consists only of approximate subgradients \(g^j\).
%Choose \(P^1 = \sum_{j \in J^1}\alpha^1_j p^j\) where \(p^j \in \partial f(x^j)\)
%
%then \(\Vert G^1-P^1\Vert \leq \Vert \sum_{j \in J^1}\alpha^1_j(g^j-p^j)\Vert \leq \sum_{j \in J^1}\alpha^1_j \Vert g^j-p^j\Vert\leq \bar{\theta} \) 
%
%used: first inequlity:  \(\alpha^k_j \geq 0 \), second inequality: sum of \(\alpha^k_j\) is one, norm because of projection \(\leq \bar{\theta}\)
%
%\(k =3\): one aggregate subgradient contains already aggregate subgradients; again with induction argument: can say that it contains only one, that is only from genuine approximate subgradients
%
%\[G^{3} = \alpha^{2}_2G^2+\sum_{j \neq 2, j \in J^{2}}\alpha^2_j g^j = \alpha^2_i \left(\sum_{\tilde{j}\in J^1}{\alpha^1_{\tilde{j}}g^{\tilde{j}}}\right) +\sum_{j \neq 2, j \in J^{2}}\alpha^2_j g^j \] 
%
%Now take for the approximate subgradients \(p^j\) as projection and for \(G^2\) \(P^2\) and build from that \(P^3\)
%
%in the same way for \(k \to k+1\) possible, because \(\Vert G^k-P^k\Vert \leq \bar{theta}\) for all aggregate subgradients and for the approximate ones of course also
%
%now have to show that still
%
%\[ \lim_{k\to\infty} \sum_{j\in J^k}\alpha^k_j P^j \in \partial f(\bar{x}) \]
%
%here \(P^j\) subgradient projections and aggregate projections.
%
%know that every \(P^j\) is a convex combination of projections hence also bounded, hence there exists converging subsequence.
%
%What does it converge to?
%Think it does not help...



%changes in the algorithm are more involved. The reason for this is that generally stronger convergence results are possible with inexactness in the convex case than in the nonconvex case. %This means however, that the inexactness cannot be incorporated as easily into the algorithm

%\begin{itemize}
	%\item stronger convergence results possible because of exploitation of convexity
	%\item changes in the algorithm because if convexity should be exploited: inexactness cannot be treated as nonconvexity
	%\item 
%\end{itemize}

%\begin{itemize}
	%\item in paper other method for calculating \(\mu = \frac{1}{t_k}\) \\
	%\textcolor{red}{how is made sure that \(\eta\) big enough???}
	%\item convergence results for nonconvex functions:?
	%\begin{itemize}
		%\item conditions on functions: \\
		%\emph{exact}: \(f\) lower-\(\mathcal{C}^2\) near the minimizers of the problem \\
		%\emph{inexact}: proper, regular, locally Lipschitz with full domain; even better: lower-\(\mathcal{C}^1\) (contains lower-\(\mathcal{C}^2\)) \(\rightarrow\) conditions more general than in exact case
		%\item convergence results: \\
		%\emph{exact}: the limit of the sequence \(\{x^{k}\}\) (which exists) or every accumulation point of the sequence \(\{\hat{x}^k\}\) is a stationary point of \(f\)\\
		%\textcolor{red}{Does this mean: \(0 \in \partial f(\bar{x}), ~\bar{x}\) being the respective limit??? incorporate set \(D\)?} \\
		%\item \emph{inexact}: \(0 \in \left(\partial f(\bar{x}) + \partial I_D(\bar{x}) \right) + B_{\bar{\theta}}(0)\) \\
		%if \(f\) lower-\(\mathcal{C}^1\): 
		%\[ \forall \varepsilon >0 ~\exists \rho >0: \quad f(y) \geq f(\bar{x})-(\bar{\theta}+\varepsilon)\|y-\bar{x}\|-2\bar{\sigma} \quad \forall y \in D \cap B_{\rho}(\bar{x}) \]
%\end{itemize}

