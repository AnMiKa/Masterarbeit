\section{Preliminaries}
\label{sec_prelim}

When it comes to nonsmooth objective functions the  derivative based framework of nonlinear optimization methods does not work any more. Meanwhile there exists a well understood theory of 'subdifferential calculus' that gives similar results in the nondifferentiable case.
The most important definitions and results of this theory together with some remarks on notation are stated in this section.


\subsection{Notation}

Let \(x\) denote a column vector. The transpose of \(x\) is denoted by \(x^{\top}\). The scalar product is written \(\langle \cdot, \cdot \rangle\).
In this thesis generally the euclidean norm is used and denoted by \(\|\cdot\|\). In section \ref{sec_variable_metric} additionally a norm is used that is induced by a symmetric matrix. Here we use the notation \(\|x\|^2_A = {\langle x, Ax \rangle}\).
Inequalities written for vectors \(x^1 \leq x^2\), \(x^1, x^2 \in \R^n\) are to be read component wise.
With \(0\) we denote the zero vector of appropriate size.  The identity matrix of appropriate size is written as \(\mathbb{I}\).

As we work with numerical methods in this thesis occur a lot of sequences of various dimensions. For vectors iteration indices are indicated by a superscript \(x^k\) whereas the components are  indicated by subscripts \(x = \left(x_1,x_2,...,x_n\right)^{\top}\). Sequences of numbers and matrices a indexed with subscripts.
For (sub-)sequences where \(k\) comes from an index set \(K \subset \N\) we write \(\{x^k\}_{k \in K}\). If \(k\) is in the natural numbers this notation is shortened to \(\{x^k\}\).
We denote the open ball around \(x\) with radius \(r\) with \(B_{r}(x)\).
The subset relation is denoted by \(A\subset B\). It is to be read in the sense that \(A\) is a subset of \(B\) or that \(A=B\). 

%\textcolor{red}{
%\begin{itemize}
	%\item iteration index as superscript \(x^k\), entry index as subscript \(x_i\)
	%\item  is the scalar product
	%\item more?
%\end{itemize}}


%\textcolor{red}{Theoretical Background, nonsmooth Analysis ???}

%\textcolor{red}{Check if requirements on functions are stated and defined.}\\

\subsection{Nonsmooth Analysis in Short}

Throughout this thesis we consider different optimization problems of the form

\begin{equation*}
\min_{x} f(x) \quad \text{s.t.} \quad x \in X \subset \R^n
\label{min_prob}
\end{equation*}

where \(f\) is a possibly nonsmooth function.

%Therefore the most important definitions and results needed when working with nonsmooth functions are stated in this section.

Nonsmooth functions have kinks where a unique gradient cannot be defined. It is however possible to define a set of tangents to the graph called subdifferential.
The subdifferential was first defined for convex functions.

\begin{definition}[{\cite[Definition 1.2.1, p. 241]{Hiriart-Urruty1996}}]
\label{conv_subdiff}
	Let \(f: \R^n \to \R \) be a convex function. The \emph{subdifferential} of \(f\) at \(x\in \R^n\) is the set 
	\[ \partial f(x) := \left\{g \in \R^n \mid f(y)-f(x) \geq \Langle g,y-x \Rangle \quad \forall y \in \R^n \right\}. \]
\end{definition}

The subdifferential is a set valued mapping. It is closed and convex. If \(f\) is differentiable, its subdifferential is single valued and coincides with its gradient \(\partial f(x) = {\nabla f(x)}\) \cite{Rockafellar1970}.


It is also possible to define a subdifferential for nonconvex functions. This is the subdifferential we will work with in this thesis most of the time.

\begin{definition}[c.f. {\cite[p. 25, 27]{Clarke1990}}]
\label{nonconv_subdiff}
	Let \(f: \R^n \to \R\) be locally Lipschitz (and not necessarily convex).
	The \emph{subdifferential} of \(f\) at \(x \in \R^n\) is the set
\[ \partial f(x) := \left\{g \in \R^n \biggm| \limsup_{y \to x,~ h \searrow 0}\frac{f(y+hv)-f(y)}{h} \quad \forall v \in \R^n \right\}. \]
\end{definition} 

All convex functions are locally Lipschitz \cite[Theorem 3.1.1, p. 16]{Hiriart-Urruty1996} so the above definition holds also for convex functions. In fact if the function is convex the subdifferential from definition \ref{nonconv_subdiff} is equivalent to the one from definition \ref{conv_subdiff} \cite[Proposition 2.2.7, p. 36]{Clarke1990}.
Due to this equivalence we call elements from both subdifferentials subgradients.

\begin{remark}
	It is important to observe that subgradient inequality 
	\[ f(y)-f(x) \geq \Langle g,y-x \Rangle \quad \forall y \in \R^n \]
	only holds in the convex case.
\end{remark}

There is also a sum rule for the subdifferential.

\begin{proposition}[{\cite[Proposition 2.3.3, p. 38]{Clarke1990}}]
	Let \(F(x)=\sum_i{f_i(x)}\) be a finite sum of nondifferentiable functions. Then it holds
	
	\[ \partial F(x) \subset \sum_i{\partial f_i(x)}. \]
\end{proposition}

Analogous to the \(\mathcal{C}^1\)-case some first order optimality conditions can be stated.
For nondifferentiable functions a \emph{stationary point} \(x\) of the function \(f\) is characterized by \cite[p. 38]{Clarke1990}
\[ 0 \in \partial f(x). \]
If the function \(f\) is convex, then every stationary point is a minimum.

%\textcolor{red}{-> characterization of minimum and stationary point}

A drawback of the subdifferential is that it does not indicate how near the evaluated point is to a stationary point or minimum of a function. This can only be seen if the evaluated point is already stationary.

This issue is addressed by the \(\varepsilon\)-\emph{subdifferential}. It gathers all information in a small neighborhood of the point \(x\).

For convex functions an \(\varepsilon\)-\emph{subgradient} of \(f(x)\) is defined as a vector \(g \in \R^n\) satisfying the inequality

\[ f(y)-f(x) \geq \Langle g,y-x \Rangle - \varepsilon \quad \forall y \in \R^n.\]

The \(\varepsilon\)-subdifferential is then the set
\[ \partial_{\varepsilon} f(x) := \left\{g \in \R^n \mid g \text{ is an } \varepsilon \text{-subgradient of } f(x) \right\}. \]

For nonconvex functions the subdifferential that is used in this thesis is the \emph{Fr\'{e}chet} \(\varepsilon\)-\emph{subdifferential}.

\begin{definition}[c.f. {\cite[p. 73]{Jofre1998}}]
	The Fr\'{e}chet \(\varepsilon\)-subdifferential of \(f(x)\) is
	\[ \partial_{[\varepsilon]}f(x) := \left\{ g \in \R^n \biggm| \liminf_{\|h\| \to 0} \frac{f(x+h)-f(x)-\langle g,h\rangle}{\|h\|}  \geq -\varepsilon\right\}. \]
\end{definition}

For \(\varepsilon = 0\) this is called \emph{Fr\'echet subdifferential}.
For convex functions the Fr\'echet \(\varepsilon\)-subdifferential and the \(\varepsilon\)-subdifferential are \emph{not} the same.

In the course of this thesis we sometimes derive stronger results for a smaller class of nonsmooth functions. Those functions are called lower-\(\mathcal{C}^2\) functions and can be defined as follows.

\begin{definition}[c.f. Definition 10.29, p. 447 in {\cite{Rockafellar2009}}]
\label{def_low_C2}
	A function \(f:\R^n \to \R\) is \emph{lower-}\(\mathcal{C}^2\), if on some neighborhood \(\Omega\) of each \(\bar{x} \in \R^n\) there exists a representation 
	
	\[ f(x) = \max_{t\in T}f_t(x) \]
	
	where the functions \(f_t\) are of class \(\mathcal{C}^2\) on \(\Omega\) and \(f_t(x)\) and all its first and second partial derivatives depend continuously on both \(x\) and \((x,t) \in \Omega \times T\).
	The index set \(T\) is a compact space.
\end{definition}

Lower-\(\mathcal{C}^2\) functions are locally Lipschitz continuous \cite[Theorem 10.31, p. 448]{Rockafellar2009}.
%\subsection{Nonsmooth analysis and optimization}

%A necessary assumption on the objective function \(f\) is that it is locally Lipschitz. This assumption assures the well-definedness of the following generalizations of derivatives. \\
%\begin{definition} \cite{Mifflin1977}
	%A function \(f: \R^n \to \R\) is called \emph{locally Lipschitz}  if it is Lipschitz on each bounded subset \(B \subseteq \R^n\)
	%\[ |f(y) - f(x)| \leq C \|y-x\| \quad \forall x,y \in B, \quad C >0. \]
%\end{definition}
%All convex functions are locally Lipschitz \cite{Hiriart-Urruty1996}. \\

%For convex functions one can define so called subgradients as a generalization of the usual derivative. They are defined using the  directional derivative. \\
%\begin{definition} \cite{Hiriart-Urruty1996}
	%The \emph{directional derivative}  of a convex function \(f\) at \(x\) in direction \(d\) is 
	%\[ f'(x,d) := \lim_{\lambda \downarrow 0}\frac{f(x+\lambda d) - f(x)}{\lambda}. \]
%\end{definition}

%\begin{definition} \cite{Hiriart-Urruty1996}
	%Let \(f\) convex. The \emph{subdifferential} \(\partial f(x)\) of \(f\) at \(x\) is the nonempty compact convex set
	%\[ \partial f(x) = \{g \in \R^n | f'(x,d) \geq \langle g,d\rangle \forall d \in \R^n \}. \]
%\end{definition}
%The subdifferential is a convex set, that supports the graph of the function \(f\) from below. If \(f\) is differentiable at the point \(x\), the subdifferential reduces to the gradient at that point \cite{Hiriart-Urruty1996}. \\

%This concept was generalized by Clarke for nonconvex functions. First a generalization of the directional derivative is given: 
%\begin{definition} \cite{Clarke1990}
	%Let \(f: \R^n \to \R \) locally Lipschitz. The \emph{generalized directional derivative}  of \(f\) at \(x\) in direction \(d\) is given by
	%\[ f^{\circ}(x,d):= \limsup_{\substack{y \to x \\ \lambda \downarrow 0}} \frac{f(y+\lambda d)-f(y)}{\lambda}. \]
%\end{definition}

%This allows for the following definition.
%\begin{definition} \cite{Clarke1990}
	%The \emph{generalized gradient} of the locally Lipschitz function \(f\) at \(x\) is a nonempty convex compact set \(\partial f(x)\) given by
	%\[ \partial f(x) := \{g \in \R^n | f^{\circ}(x,d) \geq \langle g,d\rangle \forall d \in \R^n \}. \]
%\end{definition}

%\textcolor{red}{!!!other definition --> take definition from rockefellar/Hare directly from Paper!!!}

%If \(f\) is a convex function the generalized gradient coincides with the subdifferential \(\partial f\) of \(f\) \cite{Clarke1990}.


%\textcolor{red}{For the biggest part of this thesis and if not otherwise stated \(X = \R^n\).} \\
%\textcolor{red}{Also write something about inexactness? specify \(X\) more precisely? Convex?} \\
%\textcolor{red}{Just definition, lemma, theorem or a bit explanation around it?\\
%better just in Text without Definition, ...} \\
%\textbf{\textcolor{red}{See if requirements in definitions and theorems meet what is needed/provided later.}} \\

%\begin{definition} % as in 
		%The function \(f: \R^n \to \R\) is called \emph{Lipschitz near \(x \in \R^n\)}\cite{Clarke1990} if there exist \(C > 0\) and \(\varepsilon > 0\) such that 
	%\[ |f(y_2) - f(y_1)| \leq C\|y_2 -y_1\| \quad \forall y_1, y_2 \in \mathbf{B}_{\varepsilon}(x). \]
%\end{definition}


%Throughout this thesis the objective function is assumed to be locally Lipschitz. 

%For convex functions one can define so called subgradients as a generalization of the usual derivative. They are defined using the  directional derivative. \\
%\begin{definition} \cite{Hiriart-Urruty1996}
%	The \emph{directional derivative}  of a convex function \(f\) at \(x\) in direction \(d\) is 
%	\[ f'(x,d) := \lim_{\lambda \downarrow 0}\frac{f(x+\lambda d) - f(x)}{\lambda}. \]
%\end{definition}

%\begin{definition} \cite{Hiriart-Urruty1996}
	%Let \(f\) convex. The \emph{subdifferential} \(\partial f(x)\) of \(f\) at \(x\) is the nonempty compact convex set
	%\[ \partial f(x) = \{g \in \R^n | f'(x,d) \geq \langle g,d\rangle \forall d \in \R^n \}. \]
%\end{definition}
%The subdifferential is a convex set, that supports the graph of the function \(f\) from below. If \(f\) is differentiable at the point \(x\), the subdifferential reduces to the gradient at that point \cite{Hiriart-Urruty1996}. \\
%
%This concept was generalized by Clarke for nonconvex functions. First a generalization of the directional derivative is given: 
%\begin{definition} \cite{Clarke1990}
	%Let \(f: \R^n \to \R \) locally Lipschitz. The \emph{generalized directional derivative}  of \(f\) at \(x\) in direction \(d\) is given by
	%\[ f^{\circ}(x,d):= \limsup_{\substack{y \to x \\ \lambda \downarrow 0}} \frac{f(y+\lambda d)-f(y)}{\lambda}. \]
%\end{definition}
%
%This allows for the following definition.
%\begin{definition} \cite{Clarke1990}
	%The \emph{generalized gradient} of the locally Lipschitz function \(f\) at \(x\) is a nonempty convex compact set \(\partial f(x)\) given by
	%\[ \partial f(x) := \{g \in \R^n | f^{\circ}(x,d) \geq \langle g,d\rangle \forall d \in \R^n \}. \]
%\end{definition}
%
%\textcolor{red}{!!!other definition --> take definition from rockefellar/Hare directly from Paper!!!}

%If \(f\) is a convex function the generalized gradient coincides with the subdifferential \(\partial f\) of \(f\) \cite{Clarke1990}.

%\textcolor{red}{Why epsilon Subdifferential?\\
%implementable stopping criterion \\
%dual form of bundle algorithms = same as stopping criterion?}

%\begin{definition} \cite{Mifflin1977}
	%The function \(f: \R^n \to \R\)  is \emph{semismooth} at \(x \in \R^n\)  if \(f\) is Lipschitz on a ball \(\mathbb{B}_\varepsilon(x)\)around x and for each \(d \in \R^n\) and for any sequences \(\{t_k\} \subseteq \R_{+}\), \(\{\theta_k\} \subseteq \R^n\) and \(\{g_k\} \subseteq \R^n\) such that
	%\[ \{t_k\} \downarrow 0, \quad \{\theta_k/t_k\} \to 0 \in \R^n \quad \text{and} \quad g_k \in \partial f(x+t_kd+\theta_), \]
	%the sequence  \(\{\langle g_k,d\rangle\}\) has exactly one accumulation point.
%\end{definition}

%\textcolor{red}{check if I need \(\infty\)-functions and if something changes then}
%\textcolor{red}{\(\epsilon\)-subdifferential \\
%continuity properties of generalized gradient? \\
%definitions from chapter inexact information \\
%definition of inexactness for nonconvex kind of generalization of \(\epsilon\)-subdifferential for nonconvex case (Noll, inex, nonconv)}
