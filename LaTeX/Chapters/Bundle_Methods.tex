\section{Bundle Methods}

When bundle methods were first introduced in 1975 by Claude Lemaréchal and Philip Wolfe they were developed to minimize a convex (possibly nonsmooth) function \(f\) for which at least one subgradient at any point \(x\) can be computed \cite{Mifflin2012}.
To provide an easier understanding of the proximal bundle method in \cite{Hare2016} and stress the most important ideas of how to deal with nonconvexity and inexactness first a basic bundle method is shown here. 

%\textcolor{red}{link to chapter?}

Bundle methods can be interpreted in two different ways: From the dual point of view one tries to approximate the \(\varepsilon\)-subdifferential to finally ensure first order optimality conditions. The primal point of view interprets the bundle method as a stabilized form of the cutting plane method where the objective function is modeled by tangent hyperplanes \cite{Hare2010}. We focus here on the primal approach.

\textcolor{blue}{notation, definitions}\\

\textcolor{red}{already done in previous preliminaries chapter?}

\subsection{A basic bundle method}

%\textcolor{red}{In the next two sections the function \(f\) is assumed to be convex.} \\

This section gives a short summery of the derivations and results of chapter XV in \cite{Hiriart-Urruty1993} where a primal bundle method is derived as a stabilized version of the cutting plane method. If not otherwise indicated the results in this section are therefore taken from \cite{Hiriart-Urruty1993}.

The optimization problem considered in this section is 
\begin{equation}
  \min_{x} f(x) \quad \text{s.t.} \quad x \in X
\label{min_prob_basic}
\end{equation}
where \(f\) is a convex but possibly nondifferentiable function and \(X \subseteq \R^n\) is a  closed and convex set.

%\textcolor{red}{Define Problem again?? Incorporate ``set-constraint'' by writing \(h(x):= f(x)+\mathtt{i}_X\). \(\rightarrow\) later???}

%\textcolor{blue}{explanation}

\subsubsection{Derivation of the bundle method}

The geometric idea of the \emph{cutting plane method} is to build a piecewise linear model of the objective function \(f\) that can be minimized more easily than the original objective function.
This model is built from a \emph{bundle} of information that is gathered in the previous iterations.
In the \(k\)'th iteration, the bundle consists of the previous iterates \(x^j\), the respective function values \(f(x^j)\) and a subgradient at each point \(g^j \in \partial f(x^j)\) for all indices \(j\) in the index set \(J_k\). From each of these triples, one can construct a linear function 

\begin{equation}
	l_j(x) = f(x^j) + (g^j)^{\top}(x-x^j)
\label{lin_fun}
\end{equation}  

with \(f(x^j) = l_j(x^j)\) and due to convexity \(f(x) \geq l_j(x), ~ x \in X\).\\ % x \in \R^n??
The objective function \(f\) can now be approximated by the piecewise linear function

\begin{equation}
	m_k(x) = \max_{j \in J_k} l_j(x).
\label{cp_model}
\end{equation}

A new iterate \(x^{k+1}\) is found by solving the subproblem

\begin{equation}
	\min_{x} m_k(x) \quad \text{s.t.} \quad x \in X.
\label{cp_model_fun}
\end{equation}

\textcolor{red}{Picture of function and cutting plane approximation of it}

This subproblem should of course be easier to solve than the original task. A question that depends a lot on the structure of \(X\). If \(X = \R^n\) or a polyhedron, the problem can be solved easily. Still there are some major drawbacks to the idea. For example if \(X = \R^n\) the solution of the subproblem in the first iteration is always \(-\infty\).
In general we can say that the subproblem does not necessarily have to have a solution.
To tackle this problem a penalty term is introduced to the subproblem:

\begin{equation}
	\min \tilde{m}_k(x) =  m_k(x) + \frac{1}{2 t_k}\|x-x^k\|^2 \quad \text{s.t.} \quad x \in X, ~t_k > 0.
\label{stabil_model_fun}
\end{equation}

This new subproblem is strongly convex and has therefore always a unique solution.
%\textcolor{red}{how much explanation here?}
% \(\max_{j \in J_k} l_j(\hat{x}^k + d)\)

%\textcolor{red}{Some nice sentences to explain the term a little bit more and to lead over to the next paragraph.\\ 
%To understand the deeper motivation of this term see \cite{Hiriart-Urruty1993}. For this introduction it suffices to see that due to the regularization term the subproblem is now strongly convex and therefore always uniquely solvable.} \\

This regularization term can be motivated and interpreted in many different ways, c.f. \cite{Hiriart-Urruty1993}. From different possible regularization terms the most popular in bundle methods is the penalty-like regularization used here.

The second major step towards the bundle algorithm is the introduction of a so called \emph{stability center} or  \emph{serious point} \(\hat{x}^k\). It is the iterate that yields the ``best'' approximation of the optimal point up to the \(k\)'th iteration (not necessarily the best function value though).
The updating technique for \(\hat{x}^k\) is crucial for the convergence of the method: If the next iterate yields a decrease of \(f\) that is ``big enough'', namely bigger than a fraction of the decrease suggested by the model function for this iterate, the stability center is moved to that iterate. If this is not the case, the stability center remains unchanged.

In practice this looks the following:
Define first the \emph{model decrease} \(\delta_k\) which is the decrease of the model for the new iterate \(x^{k+1}\) compared to the function value at the current stability center \(\hat{x}^k\).

\begin{equation}
	\delta_{k} = f(\hat{x}^k) - m_k(x^{k+1}) \geq 0
\label{nom_dec}
\end{equation}

%\textcolor{red}{The nominal decrease is in fact stated a little differently for different versions of the bundle algorithm, this is why I added the constant \(a_k \in \R\) here for generalization. In practice the difference between the decreases is not influencing the algorithm as \(\delta_k\) is weighted by the constant \(m \in (0,1)\) for the descent test which compensates \(a_k\).} 

If the actual decrease of the objective function is bigger than a fraction of the nominal decrease 
\[ f(\hat{x}^k) - f(x^{k+1}) \geq m \delta_k, \quad m \in (0,1) \]
set the stability center to \( \hat{x}^{k+1} = x^{k+1}\). This is called a \emph{serious} or \emph{descent step}.
If this is not the case a \emph{null step} is executed and the serious iterate remains the same \(\hat{x}^{k+1} = \hat{x}^k\).

Next to the model decrease other forms of decrease measures and variations of these are possible. Some are used in \cite{Hiriart-Urruty1993, Oliveira2014}.

The subproblem to be solved to find the next iterate can be rewritten as a smooth optimization problem. For convenience we first rewrite the affine functions \(l_j\) with respect to the stability center \(\hat{x}^k\).

%\textcolor{red}{citation for this???!!!}

\begin{align}
	l_j(x) &= f(x^j) + {g^j}^{\top} (x - x^j) \\
	&= f(\hat{x}^k)+{g^j}^{\top}(x-\hat{x}^k)-(f(\hat{x}^k)-f(x^j) + {g^j}^{\top}(x^j-\hat{x}^k)) \\
	&= f(\hat{x}^k) + {g^j}^{\top}(x-\hat{x}^k) - e^k_j
\end{align}	

where
\begin{equation}
	e^k_j := f(\hat{x}^k)-f(x^j) + {g^j}^{\top}(x^j-\hat{x}^k) \geq 0 \quad \forall j \in J_k
\label{lin_err}
\end{equation}

is the \emph{linearization error}. Its nonnegativity property is essential for the convergence theory and will also be of interest when moving on to the case of nonconvex and inexact objective functions. \\


Subproblem (\ref{stabil_model_fun}) can now be written as

\begin{align}
	& \min_{\hat{x}^k+d \in X} \tilde{m}_k(\hat{x}^k+d) = f(\hat{x}^k) + \max_{j \in J_k} \{{g^j}^{\top}d - e^k_j\} + \frac{1}{2t_k}\|d\|^2
	\label{sub_prob_long}\\
	\Leftrightarrow \quad &\min_{\substack{\hat{x}^k+d \in X, \\ \xi \in \R}} \xi + \frac{1}{2t_k}\|d\|^2 \quad \text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
\label{sub_prob_short}
\end{align}


where the constant term \(f(\hat{x}^k)\) was discarded for the sake of simplicity. \\
If \(X\) is a polyhedron this is a quadratic optimization problem that can be solved using standard methods of nonlinear optimization. The pair \((\xi_k, d^k)\) solves (\ref{sub_prob_short}) if and only if

\begin{align}
	d^k &\text{ solves the original subproblem (\ref{sub_prob_long})  and } \\
	\xi_k & =\max_{j \in J_k}{g^j}^{\top}d^k - e_j^k = m_k(\hat{x}^k+d^k) - f(\hat{x}^k). \label{xi}
\end{align}
	
The new iterate is then given by \(x^{k+1} = \hat{x}^k + d^k\). %\textcolor{red}{citation!!!}\\

\subsubsection{The prox-operator}

The constraint \(\hat{x}^k+d \in X\) can also be incorporated directly in the objective function by using the indicator function

\[ \mathtt{i}_X(x) = \left\{ \begin{array}{cl} 0, & \text{if } x \in X \\ +\infty, & \text{if } x \notin X \end{array} \right. .\]

This function is convex if and only if the set \(X\) is convex \cite{Rockafellar1996}.

Subproblem (\ref{stabil_model_fun}) then writes with respect to the serious point \(\hat{x}^k\)
\begin{equation}
	\min_{x \in \R^n} m_k(x) + \mathtt{i}_X(x) + \frac{1}{2t_k}\|x-\hat{x}^k\|^2. % \quad \text{s.t.} \quad {g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
\label{sub_prob_fin}
\end{equation}
%\textcolor{red}{check if f also not put into subproblem before}

The subproblem is now written as the \emph{Moreau-Yosida regularization} of \(\check{f}:= m_k(x) + \mathtt{i}_X(x)\). The emerging mapping is also known as \emph{proximal point mapping} \cite{Hare2010} or \emph{prox-operator}

	\begin{equation}
		prox_{t,f}(x) = \argmin_{y \in \R^n}\left\{\check{f}(y) + \frac{1}{2t}\|x-y\|^2\right\}, \quad t > 0.
	\label{prox_op}
	\end{equation}

This special form of the subproblems gives the primal bundle method its name, \emph{proximal bundle method}. The mapping also plays a key role when the method is generalized to nonconvex objective functions and inexact information.


\subsubsection{Aggregate objects}

Look again at a slightly different formulation of the bundle subproblem 

\begin{align}
	\min_{\substack{d \in \R^n, \\ \xi \in \R}} & \quad \xi + \mathtt{i}_X+\frac{1}{2t_k}\|d\|^2 \\
	\text{s.t.} & \quad {g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k. 
\label{sub_prob_smooth_iX}
\end{align}

As the objective function is still convex (\(X\) is a convex set) the following Karush-Kuhn-Tucker (KKT) conditions have to be valid for the minimizer \(\left(\xi_k,d^k\right)\) of the above subproblem \cite{Hiriart-Urruty1996} assuming a constraint qualification if the constraint set \(X\) makes it necessary \cite{Solodov2011}.

There exist a subgradient \(\nu^k \in \partial \mathtt{i}_{X}\) and Lagrangian multipliers \(\alpha_j, ~j \in J^k\) such that

\begin{align}
	& 0 = \nu^k + \frac{1}{t_k}d^k + \sum_{j \in J^k}\alpha_j g^j \label{KKT_1}\\
	& \sum_{j \in J_k}\alpha_j = 1, \label{KKT_2}\\
	& \alpha_j \geq 0, ~j \in J^k, \label{KKT_3}\\
	& {g^j}^{\top}d^k - e^k_j - \xi_k \leq 0, \label{KKT_4}\\
	& \sum_{j \in J^k} \alpha_j \left(f(\hat{x}^k)+{g^j}^{\top}d^k - e^k_j - \xi_k\right) = 0 \label{KKT_5}.
\end{align}

From condition (\ref{KKT_1}) follows then

\begin{equation}
	d^k = t_k\left(G^k+\nu^k\right) \quad \text{with} \quad G^k := \sum_{j \in J^k}\alpha_j g^j \in \partial m_k(x^{k+1})
\label{agg_subgr}
\end{equation}

with the \emph{aggregate subgradient} \(G^k\).

Rewriting condition (\ref{KKT_5}) yields the \emph{aggregate error} 

\begin{equation}
	E_k := \sum_{j \in J^k} \alpha_j e^k_j = (G^k)^{\top}d^k+f(\hat{x}^k)-m_k(x^{k+1}).
\label{agg_err}
\end{equation}

Here relation (\ref{xi}) was used to replace \(\xi_k\).

The aggregate subgradient and error are used to formulate an implementable stopping condition for the bundle algorithm. The motivation behind that becomes clear with the following lemma.

\begin{lemma}\cite[Theorem 6.68, p.387]{Geiger2002}
 Let \(X = \R^n\). Let \(\varepsilon > 0\), \(\hat{x}^k \in \R^n\) and \(g^j \in \partial f(x^j)\) for \(j \in J^k\). Then the set 

\[ \mathcal{G}_{\varepsilon}^{k} := \left\{\sum_{j \in J^k}\alpha_j g^j \mid \sum_{j \in J^k} \alpha_j e_j   \varepsilon, \sum_{j \in J^k}\alpha_j = 1, \alpha_j \geq 0, j \in J^k\right\} \]

is a subset of the \(\varepsilon\)-subdifferential of \(f(\hat{x}^k)\)
\[ \mathcal{G}_{\varepsilon}^k \subseteq \partial_{\varepsilon}f(\hat{x}^k) \].

\end{lemma}

This means that at least in the unconstrained case \(G^k \in \partial_{E_k} f(\hat{x}^k)\). So driving \(\|G^k\|\) and \(E_k\) close to zero results in some approximate \(\varepsilon\)-optimality of the objective function.

A totally different use of the aggregate objects was proposed by Kiwiel in \cite{Kiwiel1986}. The aggregate subgradient can be used to build the \emph{aggregate linearization}

\begin{equation}
	a_k(\hat{x}^k+d):=m_k(x^{k+1})+\langle G^k,d-d^k \rangle.
\label{agg_lin}
\end{equation}

This function can be used to avoid memory problems as it compresses the information of all bundle elements into one affine plane. Adding the function \(a_k\) to the cutting plane model preserves all assumptions put on the model and can therefore be used instead of or in combination with the usual cutting planes. This is shown im more detail in \textcolor{red}{reference}.

%The importance of these two objects becomes clear when looking at the bundle method from the dual point of view. It can be shown that \(G^k (+\nu^k)\) is part of the \(\varepsilon\)-subdifferential of... with \(\varepsilon = E^k\). \cite{} This motivates to drive \(\|G^k+\nu^k\| \) and \(E^k\) close to zero as a stopping condition.
%inequality for limited memory -> Kiwiel \cite{}
%\textcolor{red}{Some introduction how this and the aggregate error expression relate to each other. Why it is in this case easier to write the model in the nonsmooth form...}

%\textcolor{red}{Lemma XI 3.1.1 \(\partial g = \partial f + \partial \mathtt{i}_X\) for \(g = f + \mathtt{i}_X\).}

%One gets the following results about the step \(d^k\) of the subproblem:
%
%\begin{lemma} %Lemma XV 3.1.1 \cite{Hiriart-Urruty1993}
	%The optimization problem (\ref{sub_prob_fin}) has for \(t > 0\) a unique solution given by
	%\begin{equation}
		%d^k = -t_k(G^k + \nu^k), \quad G^k \in \partial m_k(d^k), \quad \nu^k \in \partial  \mathtt{i}_X.
	%\label{sub_prob_sol}
	%\end{equation}
%\end{lemma}












%Furthermore
%\begin{equation}
	%m_k(\hat{x}^k+d) \geq f(\hat{x}^k) + {G^k}^{\top}d - E_k \quad \forall d \in \R^n
%\label{m_inequ}
%\end{equation}
%
%\textcolor{red}{inequality because of aggregation technique. Is sharp when cutting plane model is used? source?}
%
%where 
%\begin{equation}
	%E_k := f(\hat{x}^k) - m_k(x^{k+1}) + {G^k}^{\top}d^k.
%\label{agg_err_1}
%\end{equation}
%
%\textcolor{red}{Comment on the inequality missing}
%
%The quantities \(G^k\) and \(E^k\) are the \emph{aggregate subgradient} and the \emph{aggregate error}.
%
%\textcolor{red}{Explain aggregation process in more detail}
%
%From the Karush-Kuhn-Tucker conditions (KKT-conditions) one can see that in the optimum there exist Lagrange or \emph{simplicial multiplier} \(\alpha_j^k, ~j \in J_k\) such that
%
%\begin{equation}
	%\alpha_j^k \geq 0, \quad \sum_{j \in J_k}{\alpha_j^k} = 1
%\label{alphas}
%\end{equation}
%
 %\textcolor{red}{by rewriting and so on... one can see that the above expressions are in fact }
%
%From the dual problem one obtains that the aggregate subgradient and error can also be expressed as
%
%\begin{equation}
	%E_k = \sum_{j \in J_k} \alpha_j^k e_j^k \qquad \text{and}\qquad G^k = \sum_{j \in J_k} \alpha_j^k g^j.
%\label{agg_obj}
%\end{equation}
%
%\textcolor{red}{Finally use Lemma ??? in \cite{Hiriart-Urruty1993}} 
%\[ m_k(x^{k+1}) = f(\hat{x}^k) - E_k - t_k\|G^k\|^2 \]
%to reformulate the nominal decrease \(\delta_k\):
%\[ \delta_k =  f(\hat{x}^k) - m_k(x^{k+1}) - \frac{1}{2}t_k\|G^k\|^2 = E_k + \frac{1}{2}t_k\|G^k\|^2\]
%The nominal decrease in this case is defined as:\\
%\textcolor{red}{noch mal anschauen}
%\begin{equation}
	%\delta_k := E_k + t_k\|G^k+\nu^k\|^2 = f(\hat{x}^k) - m_k(x^{k+1}) - {\nu^k}^{\top}d^k
%\label{nom_dec_1}
%\end{equation}
%
%\textcolor{red}{In practice the different definition of the decreases makes no difference because of the weighting with the descent parameter \(m\).}
%
%The following basic bundle algorithm can now be stated: 
%
%\textcolor{red}{Reformluate equations, model function \\
%introduce aggregate expressions \\
%say something to \(J\)-update, say something to \(t\)-update \\
%see if all abbreviations (\(f_j, g^j, ...\)) are introduced \\
%introduce prox-operator and proximal points}
%
%\textcolor{blue}{algorithm}
%
%\vspace{1em}
%
%\hrule  \vspace{0.4ex} \hrule
%\vspace{1ex}
%\textbf{Basic bundle method}
%\vspace{1ex}
%\hrule
%\vspace{1ex}
%Select descent parameter \( m \in (0,1)\) and a stopping tolerance \( \mathtt{tol} \geq 0\). Choose a starting point \(x^1 \in \R^n\) and compute \(f(x^1)\) and \(g^1\). Set the initial index set \(J_1:=\{1\}\) and the initial stability center to \(\hat{x}^1 := x^1\), \(f(\hat{x}^1) = f(x^1)\) and select \(t_1 > 0\).
%
%For \(k = 1,2,3  \dotsc \)   
%
%\begin{enumerate}
	%\item Calculate
	%\[ d^k = \argmin_{d \in \R^n}{m_k(\hat{x}^k+d) + \mathtt{i}_X +\frac{1}{2t_k}\|d\|^2}  \]
	%and the corresponding Lagrange multiplier \(\alpha_j^k,~ j \in J_k\).
	%\textcolor{red}{say how model \(m_k\) looks here. include \(\ \mathtt{i}_X\)}
	%\item Set
		%\begin{equation*}
			%\begin{split}
				%G^k = \sum_{j \in J_k}{\alpha^k_j g^k_j}, \quad E_k = \sum_{j \in J_k}{\alpha^k_j e^k_j}, \quad \text{and} \quad  \delta_k =  E_k + t_k\|G^k+\nu^k\|^2
			%\end{split}
		%\end{equation*}
		%If \(\delta_k \leq \mathtt{tol} \rightarrow \) STOP.
	%\item Set \( x^{k+1} = \hat{x}^k + d^k \).
	%\item Compute \(f(x^{k+1}),~ g^{k+1}\). \\
	%If 
	%\[f^{k+1} \leq \hat{f}^k - m\delta_k \quad \rightarrow  \text{serious step}. \]
	%Set \(\hat{x}^{k+1} = x^{k+1}, ~f(\hat{x}^{k+1}) = f(x^{k+1})\) and select suitable \(t_{k+1} > 0\). \\
	%Otherwise \(\rightarrow\) nullstep. \\
	%Set \(\hat{x}^{k+1} = \hat{x}^k, ~f(\hat{x}^{k+1})=f(x^{k+1})\) and choose \(t_{k+1}\) in a suitable way.
	%\item Select new bundle index set \(J_{k+1} = \{j \in J_k| \alpha_j^{k+1} \neq 0\} \cap {k+1}\), calculate \(e_j\) for \(j \in J_{k+1}\)	and update the model \(m_k\).
%\end{enumerate}
%\vspace{1ex}
%\hrule
%
%\vspace{1.5em}
%In steps 4 and 5 of the algorithm the updates of the steplength \(t_k\) and the index set \(J_k\) are are only given in a very general form.\\
%The ``suitable'' choice of \(t_k\) will be discussed more closely in the convergence analysis of \textcolor{red}{decide which method; say that \(t_k > 0 \forall k...\)}. \\
%\textcolor{red}{Comment on \(J_k\) update \(\rightarrow\) depends on what is included in thesis.} \\
%For the choice of the new index set \(J_{k+1}\) different aggregation methods to keep the memory size controllable are available. The most easy and intuitive one is to just take those parts of the model function, that are actually active in the current iteration. This is done in this basic version of the method. \\
%\textcolor{red}{Refer to low memory bundling if later in thesis.}
%Instead of keeping every index in the set \(J_k\) different compression ideas exist.  \textcolor{red}{For now I therefor stick to this update. \\
%refer to later ``low memory'' thing??}
%
%\textcolor{red}{explanation to \(t_k\) update. \(\rightarrow\) include at which point???} 
%This simple idea has however some major drawbacks \cite{Hiriart-Urruty1996}:
%\begin{itemize}
	%\item Minimization of the cutting plane model of the objective function is not trivial. Indeed unconstrained minimization of the model is never possible in the first step, where it is just a line, unless the starting point is already a minimum.
	%\textcolor{red}{\item The convergence speed is very slow.}
%\end{itemize}
%
%\textcolor{red}{If convergence speed named here, does it have to be shown (rates)? For all algorithms??? Leave out? Argue about instability?}
%
%To address those issues a regularization is added to the cutting plane model. This ensures unique solvability of the minimization of the subproblem. \textcolor{red}{By introducing a stability center and }
