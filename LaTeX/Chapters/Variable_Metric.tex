\section{Noll Part}
\label{variable_metric}

\subsection{Introduction}

\subsection{Keywords}

important in Noll for me: optimize model + \(d^{\top}(Q+\frac{1}{t_k}\mathbb{I})d\)
-> some kind of second order information

important: \(Q+\frac{1}{t_k}\mathbb{I}\) must have all eigenvalues \(\geq 0\).

idea to get \(Q\): BFGS like in Fin-papers; theory

!!! check stopping criterion
connection between \(d^k\) and \(G^k/S^k\) now:
Optimality condition: 

\begin{align}
	& 0 \in \partial M_k(x^{k+1})+\partial\mathtt{i}_{D}(x^{k+1})+\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \\
	\Rightarrow \quad & S^k(+\nu^k) = -\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k
\end{align}

From this derivation of \(\delta_k\) \(\to\) nominal (model) decrease:
\begin{align}
	\delta_k  &= \hat{f}_k - M_k(x^{k+1}) - (\nu^k)^{\top}d^k\\
	&= \hat{f}_k - A_k(x^{k+1}) - (\nu^k)^{\top}d^k\\
	&= C_k - (S^k)^{\top}d^k - (\nu^k)^{\top}d^k \\
	&= C_k - (S^k+\nu^k)^{\top}d^k \\
	&= C_k + (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k
\end{align}

\subsection{important assumptions}
eigenvalues of \(Q\) are bounded \(\to\) possible by manipulating BFGS update

\begin{align}
	\text{if} &\quad norm\left(\frac{y^k{y^k}^{\top}}{{y^k}^{\top}d^k}\right) > 10^{??} \\
	&\quad \text{set } \frac{y^k{y^k}^{\top}}{threshold} \\
	&\quad threshold = norm\left(y^k{y^k}^{\top}\right)/10^{??} \\
	\text{end} &
\end{align}
same procedure for next term; all \(<1/3C\) for some overall threshold \(C\) 

\(Q+\frac{1}{t_k}\mathbb{I}\) such that \(\succ \xi \mathbb{I}\) for some fixed \(\xi > 0\). \\

\begin{equation}
	\min_{\hat{x}+d \in D} M^k(\hat{x}^k+d^k)+d^{\top}\frac{1}{2}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d	
\label{Q_subprob}
\end{equation}
 

\subsection{Algorithm}
\vspace{1em}

\hrule  \vspace{0.4ex} \hrule
\vspace{1ex}
\textbf{Nonconvex proximal bundle method with inexact information}
\vspace{1ex}
\hrule
\vspace{1ex}
Select parameters \( m \in (0,1), \gamma > 0 \) and a stopping tolerance \( \mathtt{tol} \geq 0\). \\
Choose a starting point \(x^1 \in \R^n\) and compute \(f_1\) and \(g^1\). Set the initial metric matrix \(Q = \mathbb{I}\), the initial index set \(J_1:=\{1\}\) and the initial prox-center to \(\hat{x}^1 := x^1\), \(\hat{f}_1 = f_1\) and select \(t_1 > 0\).

For \(k = 1,2,3,  \dotsc \)   

\begin{enumerate}
	\item Calculate \[d^k = \arg \min_{d \in \R^n} \left\{ M_k(\hat{x}^k+d)+\mathbb{I}_X(\hat{x}^k+d)+\frac{1}{2}d^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d\right\}.\]
	\item Set \textcolor{red}{\(\to\) other stopping condition!!!
		\begin{align*} 
		  G^k &= \sum_{j \in J_k}{\alpha_j^k s_j^k}, \quad	\nu^k = -\frac{1}{t_k}d^k-G^k???????????\\
			C_k &= \sum_{j \in J_k}{\alpha_j^k c_j^k} \\
	    \delta_k &=  C_k + (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k
		\end{align*} }
		If \(\delta_k \leq \mathtt{tol} \rightarrow \) STOP.
	\item Set \( x^{k+1} = \hat{x}^k + d^k \).
	\item Compute \(f^{k+1}, g^{k+1}\) \\
	If 
	\[f^{k+1} \leq \hat{f}^k - m\delta_k \quad \rightarrow \text{ serious step} \]
	Set \(\hat{x}^{k+1} = x^{k+1}, \hat{f}^{k+1} = f^{k+1}\) and select \(t_{k+1} > 0\). \\
	Otherwise \(\rightarrow\) nullstep \\
	Set \(\hat{x}^{k+1} = \hat{x}^k, \hat{f}^{k+1}=f^{k+1}\) and choose \(0 < t_{k+1} \leq t_k\). 	
	\item Select new bundle index set \(J_{k+1}\), keeping all active elements. Calculate 
	\[ \eta_k \geq \max{\left\{\max_{j \in J_{k+1}, x^j \neq \hat{x}^{k+1}}{\frac{-2e_j^k}{|x^j - \hat{x}^{k+1}|^2}, 0}\right\}}+\gamma  \]
	and update the model \(M^k\)
\end{enumerate}
\vspace{1ex}
\hrule

\vspace{1.5em}

Lemma 5 in \cite{Hare2016} stays the same; no \(Q\) involved \\

\begin{theorem}
	Theorem 6 in \cite{Hare2016} \(\to\) take only part with \(\liminf_{k\to\infty}t_k > 0 \) because other one not used in null steps and algorithm this way. \\
	Let the algorithm generate and infinite number of serious steps. Then \(\delta_k \to 0\) as \(k \to \infty\). \\
	Let the sequence \(\{\eta_k\}\) be bounded. If \(\liminf_{k \to \infty} t_k > 0\) then as \(k \to \infty\) we have \(C_k \to 0\), and for ever accumulation point \(\bar{x}\) of \(\{\hat{x}^k\}\) there exists \(\bar{S}\) such that \(S^k \to \bar{S}\) and \(S^k + \nu^k \to 0\). \\
	In particular if the cardinality of \({j \in J^k|\alpha_j^k > 0}\) is uniformly bounded in \(k\) then the conclusions of Lemma 5 in \cite{Hare2016} hold.
\end{theorem}

The proof is very similar to the one stated in \cite{Hare2016} but minor changes have to be made due to the different formulation of the nominal decrease \(\delta_k\).

\begin{proof}
	At each serious step \(k\) holds
	
	\begin{equation}
		\hat{f}_{k+1} \leq \hat{f}_k - m\delta_k
	\label{nonincreasing}
	\end{equation}
	
	where \(m, ~\delta_k > 0\). From this follows that the sequence \(\{\hat{f}_k\}\) is nonincreasing.
	Since \(\{\hat{x}^k\} \subset D\) the sequence is by the fact that \(f\) is finite %\textcolor{red}{which assumption says \(f\) bounded below??? \(\to\) \(f\) subdifferentially regular \(\Rightarrow\) \(f\) finite}
	and \(|\sigma_k| < \bar{\sigma}\) the sequence \(\{f(\hat{x}^k)+\sigma_k\} = \{\hat{f}_k\}\) is bounded below. Together with the fact that \(\{\hat{f}_k\}\) is nonincreasing one can conclude that it converges. \\
	Using (\ref{nonincreasing}), one obtains
	
	\begin{equation}
		0 \leq m \sum_{k = 1}^l \delta_k \leq \sum_{k = 1}^l \left(\hat{f}_k-\hat{f}_{k+1}\right),
	\end{equation}
	
	so letting \(l \to \infty\), 
	
	\begin{equation}
		0 \leq m\sum_{k=1}^{\infty} \delta_k \leq \hat{f}_1 - \underbrace{\lim_{k \to \infty} \hat{f}_k}_{\neq \pm \infty}.
	\end{equation}
	As a result,
	\begin{equation}
		\sum_{k = 1}^{\infty} \delta_k = \sum_{k=1}^{\infty}\left(C^k+(d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\right) < \infty
	\end{equation}
	
	Hence, \(\delta_k \to 0\) as \(k \to \infty\). As all quantities above are nonnegative due to positive (semi-)definiteness of \(Q+\frac{1}{t_k}\mathbb{I}\), it also holds that
	
	\begin{equation}
		C_k \to 0 \quad \text{and} \quad (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \to 0.
	\end{equation}
	
	For any accumulation point \(\bar{x}\) of the sequence \(\{\hat{x}^k\}\) the corresponding subsequence \(d^k \to 0\) for \(k \in K \subset \{1,2,...\} \). As \(\liminf_{k \to \infty} t_k > 0\) and the eigenvalues of \(Q\) are bounded the whole expression 
	
	\begin{equation}
	 S^k + \nu^k = \left(Q+\frac{1}{t_k}I \right)d^k  \to 0 \quad \text{for} \quad k \in K.
	\end{equation}
	
	And from local Lipschitz continuity of \(f\) follows then that \(S^k \to \bar{S}\) for \(k \in K\).
	
\end{proof}

\begin{remark}
If one assumes that the set \(\Omega = \{x \in \R^n |f(x) \leq f(x^1) +2\bar{\sigma}\}\) is bounded, it is not necessary to use the constraint set \(D\). \\
Because all \(\{\hat{x}^k\} \subset \Omega\) one can deduce the boundedness of the sequence.
\end{remark}

For the case of infinitely many null steps one show:

\begin{theorem} \cite{Hare2016}
	Let a finite number of serious iterates be followed by infinite null steps. Let the sequence \(\{\eta_k\}\) be bounded and \(\liminf k \to \infty > 0\). \\
	Then \(\{x^k\} \to \hat{x}\), \(\delta_k \to 0\), \(C_k \to 0\), \(S^k + \nu^k \to 0\) and there exist \(K\subset \{1,2,...\}\) and \(\bar{S}\) such that \(S^k \to \bar{S^k}\) as \(K \ni k \to \infty\). \\
	In particular if the cardinality of \({j \in J^k|\alpha_j^k > 0}\) is uniformly bounded in \(k\) then the conclusions of Lemma 5 in \cite{Hare2016} hold for \(\bar{x} = \hat{x}\). 
\end{theorem}

\begin{proof}
	Let \(k\) be large enough such that \(k \geq \bar{k}\) and \(\hat{x}^k = \hat{x}\) and \(\hat{f}_k=\hat{f}\) are fixed.
	Define the optimal value of the subproblem (\ref{Q_subprob}) by 
	
	\begin{equation}
		\Psi_k := M_k(x^{k+1})+\left(d^k\right)^{\top}\frac{1}{2}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k.
	\end{equation}
	
	It is first shown that the sequence \(\{\Psi_k\}\) is bounded above.
	\textcolor{red}{Using the aggregate linearization}
	
	\begin{equation}
		A_k(\hat{x}) = M_k(x^{k+1})-\langle S^k,d^k \rangle.
	\end{equation}
	
	Using \(S^k+\nu^k = -\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\) and the subgradient inequality for \(\nu^k \in \partial \mathtt{i}_{D}\) one obtains
	
	\begin{align*}
		\Psi^k+\frac{1}{2}\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k &= A_k(\hat{x})+\langle S^k,d^k\rangle + \left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \\
		&= A_k(\hat{x})-\langle \nu^k,k \rangle \\
		&\leq A(\hat{x}) \\
		&\leq M_k(\hat{x}) \\
		& = \hat{f}
	\end{align*}
	
	\textcolor{red}{where the equations and inequalities follow from???}\\
	By boundedness of \(d^k\) and \(Q+\frac{1}{t_k}\mathbb{I}\) this yields that \(\Psi_k \leq \hat{f}\), so the sequence \(\{\Psi_k\}\) is bounded above.
	In the next step is shown that \(\{\Psi_k\}\) is increasing.
	
	\begin{align}
		\Psi_{k+1} &= M_k(x^{k+2})+\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I} \right)d^{k+1} \\
		&\geq A_k(x^{k+2})+\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		&= M_k(x^{k+1})+\langle S^k,x^{k+2}-x^{k+1} \rangle +\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		&= \Psi_k - \frac{1}{2}\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k} + \frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		& \qquad -\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k+1} - d^{k}\right) - \langle \nu^k, x^{k+2}-x^{k+1}\rangle\\
		&\geq \Psi_k + \frac{1}{2}\left(d^{k+1}-d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k+1} - d^{k}\right)
	\end{align}
	
\textcolor{red}{say where (in-)equalities come from} \\

As \(Q\) is fixed in null steps and \(\liminf_{k \to \infty} t_k > 0\) \(\{\Psi_k\}\) is increasing. The sequence is therefore convergent.
Consequently, taking into account that \(1/t_k \geq 1/t_{\bar{k}}\), it follows

\begin{equation}
	\|d^{k+1}-d^k\| \to 0, \quad k \to \infty.
\label{d_to_0}
\end{equation}

By the \textcolor{red}{definitions and characterizations that have to be specified} one has

\begin{align}
	\hat{f} &= \delta_k+M_k(\hat{x})-C_k-\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k}\right) \\
	&= \delta_k + M_k(x^{k+1})-\langle S^k,d^k \rangle-\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k}\right)  \\
	&= \delta_k \\
	&\geq \delta_k+M_k(\hat{x}+d^k)
\end{align}

Where the last inequality is given by \(\nu^k \in \partial\mathtt{i}_{D}(x^{k+1})\). Therefore 

\begin{equation}
	\delta^{k+1} \leq \hat{f}-M_{k+1}(\hat{x}+d^{k+1}).
\end{equation}

By the first inequality in assumption \textcolor{red}{define assumption} on the model, written for \(d=d^{k+1}\),

\begin{equation}
	-\hat{f}_{k+1}+c^{k+1}_{k+1}-\left\langle s^{k+1}_{k+1},d^{k+1}\right\rangle \geq -M_{k+1}(\hat{x}+d^{k+1}).
\end{equation}
As \(\hat{f}_{k+1}=\hat{f}\), adding condition \textcolor{red}{???} to the inequality above, one obtains that

\begin{equation}
	m\delta_k+\left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle \geq \hat{f}-M_{k+1}(\hat{x}+d^{k+1}).
\end{equation}

Combining this relation with \textcolor{red}{???} yields

\begin{equation}
	0 \leq \delta_{k+1} \leq m\delta_k + \left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle.
	\label{delta_ineq}
\end{equation}

Since \(m \in (0,1)\) and \(\left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle \to 0\) as \(k \to \infty\) due to (\ref{d_to_0}) and the boundedness of \(\{\eta_k\}\) using \cite[Lemma 3, p.45]{Polyak1987} it follows from  (\ref{delta_ineq}) that 

\begin{equation}
	\lim_{k \to \infty} \delta_k = 0.
\end{equation}

From the formulation \(\delta_k = C_k + \left(d^k\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\) follows that \(C_k \to 0\) as \(k \to \infty\). As \textcolor{red}{\(Q+\frac{1}{t_k}\mathbb{I} \succ \xi\mathbb{I}\)} it follows that 

\begin{equation}
	\xi \left(d^k\right)^{\top}d^k \leq \left(d^k\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \to 0
\end{equation}

\end{proof}

\begin{remark}
	To the boundedness of \(Q\) and the resulting inequalities:
	\[ A \succ B \Leftrightarrow A - B \succ 0\Leftrightarrow A-B \text{ is positive definite}\]
	Respectively for \(A \prec B\) \\
  For a real symmetric matrix \(A\) and a vector \(d\in \R^n\) the following result holds: 
	\[ A \prec \xi \mathbb{I} \Rightarrow Ad < \xi d \]
	\textbf{Proof:} \(A\) real and symmetric \(\Rightarrow\) it is orthogonally diagonalizeable:
	\begin{equation}
	\begin{split}
		\exists \lambda_i \in \R \text{ eigenvalues} \quad v^i \in \R^n \text{ eignvektors} \\
		Av^i= \lambda_iv^i \\
		\text{and} \quad \exists \alpha_i \in \R: \quad d = \sum_{i} {\alpha_i v^i} \\
		\Rightarrow Ad = A \sum_i{\alpha_i v^i} = \sum_i \alpha_i Av^i = \sum_i \alpha_i \lambda_i v^i\\
		A \prec \xi \mathbb{I}  \Leftrightarrow \max_i \lambda_i < \xi \\
		\Rightarrow Ad < \xi \sum_i \alpha_i v^i = \xi d
  \end{split}
	\end{equation}
\end{remark}












%\subsection{Subproblem Variable Metric}
%
%For comparision: Subproblem proximal bundle
%
%\begin{align}
	%&\min_{d \in \R^n, \xi \in \R} \xi + \frac{1}{2t_k}\|d\|^2 = \xi + \frac{1}{2}d^{\top}\left(\frac{1}{t_k}\mathbf{I}\right)d \\
	%&\text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
%\end{align}
%
%Subproblem variable metric:
%\begin{align}
	%&\min_{d \in \R^n, \xi in \R} \xi + \frac{1}{2}d^{\top}D_kd \\
	%&\text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
%\end{align}
%These are \(\R^{n+1}\) dimensional quadratic optimization problems.\\
%
%\textcolor{red}{Find out if \(D_k\) is diagonal matrix! Think not.} \\
%
%Approaches not so different. Instead of just scaling the identity \(\rightarrow\) induce ``curvature information'' via past subgradients. \\
%
%Dual proximal subproblem:
%\begin{align}
	%&\min_{\alpha \in \R^{|J_k|}} \frac{1}{2} \left(\sum_{j \in J_k}{\alpha_jg^j}\right)^{\top} t_k\mathbf{I} \left(\sum_{j \in J_k}{\alpha_jg^j}\right) + \sum_{j \in J_k}{\alpha_j e_j^k} \\
		%&\text{s.t.} \quad \sum_{j \in J_k}{\alpha_j} = 1 \text{ and } \alpha_j \geq 0 ~ j \in J_k
%\end{align}
%
%
%Dual variable metric subproblem:
%\begin{align}
	%&\min_{\alpha \in \R^{|J_k|}} \frac{1}{2} \left(\sum_{j \in J_k}{\alpha_jg^j}\right)^{\top} D^{-1}_k \left(\sum_{j \in J_k}{\alpha_jg^j}\right) + \sum_{j \in J_k}{\alpha_j e_j^k} \\
		%&\text{s.t.} \quad \sum_{j \in J_k}{\alpha_j} = 1 \text{ and } \alpha_j \geq 0 ~ j \in J_k
%\end{align}
%
%These are \(\R^{|J_k|}\) dimensional quadratic optimization problems. \\
%
%\textcolor{red}{check linear independent \(g^j\)'s.}
