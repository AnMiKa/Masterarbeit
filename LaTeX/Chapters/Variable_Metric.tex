\section{Variable Metric Bundle Algorithm}
\label{variable_metric}

...
We are now interested to include also curvature information into the method as soon as the objective function provides it.... \\

The proximal stabilization term mimics a curvature term pretty roughly; idea of variable metric bundle methods: allow not only Euclidean norm but a general norm \(\|x\cdot\|^2_{A} = \langle x,Ax\rangle\) with a symmetric and positive definite matrix \(A\).

This is also used by Noll et al. in \cite{Noll2012} and Noll in \cite{Noll2013} for nonconvex functions with exact and inexact information respectively.
In the original papers the motivation is to approximate the original objective function by a quadratic model

\begin{equation}
	\Phi(x,\hat{x}) = \phi(x,\hat{x})+\frac{1}{2}\langle x-\hat{x},Q(\hat{x})(x-\hat{x})\rangle
\end{equation}

with the convex and possibly nonsmooth first order model \(\phi(\cdot,\hat{x})\) and the quadratic but not necessarily convex second order part \(\frac{1}{2}\langle \cdot-\hat{x},Q(\hat{x})(\cdot-\hat{x})\rangle\).
This model is then again approximated to solve the bundle subproblem.

Here we take the same idea but interpret it more in the sense of a stabilization. The objective function is still first convexified and then this convexification approximated by a cutting plane model. The matrix \(Q(\hat{x})\) is added into the stabilization so that the \(k\)'th bundle subproblem is

\begin{equation}
	\min_{\hat{x}^k+d \in X} M_k(\hat{x}^k+d) + \frac{1}{2}\langle d,\left(Q+\frac{1}{t_k}\mathbb{I} \right) d \rangle
\end{equation}

...???


In \cite{Noll2012} and \cite{Noll2013} it is not specified how the matrix \(Q(\hat{x}\) is to be chosen.
For convergence it is necessary that the eigenvalues of \(Q(\hat{x}\) are bounded. We adopt here the notation in \cite{Noll2013} and say

\begin{equation}
	\exists q > 0 \text{ such that } -q\mathbb{I} \prec Q(\hat{x}) \prec q\mathbb{I}.
\end{equation}

Here \(A \prec B \) means that the matrix \(B-A\) is positive definite.

Additionally the matrix \(Q(\hat{x}+\frac{1}{t_k}\mathbb{I}\) has to be positive definite.

There are no other conditions put on \(Q(\hat{x}\).

It is not stated explicitly how to form such a matrix \(Q(\hat{x})\). Here we take an idea developed by Vl\v{c}ek  and Luk\v{s}an for a variable metric bundle method in \cite{Vlcek2001}.

The matrix \(Q(\hat{x}\) is formed by a BFGS-Update from the \textcolor{red}{last known - which? how many? all from bundle??} subgradients.



\begin{itemize}
	\item \(Q\) only updated in serious steps - why?
	\item 
\end{itemize}


\textcolor{red}{possible to do also SR1-update in null steps???}

\subsubsection{how to get Q and ensure all conditions}

\subsubsection{the new stopping criterion}

There are some minor changes that have to be made compared to the algorithm proposed by Hare et al. the biggest being the stopping condition.

In the same way as for (???) from the optimality condition

\begin{align}
	& 0 \in \partial M_k(x^{k+1})+\partial\mathtt{i}_{D}(x^{k+1})+\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \\
	\label{Noll_opt_cond}
\end{align}

follows that 
\begin{equation}
	S^k+\nu^k = -\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k.
\end{equation}

From this the model decrease (???) can be recovered using (???), (??) and (\ref{Noll_opt_cond}):

\begin{align}
	\delta_k  &= \hat{f}_k - M_k(x^{k+1}) - (\nu^k)^{\top}d^k\\
	&= \hat{f}_k - A_k(x^{k+1}) - (\nu^k)^{\top}d^k\\
	%&= C_k - (S^k)^{\top}d^k - (\nu^k)^{\top}d^k \\
	&= C_k - (S^k+\nu^k)^{\top}d^k \\
	&= C_k + (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k
	\label{Noll_delta}
\end{align}







\subsection{Keywords}

%important in Noll for me: optimize model + \(d^{\top}(Q+\frac{1}{t_k}\mathbb{I})d\)
%-> some kind of second order information

%important: \(Q+\frac{1}{t_k}\mathbb{I}\) must have all eigenvalues \(\geq 0\).

%idea to get \(Q\): BFGS like in Fin-papers; theory

%!!! check stopping criterion
%connection between \(d^k\) and \(G^k/S^k\) now:
%Optimality condition: 


\subsection{important assumptions}
eigenvalues of \(Q\) are bounded \(\to\) possible by manipulating BFGS update

\begin{align}
	\text{if} &\quad norm\left(\frac{y^k{y^k}^{\top}}{{y^k}^{\top}d^k}\right) > 10^{??} \\
	&\quad \text{set } \frac{y^k{y^k}^{\top}}{threshold} \\
	&\quad threshold = norm\left(y^k{y^k}^{\top}\right)/10^{??} \\
	\text{end} &
\end{align}
same procedure for next term; all \(<1/3C\) for some overall threshold \(C\) 

\(Q+\frac{1}{t_k}\mathbb{I}\) such that \(\succ \xi \mathbb{I}\) for some fixed \(\xi > 0\). \\

\begin{equation}
	\min_{\hat{x}+d \in D} M^k(\hat{x}^k+d^k)+d^{\top}\frac{1}{2}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d	
\label{Q_subprob}
\end{equation}
 

\subsection{Algorithm}
\vspace{1em}

\hrule  \vspace{0.4ex} \hrule
\vspace{1ex}
\textbf{Nonconvex variable metric bundle method with inexact information}
\vspace{1ex}
\hrule
\vspace{1ex}
Select parameters \( m \in (0,1), \gamma > 0 \) and a stopping tolerance \( \mathtt{tol} \geq 0\). \\
Choose a starting point \(x^1 \in \R^n\) and compute \(f_1\) and \(g^1\). Set the initial metric matrix \(Q = \mathbb{I}\), the initial index set \(J_1:=\{1\}\) and the initial prox-center to \(\hat{x}^1 := x^1\), \(\hat{f}_1 = f_1\) and select \(t_1 > 0\).

For \(k = 1,2,3,  \dotsc \)   

\begin{enumerate}
	\item Calculate \[d^k = \arg \min_{d \in \R^n} \left\{ M_k(\hat{x}^k+d)+\mathbb{I}_X(\hat{x}^k+d)+\frac{1}{2}d^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d\right\}.\]
	\item Set %\textcolor{red}{\(\to\) other stopping condition!!!
		\begin{align*} 
		  G^k &= \sum_{j \in J_k}{\alpha_j^k s_j^k} \\ %, \quad	\nu^k = -\frac{1}{t_k}d^k-G^k???????????\\
			C_k &= \sum_{j \in J_k}{\alpha_j^k c_j^k} \\
	    \delta_k &=  C_k + (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k
		\end{align*}
		If \(\delta_k \leq \mathtt{tol} \rightarrow \) STOP.
	\item Set \( x^{k+1} = \hat{x}^k + d^k \).
	\item Compute \(f^{k+1}, g^{k+1}\) \\
	If 
	\[f^{k+1} \leq \hat{f}^k - m\delta_k \quad \rightarrow \text{ serious step} \]
	Set \(\hat{x}^{k+1} = x^{k+1}, \hat{f}^{k+1} = f^{k+1}\) and select \(t_{k+1} > 0\). \\
	Otherwise \(\rightarrow\) nullstep \\
	Set \(\hat{x}^{k+1} = \hat{x}^k, \hat{f}^{k+1}=f^{k+1}\) and choose \(0 < t_{k+1} \leq t_k\). 	
	\item Select new bundle index set \(J_{k+1}\), keeping all active elements. Calculate 
	\[ \eta_k \geq \max{\left\{\max_{j \in J_{k+1}, x^j \neq \hat{x}^{k+1}}{\frac{-2e_j^k}{|x^j - \hat{x}^{k+1}|^2}, 0}\right\}}+\gamma  \]
	and update the model \(M^k\)
\end{enumerate}
\vspace{1ex}
\hrule

\vspace{1.5em}

Lemma 5 in \cite{Hare2016} stays the same; no \(Q\) involved \\

\begin{theorem}(c.f.\cite[Theorem 6]{Hare2016}\\
	\(\to\) take only part with \(\liminf_{k\to\infty}t_k > 0 \) because other one not used in null steps and algorithm this way. \\
	Let the algorithm generate and infinite number of serious steps. Then \(\delta_k \to 0\) as \(k \to \infty\). \\
	Let the sequence \(\{\eta_k\}\) be bounded. If \(\liminf_{k \to \infty} t_k > 0\) then as \(k \to \infty\) we have \(C_k \to 0\), and for ever accumulation point \(\bar{x}\) of \(\{\hat{x}^k\}\) there exists \(\bar{S}\) such that \(S^k \to \bar{S}\) and \(S^k + \nu^k \to 0\). \\
	In particular if the cardinality of \({j \in J^k|\alpha_j^k > 0}\) is uniformly bounded in \(k\) then the conclusions of Lemma 5 in \cite{Hare2016} hold.
\end{theorem}

The proof is very similar to the one stated in \cite{Hare2016} but minor changes have to be made due to the different formulation of the nominal decrease \(\delta_k\).

\begin{proof}
	At each serious step \(k\) holds
	
	\begin{equation}
		\hat{f}_{k+1} \leq \hat{f}_k - m\delta_k
	\label{nonincreasing}
	\end{equation}
	
	where \(m, ~\delta_k > 0\). From this follows that the sequence \(\{\hat{f}_k\}\) is nonincreasing.
	Since \(\{\hat{x}^k\} \subset D\) the sequence is by the fact that \(f\) is finite %\textcolor{red}{which assumption says \(f\) bounded below??? \(\to\) \(f\) subdifferentially regular \(\Rightarrow\) \(f\) finite}
	and \(|\sigma_k| < \bar{\sigma}\) the sequence \(\{f(\hat{x}^k)+\sigma_k\} = \{\hat{f}_k\}\) is bounded below. Together with the fact that \(\{\hat{f}_k\}\) is nonincreasing one can conclude that it converges. \\
	Using (\ref{nonincreasing}), one obtains
	
	\begin{equation}
		0 \leq m \sum_{k = 1}^l \delta_k \leq \sum_{k = 1}^l \left(\hat{f}_k-\hat{f}_{k+1}\right),
	\end{equation}
	
	so letting \(l \to \infty\), 
	
	\begin{equation}
		0 \leq m\sum_{k=1}^{\infty} \delta_k \leq \hat{f}_1 - \underbrace{\lim_{k \to \infty} \hat{f}_k}_{\neq \pm \infty}.
	\end{equation}
	As a result,
	\begin{equation}
		\sum_{k = 1}^{\infty} \delta_k = \sum_{k=1}^{\infty}\left(C^k+(d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\right) < \infty
	\end{equation}
	
	Hence, \(\delta_k \to 0\) as \(k \to \infty\). As all quantities above are nonnegative due to positive (semi-)definiteness of \(Q+\frac{1}{t_k}\mathbb{I}\), it also holds that
	
	\begin{equation}
		C_k \to 0 \quad \text{and} \quad (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \to 0.
	\end{equation}
	
	For any accumulation point \(\bar{x}\) of the sequence \(\{\hat{x}^k\}\) the corresponding subsequence \(d^k \to 0\) for \(k \in K \subset \{1,2,...\} \). As \(\liminf_{k \to \infty} t_k > 0\) and the eigenvalues of \(Q\) are bounded the whole expression 
	
	\begin{equation}
	 S^k + \nu^k = \left(Q+\frac{1}{t_k}I \right)d^k  \to 0 \quad \text{for} \quad k \in K.
	\end{equation}
	
	And from local Lipschitz continuity of \(f\) follows then that \(S^k \to \bar{S}\) for \(k \in K\).
	
\end{proof}

\begin{remark}
If one assumes that the set \(\Omega = \{x \in \R^n |f(x) \leq f(x^1) +2\bar{\sigma}\}\) is bounded, it is not necessary to use the constraint set \(D\). \\
Because all \(\{\hat{x}^k\} \subset \Omega\) one can deduce the boundedness of the sequence.
\end{remark}

For the case of infinitely many null steps one show:

\begin{theorem} \cite{Hare2016}
	Let a finite number of serious iterates be followed by infinite null steps. Let the sequence \(\{\eta_k\}\) be bounded and \(\liminf k \to \infty > 0\). \\
	Then \(\{x^k\} \to \hat{x}\), \(\delta_k \to 0\), \(C_k \to 0\), \(S^k + \nu^k \to 0\) and there exist \(K\subset \{1,2,...\}\) and \(\bar{S}\) such that \(S^k \to \bar{S^k}\) as \(K \ni k \to \infty\). \\
	In particular if the cardinality of \({j \in J^k|\alpha_j^k > 0}\) is uniformly bounded in \(k\) then the conclusions of Lemma 5 in \cite{Hare2016} hold for \(\bar{x} = \hat{x}\). 
\end{theorem}

\begin{proof}
	Let \(k\) be large enough such that \(k \geq \bar{k}\) and \(\hat{x}^k = \hat{x}\) and \(\hat{f}_k=\hat{f}\) are fixed.
	Define the optimal value of the subproblem (\ref{Q_subprob}) by 
	
	\begin{equation}
		\Psi_k := M_k(x^{k+1})+\left(d^k\right)^{\top}\frac{1}{2}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k.
		\label{Psi}
	\end{equation}
	
	It is first shown that the sequence \(\{\Psi_k\}\) is bounded above.
	From the definition

\begin{equation}	
	A_k(\hat{x}+d) := M_k(x^{k+1})+\langle S^k,d-d^k\rangle
	\label{Noll_agg_lin}
	of the aggregate linearization follows
	
	\begin{equation}
		A_k(\hat{x}) = M_k(x^{k+1})-\langle S^k,d^k \rangle.
	\end{equation}
	
	Using \(S^k+\nu^k = -\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\) and the subgradient inequality for \(\nu^k \in \partial \mathtt{i}_{D}\) one obtains
	
	\begin{align*}
		\Psi^k+\frac{1}{2}\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k &= A_k(\hat{x})+\langle S^k,d^k\rangle + \left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \\
		&= A_k(\hat{x})-\langle \nu^k,k \rangle \\
		&\leq A(\hat{x}) \\
		&\leq M_k(\hat{x}) \\
		& = \hat{f}
	\end{align*}
	
	\textcolor{red}{where the equations and inequalities follow from???}\\
	By boundedness of \(d^k\) and \(Q+\frac{1}{t_k}\mathbb{I}\) this yields that \(\Psi_k \leq \hat{f}\), so the sequence \(\{\Psi_k\}\) is bounded above.
	In the next step is shown that \(\{\Psi_k\}\) is increasing.
	
	\begin{align}
		\Psi_{k+1} &= M_k(x^{k+2})+\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I} \right)d^{k+1} \\
		&\geq A_k(x^{k+2})+\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		&= M_k(x^{k+1})+\langle S^k,x^{k+2}-x^{k+1} \rangle +\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		&= \Psi_k - \frac{1}{2}\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k} + \frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		& \qquad -\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k+1} - d^{k}\right) - \langle \nu^k, x^{k+2}-x^{k+1}\rangle\\
		&\geq \Psi_k + \frac{1}{2}\left(d^{k+1}-d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k+1} - d^{k}\right)
	\end{align}
	
\textcolor{red}{say where (in-)equalities come from} \\

As \(Q\) is fixed in null steps and \(\liminf_{k \to \infty} t_k > 0\) \(\{\Psi_k\}\) is increasing. The sequence is therefore convergent.
Consequently, taking into account that \(1/t_k \geq 1/t_{\bar{k}}\), it follows

\begin{equation}
	\|d^{k+1}-d^k\| \to 0, \quad k \to \infty.
\label{d_to_0}
\end{equation}

By the \textcolor{red}{definitions and characterizations that have to be specified} one has

\begin{align}
	\hat{f} &= \delta_k+M_k(\hat{x})-C_k-\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k}\right) \\
	&= \delta_k + M_k(x^{k+1})-\langle S^k,d^k \rangle-\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k}\right)  \\
	&= \delta_k \\
	&\geq \delta_k+M_k(\hat{x}+d^k)
\end{align}

Where the last inequality is given by \(\nu^k \in \partial\mathtt{i}_{D}(x^{k+1})\). Therefore 

\begin{equation}
	\delta^{k+1} \leq \hat{f}-M_{k+1}(\hat{x}+d^{k+1}).
\end{equation}

By the first inequality in assumption \textcolor{red}{define assumption} on the model, written for \(d=d^{k+1}\),

\begin{equation}
	-\hat{f}_{k+1}+c^{k+1}_{k+1}-\left\langle s^{k+1}_{k+1},d^{k+1}\right\rangle \geq -M_{k+1}(\hat{x}+d^{k+1}).
\end{equation}
As \(\hat{f}_{k+1}=\hat{f}\), adding condition \textcolor{red}{???} to the inequality above, one obtains that

\begin{equation}
	m\delta_k+\left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle \geq \hat{f}-M_{k+1}(\hat{x}+d^{k+1}).
\end{equation}

Combining this relation with \textcolor{red}{???} yields

\begin{equation}
	0 \leq \delta_{k+1} \leq m\delta_k + \left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle.
	\label{delta_ineq}
\end{equation}

Since \(m \in (0,1)\) and \(\left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle \to 0\) as \(k \to \infty\) due to (\ref{d_to_0}) and the boundedness of \(\{\eta_k\}\) using \cite[Lemma 3, p.45]{Polyak1987} it follows from  (\ref{delta_ineq}) that 

\begin{equation}
	\lim_{k \to \infty} \delta_k = 0.
\end{equation}

From the formulation \(\delta_k = C_k + \left(d^k\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\) follows that \(C_k \to 0\) as \(k \to \infty\). As \textcolor{red}{\(Q+\frac{1}{t_k}\mathbb{I} \succ \xi\mathbb{I}\)} it follows that 

\begin{equation}
	\xi \left(d^k\right)^{\top}d^k \leq \left(d^k\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \to 0
\end{equation}

\end{proof}

\begin{remark}
	To the boundedness of \(Q\) and the resulting inequalities:
	\[ A \succ B \Leftrightarrow A - B \succ 0\Leftrightarrow A-B \text{ is positive definite}\]
	Respectively for \(A \prec B\) \\
  For a real symmetric matrix \(A\) and a vector \(d\in \R^n\) the following result holds: 
	\[ A \prec \xi \mathbb{I} \Rightarrow Ad < \xi d \]
	\textbf{Proof:} \(A\) real and symmetric \(\Rightarrow\) it is orthogonally diagonalizeable:
	\begin{equation}
	\begin{split}
		\exists \lambda_i \in \R \text{ eigenvalues} \quad v^i \in \R^n \text{ eignvektors} \\
		Av^i= \lambda_iv^i \\
		\text{and} \quad \exists \alpha_i \in \R: \quad d = \sum_{i} {\alpha_i v^i} \\
		\Rightarrow Ad = A \sum_i{\alpha_i v^i} = \sum_i \alpha_i Av^i = \sum_i \alpha_i \lambda_i v^i\\
		A \prec \xi \mathbb{I}  \Leftrightarrow \max_i \lambda_i < \xi \\
		\Rightarrow Ad < \xi \sum_i \alpha_i v^i = \xi d
  \end{split}
	\end{equation}
\end{remark}












%\subsection{Subproblem Variable Metric}
%
%For comparision: Subproblem proximal bundle
%
%\begin{align}
	%&\min_{d \in \R^n, \xi \in \R} \xi + \frac{1}{2t_k}\|d\|^2 = \xi + \frac{1}{2}d^{\top}\left(\frac{1}{t_k}\mathbf{I}\right)d \\
	%&\text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
%\end{align}
%
%Subproblem variable metric:
%\begin{align}
	%&\min_{d \in \R^n, \xi in \R} \xi + \frac{1}{2}d^{\top}D_kd \\
	%&\text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
%\end{align}
%These are \(\R^{n+1}\) dimensional quadratic optimization problems.\\
%
%\textcolor{red}{Find out if \(D_k\) is diagonal matrix! Think not.} \\
%
%Approaches not so different. Instead of just scaling the identity \(\rightarrow\) induce ``curvature information'' via past subgradients. \\
%
%Dual proximal subproblem:
%\begin{align}
	%&\min_{\alpha \in \R^{|J_k|}} \frac{1}{2} \left(\sum_{j \in J_k}{\alpha_jg^j}\right)^{\top} t_k\mathbf{I} \left(\sum_{j \in J_k}{\alpha_jg^j}\right) + \sum_{j \in J_k}{\alpha_j e_j^k} \\
		%&\text{s.t.} \quad \sum_{j \in J_k}{\alpha_j} = 1 \text{ and } \alpha_j \geq 0 ~ j \in J_k
%\end{align}
%
%
%Dual variable metric subproblem:
%\begin{align}
	%&\min_{\alpha \in \R^{|J_k|}} \frac{1}{2} \left(\sum_{j \in J_k}{\alpha_jg^j}\right)^{\top} D^{-1}_k \left(\sum_{j \in J_k}{\alpha_jg^j}\right) + \sum_{j \in J_k}{\alpha_j e_j^k} \\
		%&\text{s.t.} \quad \sum_{j \in J_k}{\alpha_j} = 1 \text{ and } \alpha_j \geq 0 ~ j \in J_k
%\end{align}
%
%These are \(\R^{|J_k|}\) dimensional quadratic optimization problems. \\
%
%\textcolor{red}{check linear independent \(g^j\)'s.}
