\section{Variable Metric Bundle Method}
\label{variable_metric}

\textcolor{blue}{introduction}

A way to extend the proximal bundle method is to use an arbitrary metric \(\frac{1}{2}\Langle d,W_kd \Rangle\) with a symmetric and positive definite matrix \(W_k\) instead of the Euclidean metric for the stabilization term \(\frac{1}{2t_k}\|d\|^2\). Methods doing so are called \emph{variable metric bundle methods}.
This section combines the method of Hare et al. presented in section \ref{sec_nonconv_inex} with the second order model function used by Noll in \cite{Noll2013} to a metric bundle method suitable for nonconvex functions with noise.

This section therefore starts by explaining the ideas from \cite{Noll2013} used to extend the method presented above. It then gives an explicit strategy how to update the metric during the steps of the algorithm and concludes with a convergence proof for the developed method.

Names and definitions of the objects are the ones used in section \ref{sec_nonconv_inex}.

\subsection{The Main Ingredients to the Method}
\textcolor{blue}{explanation}

As already mentioned in section \ref{sec_conv_ex} the stabilization term can be interpreted in many different ways. In the context of this section we can understand it as a pretty rough approximation of the curvature of the objective function.
Of course bundle methods are designed to work with non differentiable objectives so it cannot be expected that the function provides any kind of curvature. However, if it does, incorporating it into the method could speed up convergence.

\subsubsection{Variable Metric Bundle Methods}

Variable metric bundle methods use an approach that can be motivated by the thoughts stated above.
Instead of using the Euclidean norm for the stabilization term \(\frac{1}{2}\|d\|^2 \) the metric is derived from as symmetric and positive definite matrix \(W_k\). As the name of the method suggests, this matrix can vary over the iterations of the algorithm. The subproblem in the \(k\)'th iteration therefore reads 

\begin{equation*}
	\min_{\hat{x}^k+d} M_k(\hat{x}^k+d)+\frac{1}{2}\Langle d,W_kd \Rangle.
\end{equation*}

\textcolor{red}{some more explanation}

stabilization term mimics curvature \(\to\) more sophisticated in variable metric bundle methods \(\to\) explain those; examples \(\to\) Noll also kind of variable metric bundle method \(\to\) explain noll again (ref to chapter 2) and relate to variable metric

Paper that use variable metric bundle methods: \cite{Lemarechal1994,Lemarechal1997,Haarala2007,Vlcek2001}

\subsubsection{Second Order Model}


Describe what is needed from Noll \(\to\) describe how to really ensure desired properties (Vlcek) \(\to\) explain other changes like \(\delta_k\) \(\to\) say what is not changed






%Inspired by the work of Noll in \cite{Noll2013} which is, to our best knowledge, the only other paper dealing with bundle methods for nonconvex functions with inexactness, we are interested to include some kind of curvature information into the bundle method from section \ref{sec_nonconv_inex}.


This is also used by Noll et al. in \cite{Noll2012} and Noll in \cite{Noll2013} for nonconvex functions with exact and inexact information respectively.
In the original papers the motivation is to approximate the original objective function by a quadratic model

\begin{equation}
	\Phi(x,\hat{x}) = \phi(x,\hat{x})+\frac{1}{2}\langle x-\hat{x},Q(\hat{x})(x-\hat{x})\rangle
\end{equation}

with the convex and possibly nonsmooth first order model \(\phi(\cdot,\hat{x})\) and the quadratic but not necessarily convex second order part \(\frac{1}{2}\langle \cdot-\hat{x},Q(\hat{x})(\cdot-\hat{x})\rangle\).
This model is then again approximated to solve the bundle subproblem.

Here we take the same idea but interpret it more in the sense of a stabilization. The objective function is still first convexified and then this convexification approximated by a cutting plane model. The matrix \(Q(\hat{x})\) is added into the stabilization so that the \(k\)'th bundle subproblem is

\begin{equation}
	\min_{\hat{x}^k+d \in X} M_k(\hat{x}^k+d) + \frac{1}{2}\langle d,\left(Q+\frac{1}{t_k}\mathbb{I} \right) d \rangle
\end{equation}

...???


In \cite{Noll2012} and \cite{Noll2013} it is not specified how the matrix \(Q(\hat{x}\) is to be chosen.
For convergence it is necessary that the eigenvalues of \(Q(\hat{x}\) are bounded. We adopt here the notation in \cite{Noll2013} and say

\begin{equation}
	\exists q > 0 \text{ such that } -q\mathbb{I} \prec Q(\hat{x}) \prec q\mathbb{I}.
\end{equation}

Here \(A \prec B \) means that the matrix \(B-A\) is positive definite.

Additionally the matrix \(Q(\hat{x}+\frac{1}{t_k}\mathbb{I}\) has to be positive definite.

There are no other conditions put on \(Q(\hat{x}\).

It is not stated explicitly how to form such a matrix \(Q(\hat{x})\). Here we take an idea developed by Vl\v{c}ek  and Luk\v{s}an for a variable metric bundle method in \cite{Vlcek2001}.

The matrix \(Q(\hat{x}\) is formed by a BFGS-Update from the \textcolor{red}{last known - which? how many? all from bundle??} subgradients.



\begin{itemize}
	\item \(Q\) only updated in serious steps - why?
	\item 
\end{itemize}


\textcolor{red}{possible to do also SR1-update in null steps???}

\subsubsection{how to get Q and ensure all conditions}

\subsubsection{the new stopping criterion}

There are some minor changes that have to be made compared to the algorithm proposed by Hare et al. the biggest being the stopping condition.

In the same way as for (???) from the optimality condition

\begin{align}
	& 0 \in \partial M_k(x^{k+1})+\partial\mathtt{i}_{D}(x^{k+1})+\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \\
	\label{Noll_opt_cond}
\end{align}

follows that 
\begin{equation}
	S^k+\nu^k = -\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k.
	\label{subgr_Q}
\end{equation}

From this the model decrease (???) can be recovered using (???), (??) and (\ref{Noll_opt_cond}):

\begin{equation}
\begin{split}
	\delta_k  &= \hat{f}_k - M_k(x^{k+1}) - (\nu^k)^{\top}d^k\\
	&= \hat{f}_k - A_k(x^{k+1}) - (\nu^k)^{\top}d^k\\
	%&= C_k - (S^k)^{\top}d^k - (\nu^k)^{\top}d^k \\
	&= C_k - (S^k+\nu^k)^{\top}d^k \\
	&= C_k + (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k
	\label{Noll_delta}
\end{split}
\end{equation}


As the changes in the algorithm concern only the stabilization and the model decrease \(d_k\) all relations that were obtained for the different prats of the model \(M_k\) in section \ref{sec_nonconv_inex} are still valid.




%\subsection{Keywords}

%important in Noll for me: optimize model + \(d^{\top}(Q+\frac{1}{t_k}\mathbb{I})d\)
%-> some kind of second order information

%important: \(Q+\frac{1}{t_k}\mathbb{I}\) must have all eigenvalues \(\geq 0\).

%idea to get \(Q\): BFGS like in Fin-papers; theory

%!!! check stopping criterion
%connection between \(d^k\) and \(G^k/S^k\) now:
%Optimality condition: 


\subsection{Keywords}
eigenvalues of \(Q\) are bounded \(\to\) possible by manipulating BFGS update

\begin{align}
	\text{if} &\quad \text{norm}\left(\frac{y^k{y^k}^{\top}}{{y^k}^{\top}d^k}\right) > 1/3C \\
	&\quad \text{set } \frac{y^k{y^k}^{\top}}{\zeta} \\
	&\quad \zeta = \frac{\text{norm}\left(y^k{y^k}^{\top}\right)}{1/3C} \\
	\text{end} &
\end{align}
same procedure for next term; all \(<1/3C\) for some overall threshold \(C\) 

\(Q+\frac{1}{t_k}\mathbb{I}\) such that \(\succ \xi \mathbb{I}\) for some fixed \(\xi > 0\). \\

\begin{equation}
	\min_{\hat{x}+d \in D} M^k(\hat{x}^k+d^k)+d^{\top}\frac{1}{2}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d	
\label{Q_subprob}
\end{equation}
 

\subsection{Algorithm}

\textcolor{red}{same form as Hare algorithm (nullstep)\\
add \(Q\) calculation}
\vspace{1em}

\hrule  \vspace{0.4ex} \hrule
\vspace{1ex}
\textbf{Nonconvex Variable Metric Bundle Method with Inexact Information}
\vspace{1ex}
\hrule
\vspace{1ex}
Select parameters \( m \in (0,1), \gamma > 0 \) and a stopping tolerance \( \mathtt{tol} \geq 0\). \\
Choose a starting point \(x^1 \in \R^n\) and compute \(f_1\) and \(g^1\). Set the initial metric matrix \(Q = \mathbb{I}\), the initial index set \(J_1:=\{1\}\) and the initial prox-center to \(\hat{x}^1 := x^1\), \(\hat{f}_1 = f_1\) and select \(t_1 > 0\).

For \(k = 1,2,3,  \dotsc \)   

\begin{enumerate}
	\item Calculate \[d^k = \arg \min_{d \in \R^n} \left\{ M_k(\hat{x}^k+d)+\mathbb{I}_X(\hat{x}^k+d)+\frac{1}{2}d^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d\right\}.\]
	\item Set %\textcolor{red}{\(\to\) other stopping condition!!!
		\begin{align*} 
		  G^k &= \sum_{j \in J_k}{\alpha_j^k s_j^k}, \\ %, \quad	\nu^k = -\frac{1}{t_k}d^k-G^k???????????\\
			C_k &= \sum_{j \in J_k}{\alpha_j^k c_j^k}, \\
	    \delta_k &=  C_k + (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k.
		\end{align*}
		If \(\delta_k \leq \mathtt{tol} \rightarrow \) STOP.
	\item Set \( x^{k+1} = \hat{x}^k + d^k \).
	\item Compute \(f^{k+1}, g^{k+1}\). \\
	If 
	\[f^{k+1} \leq \hat{f}^k - m\delta_k \quad \rightarrow \text{ serious step} \]
	Set \(\hat{x}^{k+1} = x^{k+1}, \hat{f}^{k+1} = f^{k+1}\) and select \(t_{k+1} > 0\). \\
	Calculate \(Q(\hat{x}^k)\) ...
	Otherwise \(\rightarrow\) nullstep \\
	Set \(\hat{x}^{k+1} = \hat{x}^k, \hat{f}^{k+1}=f^{k+1}\) and choose \(0 < t_{k+1} \leq t_k\). 	
	\item Select new bundle index set \(J_{k+1}\), keeping all active elements. Calculate 
	\[ \eta_k \geq \max{\left\{\max_{j \in J_{k+1}, x^j \neq \hat{x}^{k+1}}{\frac{-2e_j^k}{|x^j - \hat{x}^{k+1}|^2}, 0}\right\}}+\gamma  \]
	and update the model \(M^k\).
\end{enumerate}
\vspace{1ex}
\hrule

\vspace{1.5em}


\subsection{Convergence Analysis}

In this section the convergence properties of the new method are analyzed. We do this the same way it is done by Hare et al. in \cite{Hare2016}.

In the paper all convergence properties are first stated in \cite[Lemma 5]{Hare2016}. It is then shown that all sequences generated by the method meet the requirements of this lemma which we repeat here for convenience.

\begin{lemma}[{\cite[Lemma 5]{Hare2016}}]\label{Lemma5}
	Suppose that the cardinality of the set \(\{j \in J^k| \alpha_j^k > 0\}\) is uniformly bounded in \(k\).
	
	(i) If \(C^k \to 0\) as \(k \to \infty\), then 
	\[ \sum_{j \in J^k}\alpha_j^k\|x^j-\hat{x}^k\| \to 0 \text{ as } k \to \infty. \]
	
	(ii) If additionally for some subset \(K \subset \{1,2,\dots\}\),
	\[\hat{x}^k \to \bar{x}, S^k \to \bar{S} \text{ as } K \ni k \to \infty, \text{ with } \{\eta_k|k \in K\} \text{ bounded,} \]
	
	then we also have 
	\[\bar{S} \in \partial f(\bar{x})+B_{\bar{\theta}}(0).\]
	
	(iii) If in addition \(S^k + \nu^k \to 0\) as \(K \in k \to \infty\), then \(\bar{x}\) satisfies the approximate stationarity condition 
	\begin{equation}
		0 \in \left(\partial f(\bar{x}) + \partial \mathtt{i}_X(\bar{x}) \right) + B_{\bar{\theta}}(0).
	\end{equation}
	
	(iv) Finally if \(f\) is also lower-\(\mathcal{C}^1\), then for each \(\varepsilon > 0\) there exists \(\rho > 0\) such that
	\begin{equation}
		f(y) \geq f(\bar{x})-(\bar{\theta}+\varepsilon)\|y-\bar{x}\|-2\bar{\sigma}, \quad \text{ for all } y \in X\cup B_{\rho}(\bar{x}).
	\end{equation}
\end{lemma}

As the neither the stabilization nor the descent test is involved in the proof of Lemma \ref{Lemma5} it is the same as in \cite{Hare2016}.

We prove now that also the variable metric version of the algorithm fulfills all requirements of Lemma \ref{Lemma5}.
The proof is divided into two parts. The first case covers the case of infinitely many serious steps, the second one considers infinitely many null steps.

For both proofs the following lemma is needed:

\begin{lemma}
	For a symmetric matrix \(A \in \R^{n\times n}\) and a vector \(d\in \R^n\) the following result holds: 
	\[ A \prec \xi \mathbb{I} \Rightarrow Ad < \xi d \]
\end{lemma}

\begin{proof}
	As the matrix \(A\) is real and symmetric it is orthogonally diagonalizeable.
	There exist eigenvalues \(\lambda_i \in \R, i = \{1,...,n\}\) and corresponding eigenvectors \(v^i \in \R^n, i = \{1,...,n\}\) that satisfy the equations
	
	\begin{equation*}
		Av^i= \lambda_iv^i \quad i = \{1,...,n\}.
		\label{A_d_rel}
	\end{equation*}
	
	The eigenvectors \(v^i\) generate a basis for \(\R^n\) so any vector \(d \in \R^n\) can be written as
	
	\[ d = \sum_{i} {\alpha_i v^i} \]
	
	for \(\alpha_i \in \R^n, i = \{1,...,n\}\).
	
	This yields
	
	\begin{equation}
		Ad = A \sum_i{\alpha_i v^i} = \sum_i \alpha_i \lambda_i v^i.
		\label{Ad}
	\end{equation}
	
	Plugging the assumption \(A \prec \xi \mathbb{I}\) which is equivalent to \(\max_i \lambda_i < \xi\) into (\ref{Ad})
	we get relation (\ref{A_d_rel}) by
		\[Ad < \xi \sum_i \alpha_i v^i = \xi d. \]

	%\[ A \succ B \Leftrightarrow A - B \succ 0\Leftrightarrow A-B \text{ is positive definite}\]
	%Respectively for \(A \prec B\) \\
\end{proof}

\begin{theorem}[c.f.{\cite[Theorem 6]{Hare2016}}]
	%\(\to\) take only part with \(\liminf_{k\to\infty}t_k > 0 \) because other one not used in null steps and algorithm this way. \\
	Let the algorithm generate and infinite number of serious steps. Then \(\delta_k \to 0\) as \(k \to \infty\). \\
	Let the sequence \(\{\eta_k\}\) be bounded. If \(\liminf_{k \to \infty} t_k > 0\) then as \(k \to \infty\) we have \(C_k \to 0\), and for every accumulation point \(\bar{x}\) of \(\{\hat{x}^k\}\) there exists \(\bar{S}\) such that \(S^k \to \bar{S}\) and \(S^k + \nu^k \to 0\). \\
	In particular if the cardinality of \(\{j \in J^k|\alpha_j^k > 0\}\) is uniformly bounded in \(k\) then the conclusions of Lemma \ref{Lemma5} hold.
\end{theorem}

The proof is very similar to the one stated in \cite{Hare2016} but minor changes have to be made due to the different formulation of the nominal decrease \(\delta_k\).

\begin{proof}
	At each serious step we have
	
	\begin{equation}
		\hat{f}_{k+1} \leq \hat{f}_k - m\delta_k
	\label{nonincreasing}
	\end{equation}
	
	where \(m, ~\delta_k > 0\). From this follows that the sequence \(\{\hat{f}_k\}\) is nonincreasing.
	Since \(\{\hat{x}^k\} \subset X\) and \(f\) is continuous the sequence \(f(\hat{x}^k)\) is bounded. %\textcolor{red}{which assumption says \(f\) bounded below??? \(\to\) \(f\) subdifferentially regular \(\Rightarrow\) \(f\) finite}
	With \(|\sigma_k| < \bar{\sigma}\) the sequence \(\{f(\hat{x}^k)+\sigma_k\} = \{\hat{f}_k\}\) is bounded below. Together with the fact that \(\{\hat{f}_k\}\) is nonincreasing one can conclude that it converges. \\
	Using (\ref{nonincreasing}), one obtains
	
	\begin{equation*}
		0 \leq m \sum_{k = 1}^l \delta_k \leq \sum_{k = 1}^l \left(\hat{f}_k-\hat{f}_{k+1}\right),
	\end{equation*}
	
	so letting \(l \to \infty\), 
	
	\begin{equation*}
		0 \leq m\sum_{k=1}^{\infty} \delta_k \leq \hat{f}_1 - \underbrace{\lim_{k \to \infty} \hat{f}_k}_{\neq \pm \infty}.
	\end{equation*}
	
	This yields
	\begin{equation*}
		\sum_{k = 1}^{\infty} \delta_k = \sum_{k=1}^{\infty}\left(C^k+(d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k\right) < \infty
	\end{equation*}
	
	Hence, \(\delta_k \to 0\) as \(k \to \infty\). All quantities above are nonnegative due to positive definiteness of \(Q+\frac{1}{t_k}\mathbb{I}\), so it also holds that
	
	\begin{equation*}
		C_k \to 0 \quad \text{and} \quad (d^k)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \to 0.
	\end{equation*}
	
	For any accumulation point \(\bar{x}\) of the sequence \(\{\hat{x}^k\}\) the corresponding subsequence \(d^k \to 0\) for \(k \in K \subset \{1,2,...\} \). As \(\liminf_{k \to \infty} t_k > 0\) and the eigenvalues of \(Q\) are bounded the whole expression 
	
	\begin{equation*}
	 S^k + \nu^k = \left(Q+\frac{1}{t_k}I \right)d^k  \to 0 \quad \text{for} \quad k \in K.
	\end{equation*}
	
	And from local Lipschitz continuity of \(f\) follows then that \(S^k \to \bar{S}\) for \(k \in K\).
	
\end{proof}

%\begin{remark}
%If one assumes that the set \(\Omega = \{x \in \R^n |f(x) \leq f(x^1) +2\bar{\sigma}\}\) is bounded, it is not necessary to use the constraint set \(X\). \\
%Because \(\{\hat{x}^k\} \subset \Omega\) one can deduce the boundedness of the sequence.
%\end{remark}

For the case of infinitely many null steps we need result (31) from \cite{Hare2016}. It only depends on the definitions of the augmented linearization error and subgradient.

Whenever \(x^{k+1}\) is as declared a null step, the relation 

\begin{equation}
	-c^{k+1}_{k+1} + \Langle s^{k+1}_{k+1}, x^{k+1}-\hat{x}^k\Rangle \geq -m\delta_k
\label{no_ls}
\end{equation}

holds.

\begin{theorem} [c.f. {\cite[Theorem 7]{Hare2016}}]
	Let a finite number of serious iterates be followed by infinite null steps. Let the sequence \(\{\eta_k\}\) be bounded and \(\liminf k \to \infty > 0\). \\
	Then \(\{x^k\} \to \hat{x}\), \(\delta_k \to 0\), \(C_k \to 0\), \(S^k + \nu^k \to 0\) and there exist \(K\subset \{1,2,...\}\) and \(\bar{S}\) such that \(S^k \to \bar{S^k}\) as \(K \ni k \to \infty\). \\
	In particular if the cardinality of \(\{j \in J^k|\alpha_j^k > 0 \}\) is uniformly bounded in \(k\) then the conclusions of Lemma \ref{Lemma5} hold for \(\bar{x} = \hat{x}\). 
\end{theorem}

\begin{proof}
	Let \(k\) be large enough such that \(k \geq \bar{k}\) and \(\hat{x}^k = \hat{x}\) and \(\hat{f}_k=\hat{f}\) are fixed.
	Define the optimal value of the subproblem (\ref{Q_subprob}) by 
	
	\begin{equation}
		\Psi_k := M_k(x^{k+1})+\left(d^k\right)^{\top}\frac{1}{2}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k.
		\label{Psi}
	\end{equation}
	
	It is first shown that the sequence \(\{\Psi_k\}\) is bounded above.
	From definition (\ref{aug_agg_lin}) follows

%\begin{equation*}	
	%A_k(\hat{x}+d) := M_k(x^{k+1})+\langle S^k,d-d^k\rangle
	%\label{Noll_agg_lin}
%\end{equation*}
	%of the aggregate linearization follows
	
	\begin{equation*}
		A_k(\hat{x}) = M_k(x^{k+1})-\langle S^k,d^k \rangle.
	\end{equation*}
	
Using (\ref{subgr_Q}) for the second equality, the subgradient inequality for \(\nu^k \in \partial \mathtt{i}_{D}\) in the first inequality and (\ref{A_leq_M}) for the second inequality  one obtains
	
	\begin{align*}
		\Psi^k+\frac{1}{2}\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k &= A_k(\hat{x})+\langle S^k,d^k\rangle + \left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \\
		&= A_k(\hat{x})-\langle \nu^k,k \rangle \\
		&\leq A(\hat{x}) \\
		&\leq M_k(\hat{x}) \\
		& = \hat{f}.
	\end{align*}
	
By boundedness of \(d^k\) and \(Q+\frac{1}{t_k}\mathbb{I}\) this yields that \(\Psi_k \leq \hat{f}\), so the sequence \(\{\Psi_k\}\) is bounded above.
	In the next step is shown that \(\{\Psi_k\}\) is increasing. For this we obtain
	
	\begin{align*}
		\Psi_{k+1} &= M_k(x^{k+2})+\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I} \right)d^{k+1} \\
		&\geq A_k(x^{k+2})+\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		&= M_k(x^{k+1})+\langle S^k,x^{k+2}-x^{k+1} \rangle +\frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		&= \Psi_k - \frac{1}{2}\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k} + \frac{1}{2}\left(d^{k+1}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^{k+1} \\
		& \qquad -\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k+1} - d^{k}\right) - \langle \nu^k, x^{k+2}-x^{k+1}\rangle\\
		&\geq \Psi_k + \frac{1}{2}\left(d^{k+1}-d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k+1} - d^{k}\right),
	\end{align*}
	
where the first inequality comes from (\ref{A_leq_M}) and the fact that \(t_{k+1} \leq t_{k}\) for null steps.The second equality follows from (\ref{aug_agg_lin}), the third equation by (\ref{subgr_Q}) and (\ref{Psi}) and the last inequality holds y \(\nu^k \in \partial \mathtt{i}_X(x^{k+1})\).

As \(Q\) is fixed in null steps and \(\liminf_{k \to \infty} t_k > 0\) \(\{\Psi_k\}\) is increasing. The sequence is therefore convergent.
Taking into account that \(1/t_k \geq 1/t_{\bar{k}}\), it therefore follows that

\begin{equation}
	\|d^{k+1}-d^k\| \to 0, \quad k \to \infty.
\label{d_to_0}
\end{equation}

By definition (\ref{Noll_delta}) and the fact that the augmented aggregate error can be expressed as

\begin{equation*}
	C_k = \hat{f}-M_k(x^{k+1})+\Langle S^k,d^k \Rangle 
\end{equation*}

by the KKT conditions follows

\begin{align*}
	\hat{f} &= \delta_k+M_k(\hat{x})-C_k-\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k}\right) \\
	&= \delta_k + M_k(x^{k+1})-\langle S^k,d^k \rangle-\left(d^{k}\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)\left(d^{k}\right)  \\
	&= \delta_k + M_k(\hat{x}+d^k)+\Langle \nu^k, d^k \Rangle\\
	&\geq \delta_k+M_k(\hat{x}+d^k)
\end{align*}

Where the last inequality is given by \(\nu^k \in \partial\mathtt{i}_{X}(x^{k+1})\). Therefore 

\begin{equation}
	\delta^{k+1} \leq \hat{f}-M_{k+1}(\hat{x}+d^{k+1}).
	\label{dfM}
\end{equation}

By assumption (\ref{Model_update_1}) on the model, written for \(d=d^{k+1}\),

\begin{equation*}
	-\hat{f}_{k+1}+c^{k+1}_{k+1}-\left\langle s^{k+1}_{k+1},d^{k+1}\right\rangle \geq -M_{k+1}(\hat{x}+d^{k+1}).
\end{equation*}

In the nullstep case \(\hat{f}_{k+1}=\hat{f}\) so adding condition (\ref{no_ls}) to the inequality above, one obtains that

\begin{equation*}
	m\delta_k+\left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle \geq \hat{f}-M_{k+1}(\hat{x}+d^{k+1}).
\end{equation*}

Combining this relation with (\ref{dfM}) yields

\begin{equation*}
	0 \leq \delta_{k+1} \leq m\delta_k + \left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle.
	\label{delta_ineq}
\end{equation*}

Because \(m \in (0,1)\) and \(\left\langle s^{k+1}_{k+1},d^k-d^{k+1}\right\rangle \to 0\) as \(k \to \infty\) due to (\ref{d_to_0}) and the boundedness of \(\{\eta_k\}\) using \cite[Lemma 3, p.45]{Polyak1987} it follows from  (\ref{delta_ineq}) that 

\begin{equation*}
	\lim_{k \to \infty} \delta_k = 0.
\end{equation*}

From  formulation (\ref{Noll_delta}) of the model decrease follows that \(C_k \to 0\) as \(k \to \infty\). Since {\(Q+\frac{1}{t_k}\mathbb{I} \succ \xi\mathbb{I}\)} due to \(\liminf_{k \to \infty} > 0\) and the bounded eigenvalues of \(Q\) we have 

\begin{equation*}
	\xi \left(d^k\right)^{\top}d^k \leq \left(d^k\right)^{\top}\left(Q+\frac{1}{t_k}\mathbb{I}\right)d^k \to 0
\end{equation*}

This means that \(d^k \to 0\) for \(k \to \infty\) and therefore \(\lim_{k \to \infty}x^k = \hat{x}\). It also follows that \(\|S^k+\\vu^k\| \to 0\) as \(k \to \infty\). Passing to some subsequence if necessary we can conclude that \(S^k\) converges to some \(\bar{S}\) and as \(\hat{x}^k = \bar{x}\) for all \(k\) all requirements of Lemma \ref{Lemma5} are fulfilled.

\end{proof}

\begin{remark}
	All results deduced in section \ref{On_diff_conv_res} are still valid for this algorithm.
\end{remark}









%\subsection{Subproblem Variable Metric}
%
%For comparision: Subproblem proximal bundle
%
%\begin{align}
	%&\min_{d \in \R^n, \xi \in \R} \xi + \frac{1}{2t_k}\|d\|^2 = \xi + \frac{1}{2}d^{\top}\left(\frac{1}{t_k}\mathbf{I}\right)d \\
	%&\text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
%\end{align}
%
%Subproblem variable metric:
%\begin{align}
	%&\min_{d \in \R^n, \xi in \R} \xi + \frac{1}{2}d^{\top}D_kd \\
	%&\text{s.t.} \quad f(\hat{x}^k)+{g^j}^{\top}d - e^k_j - \xi \leq 0, \quad j \in J_k
%\end{align}
%These are \(\R^{n+1}\) dimensional quadratic optimization problems.\\
%
%\textcolor{red}{Find out if \(D_k\) is diagonal matrix! Think not.} \\
%
%Approaches not so different. Instead of just scaling the identity \(\rightarrow\) induce ``curvature information'' via past subgradients. \\
%
%Dual proximal subproblem:
%\begin{align}
	%&\min_{\alpha \in \R^{|J_k|}} \frac{1}{2} \left(\sum_{j \in J_k}{\alpha_jg^j}\right)^{\top} t_k\mathbf{I} \left(\sum_{j \in J_k}{\alpha_jg^j}\right) + \sum_{j \in J_k}{\alpha_j e_j^k} \\
		%&\text{s.t.} \quad \sum_{j \in J_k}{\alpha_j} = 1 \text{ and } \alpha_j \geq 0 ~ j \in J_k
%\end{align}
%
%
%Dual variable metric subproblem:
%\begin{align}
	%&\min_{\alpha \in \R^{|J_k|}} \frac{1}{2} \left(\sum_{j \in J_k}{\alpha_jg^j}\right)^{\top} D^{-1}_k \left(\sum_{j \in J_k}{\alpha_jg^j}\right) + \sum_{j \in J_k}{\alpha_j e_j^k} \\
		%&\text{s.t.} \quad \sum_{j \in J_k}{\alpha_j} = 1 \text{ and } \alpha_j \geq 0 ~ j \in J_k
%\end{align}
%
%These are \(\R^{|J_k|}\) dimensional quadratic optimization problems. \\
%
%\textcolor{red}{check linear independent \(g^j\)'s.}
