% Appendix
\section{Appendix}

\subsection{Omitted Proofs}

In this section the proofs that were omitted in the main part of the thesis are given.

\subsubsection{Eigenvalues of the Metric Matrix}
\label{proof_eigval}

\begin{proposition}
		Let \(A \in \R^{n\times n}\), \(b \in \R\) and \(\mathbb{I} \in \R^{n\times n}\) the identity matrix.
		Let \(\lambda^A_i\), \(i = 1,...,n\) be the eigenvalues of the matrix \(A\). Then the eigenvalues of the matrix \(A+b\mathbb{I}\) are given by \(\tilde{\lambda}_i := \lambda^A_i + b\) for all \(i = 1,...,n\).
\end{proposition}

\begin{proof}
	Let \(v^i\), \(i = 1,...,n\) be the corresponding eigenvectors to the eigenvalues \(\lambda^A+i\). Then it follows that for \(i = 1,...,n\)
	
	\[ (A+b\mathbb{I})v^i = Av^i+bv^i = (\lambda_i^A + b)v^i. \]
	
	This means that \(v^i\) is an eigenvector of \(A+b\mathbb{I}\) to the eigenvalue \(\lambda_i^A + b\) for all \(i = 1,...,n\).
\end{proof}

\subsubsection{Proof of Proposition \ref{prop_norm}}
\label{proof_norm}

\begin{proof}
	We show that the scalar product \(\Langle x,y \Rangle_{Q_k+\frac{1}{t_k}\mathbb{I}} := x^{\top} (Q_k+\frac{1}{t_k}\mathbb{I})y\) is well-defined.
	This yields directly that also the norm induced by the scalar product is well-defined (see for example \cite[Corollary 12.6, p.172]{Liesen2015}).
	
	By proposition \ref{prop_bounded} the matrix \(Q_k+\frac{1}{t_k}\mathbb{I}\) is bounded and relation (\ref{rel_bound}) assures that the following calculations are valid for all \(k\).
	
	We prove now that the matrix \(Q_k+\frac{1}{t_k}\mathbb{I}\) can be used to define a scalar product.
	
From the rules for matrix-vector multiplication follows that

\[ (x+y)^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)z = x^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)z + y^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)z, \quad x,y,z \in \R^n\]

and 

\[ x^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)(y+z) = x^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)y + x^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)z, \quad x,y,z \in \R^n.\]

Thus linearity of the defined scalar product is proven.

The symmetry of \(Q_k+\frac{1}{t_k}\mathbb{I}\) yields symmetry of the scalar product by

\[ x^{\top} \underbrace{\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)y}_{:=\tilde{y}} = \tilde{y}^{\top}x = y^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)^{\top}x = y^{\top}\left(Q_k+\frac{1}{t_k}\mathbb{I}\right)x.   \]

Finally positive definiteness of the scalar product follows directly from positive definiteness of the matrix \(Q_k+\frac{1}{t_k}\mathbb{I}\).

This means the scalar product \(\langle \cdot,\cdot \rangle_{Q_k+\frac{1}{t_k}\mathbb{I}}\) is well defined and thus induces the norm \(\Vert \cdot \Vert_{Q_k+\frac{1}{t_k}\mathbb{I}}\).
\end{proof}


\subsection{Additional Figures}

\subsubsection{Variable Metric Bundle Method}

The following plots show the behavior in accuracy and number of steps of the proximal bundle algorithm \ref{sec_nonconv_inex}.1 and different realizations of the variable metric bundle method \ref{sec_variable_metric}.1 when optimizing the Ferrier polynomials \(f_1\) to \(f_5\) in different dimensions and for different noise forms.
The conditions and parameters used for the plots are described in section \ref{sec_num_test_ferr}

The two plots below depict the situation for \(x \in \R^n\) for \(n = 2,3,...,15\).

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/constant_gradient_noise.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_gradient_noise.eps}%
	\end{subfigure}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of constant gradient noise}%
	\label{fig_const_grad_noise}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_gradient_noise.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_gradient_noise.eps}%
	\end{subfigure}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of vanishing gradient noise}%
	\label{fig_van_grad_noise}%
\end{figure}


The following plots show the situation for larger dimensions \(n = \{20,25,30,40,50\}\).


\begin{figure}[H]%
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/no_noise_b.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_no_noise_b.eps}%
	\end{subfigure}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of no noise}
	\label{fig_no_noise_large}

\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/constant_noise_b.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_noise_b.eps}%
	\end{subfigure}
	\vspace{-.5em}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of constant noise}%
	\label{fig_const_noise_large}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_noise_b.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_noise_b.eps}%
	\end{subfigure}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of vanishing noise}%
	\label{fig_van_noise_large}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/constant_gradient_noise_b.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_gradient_noise_b.eps}%
	\end{subfigure}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of constant gradient noise}%
	\label{fig_const_grad_noise_large}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_gradient_noise_b.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_gradient_noise_b.eps}%
	\end{subfigure}
	\caption{Comparison of accuracy and number of steps for the proximal bundle algorithm and the variable metric bundle algorithm in the case of vanishing gradient noise}%
	\label{fig_van_grad_noise_large}%
\end{figure}

Next come the figures that illustrate the difference in behavior for different step size updating parameter \(\kappa_+\) and the performance of the hybrid method. In this method the metric matrix is scaled for boundedness of the eigenvalues and then scaled again by \(1/k\).


%\begin{figure}[H]
	%\begin{subfigure}{0.49\textwidth}
		%%\includegraphics[width=\textwidth]{Pictures/Plots/constant_noise_comp.eps}%
	%\end{subfigure}
	%\begin{subfigure}{0.49\textwidth}
		%%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_noise_comp.eps}%
	%\end{subfigure}
	%\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for constant noise. The reached accuracy is depicted on the left, the needed number of steps on the right.}%
	%\label{fig_const_noise_comp}%
%\end{figure}
%
%\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_noise_comp.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_noise_comp.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for vanishing noise.}%
	\label{fig_van_noise_comp}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/constant_gradient_noise_comp.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_gradient_noise_comp.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for constant gradient noise.}%
	\label{fig_const_grad_noise_comp}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_gradient_noise_comp.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_gradient_noise_comp.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for vanishing gradient noise.}%
	\label{fig_van_grad_noise_comp}%
\end{figure}

\vspace{-1.5em}


%Following plots are in Noll part with \(\kappa_+ = 1.2, 2\) and in \(1/k\) part with \(\kappa_+ = 1.2\) for the large dimension
\begin{figure}[H]%
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/no_noise_compb.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_no_noise_compb.eps}%
	\end{subfigure}
	\label{fig_no_noise_comp_large}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for the exact case for higher \(x\)-dimensions. The reached accuracy is depicted on the left, the needed number of steps on the right.}
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/constant_noise_compb.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_noise_compb.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for constant noise.}%
	\label{fig_const_noise_comp_large}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_noise_compb.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_noise_compb.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for vanishing noise.}%
	\label{fig_van_noise_comp_large}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/constant_gradient_noise_compb.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_constant_gradient_noise_compb.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for constant gradient noise.}%
	\label{fig_const_grad_noise_comp_large}%
\end{figure}

\vspace{-1.5em}

\begin{figure}[H]
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/vanishing_gradient_noise_compb.eps}%
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		%\includegraphics[width=\textwidth]{Pictures/Plots/steps_vanishing_gradient_noise_compb.eps}%
	\end{subfigure}
	\caption{Influence of the step size updating parameter \(\kappa_+ = 1.2\) and \(\kappa_+ =2 \) and performance of the hybrid method for vanishing gradient noise.}%
	\label{fig_van_grad_noise_comp_large}%
\end{figure}

